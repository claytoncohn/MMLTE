{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Clayton Cohn<br>\n",
        "4 Nov 2023<br>\n",
        "OELE Lab<br>\n",
        "Vanderbilt University\n",
        "\n",
        "#<center> S18 Acronym Mapping"
      ],
      "metadata": {
        "id": "9PKxx1Tapha2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction and Attribution\n",
        "\n",
        "This notebook was create by Clayton Cohn for the purpose of creating the MMLTE survey's final consensus document.\n",
        "\n",
        "In this notebook, we will:\n",
        "*   Map all extracted features to acronyms\n",
        "*   Export IRR spreadsheet for Cohen's *k*\n",
        "\n",
        "In the next notebook, we will:\n",
        "*   Calculate Cohen's k for the additional extracted features\n",
        "*   Export final consensus and spreadsheet\n",
        "\n",
        "The MMLTE survey project is a collaborative effor between Dr. Gautam Biswas, Clayton Cohn, Eduardo Davalos, Joyce Fonteles, Dr. Meiyi Ma, Caleb Vatral, and Hanchen (David) Wang."
      ],
      "metadata": {
        "id": "J01H3CtWqLU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Import"
      ],
      "metadata": {
        "id": "KQofVjXmqalW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdabTZPdpCs4",
        "outputId": "9b0334fa-d69e-4b8f-9cbe-b907860b2e20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "S18_PATH = \"drive/My Drive/Clayton/20230420_MMLTE/S18_Consensus.csv\""
      ],
      "metadata": {
        "id": "F7V8sEagq0wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display max rows, columns, column length\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "df = pd.read_csv(S18_PATH,header=0)\n",
        "\n",
        "for col in df:\n",
        "  if col not in {\"Analysis Results (w/ multimodal advantages)\",\"Reviewer Notes\"}:\n",
        "    assert not df[col].isnull().values.any(), print(col)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tuftYmRbrDV5",
        "outputId": "e12e332d-5137-4dc8-82d9-e18dffde360e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums Mapped Modalities Mapped Analysis Methods  \\\n",
              "0                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "1                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "2                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "3             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "4             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "\n",
              "  Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0                LATE                     MLPALA   \n",
              "1                LATE                     MLPALA   \n",
              "2                LATE                     MLPALA   \n",
              "3              HYBRID                       BJET   \n",
              "4              HYBRID                       BJET   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "\n",
              "  Environment Setting Environment Sub     Participant Structure  \\\n",
              "0            physical            STEM  individual, multi-person   \n",
              "1            physical            STEM  individual, multi-person   \n",
              "2            physical            STEM  individual, multi-person   \n",
              "3            physical            STEM              multi-person   \n",
              "4            physical            STEM              multi-person   \n",
              "\n",
              "  Didactic Nature Level of Instruction or Training Analysis Approach  \\\n",
              "0   instructional                      unspecified       model-based   \n",
              "1   instructional                      unspecified       model-based   \n",
              "2   instructional                      unspecified       model-based   \n",
              "3   instructional                             K-12       model-based   \n",
              "4   instructional                             K-12       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-280f8f1f-a7d1-4dfb-84d2-7fd14e37f906\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Sub</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>physical</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>physical</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>physical</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>physical</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>physical</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-280f8f1f-a7d1-4dfb-84d2-7fd14e37f906')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-280f8f1f-a7d1-4dfb-84d2-7fd14e37f906 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-280f8f1f-a7d1-4dfb-84d2-7fd14e37f906');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-21e0938a-02e7-4d4f-91e5-c40b54d7ccbd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-21e0938a-02e7-4d4f-91e5-c40b54d7ccbd')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-21e0938a-02e7-4d4f-91e5-c40b54d7ccbd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Map to Acronyms"
      ],
      "metadata": {
        "id": "sq-5S8zItEr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Setting"
      ],
      "metadata": {
        "id": "eLiKIi0QtHmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_setting_vals = set()\n",
        "env_setting_list = [l.split(\", \") for l in list(df[\"Environment Setting\"])]\n",
        "for item in env_setting_list:\n",
        "  for val in item:\n",
        "    env_setting_vals.add(val.lower())\n",
        "\n",
        "env_setting_vals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7cKYDIgtLZJ",
        "outputId": "92584373-2df4-4fd3-b420-b11fb7d72ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'blended', 'physical', 'unspecified', 'virtual', 'virutal'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_setting_map = {\n",
        "    \"blended\": \"BLND\",\n",
        "    \"physical\": \"PHYS\",\n",
        "    \"virtual\": \"VIRT\",\n",
        "    \"virutal\": \"VIRT\",\n",
        "    \"unspecified\": \"UNSP\"\n",
        "}"
      ],
      "metadata": {
        "id": "xnQs3ZWDu5Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_env_setting_list = []\n",
        "\n",
        "for item in env_setting_list:\n",
        "  new_item = set()\n",
        "  for val in item:\n",
        "    new_item.add(env_setting_map[val.lower()])\n",
        "  new_env_setting_list.append(list(new_item))\n",
        "\n",
        "new_env_setting_list[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26h3gmGhtLbC",
        "outputId": "b2306c6a-ce71-4106-e0bf-48835c6c0d0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['PHYS'], ['PHYS'], ['PHYS'], ['PHYS'], ['PHYS']]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_env_setting_list_combined = []\n",
        "\n",
        "for item in new_env_setting_list:\n",
        "  item = sorted(item)\n",
        "  new_item = \", \".join(item)\n",
        "  new_env_setting_list_combined.append(new_item)\n",
        "\n",
        "new_env_setting_list_combined[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyauyk1UtG9d",
        "outputId": "ffd3104a-bfdc-4d97-f0a0-fc1b01bb1fcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PHYS', 'PHYS', 'PHYS', 'PHYS', 'PHYS']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.insert(13, \"Environment Setting (mapped)\", new_env_setting_list_combined)\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZPE0OIAuyCn4",
        "outputId": "0c40cad0-98b8-48b5-8c80-220be32d78b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "5  1469065963   \n",
              "6  1598166515   \n",
              "7  1598166515   \n",
              "8  1598166515   \n",
              "9  1877483551   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "5  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "6                                                            multimodal learning analytics for game-based learning   \n",
              "7                                                            multimodal learning analytics for game-based learning   \n",
              "8                                                            multimodal learning analytics for game-based learning   \n",
              "9                           motion-based educational games: using multi-modal data to predict player’s performance   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "5            Andy Nguyen  2022                                Learning   \n",
              "6         Andrew Emerson  2020                                Learning   \n",
              "7         Andrew Emerson  2020                                Learning   \n",
              "8         Andrew Emerson  2020                                Learning   \n",
              "9     Serena Lee-Cultura  2020                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums         Mapped Modalities  \\\n",
              "0                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "1                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "2                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "3             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "4             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "5             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "6                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "7                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "8                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "9               VIDEO,EYE,SENSOR  PULSE,TEMP,EDA,GAZE,POSE   \n",
              "\n",
              "  Mapped Analysis Methods Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0               CLS,CLUST                LATE                     MLPALA   \n",
              "1               CLS,CLUST                LATE                     MLPALA   \n",
              "2               CLS,CLUST                LATE                     MLPALA   \n",
              "3          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "4          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "5          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "6               CLS,STATS                 MID                       BJET   \n",
              "7               CLS,STATS                 MID                       BJET   \n",
              "8               CLS,STATS                 MID                       BJET   \n",
              "9                     CLS                 MID                        COG   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "5                   British Journal of Educational Technology            4   \n",
              "6                   British Journal of Educational Technology            5   \n",
              "7                   British Journal of Educational Technology            5   \n",
              "8                   British Journal of Educational Technology            5   \n",
              "9                                    IEEE Conference on Games            6   \n",
              "\n",
              "  Environment Setting Environment Setting (mapped) Environment Sub  \\\n",
              "0            physical                         PHYS            STEM   \n",
              "1            physical                         PHYS            STEM   \n",
              "2            physical                         PHYS            STEM   \n",
              "3            physical                         PHYS            STEM   \n",
              "4            physical                         PHYS            STEM   \n",
              "5            physical                         PHYS            STEM   \n",
              "6             virtual                         VIRT            STEM   \n",
              "7             virtual                         VIRT            STEM   \n",
              "8             virtual                         VIRT            STEM   \n",
              "9             blended                         BLND            STEM   \n",
              "\n",
              "      Participant Structure Didactic Nature Level of Instruction or Training  \\\n",
              "0  individual, multi-person   instructional                      unspecified   \n",
              "1  individual, multi-person   instructional                      unspecified   \n",
              "2  individual, multi-person   instructional                      unspecified   \n",
              "3              multi-person   instructional                             K-12   \n",
              "4              multi-person   instructional                             K-12   \n",
              "5              multi-person   instructional                             K-12   \n",
              "6                individual        informal                    undergraduate   \n",
              "7                individual        informal                    undergraduate   \n",
              "8                individual        informal                    undergraduate   \n",
              "9                individual   instructional                             K-12   \n",
              "\n",
              "  Analysis Approach  \\\n",
              "0       model-based   \n",
              "1       model-based   \n",
              "2       model-based   \n",
              "3       model-based   \n",
              "4       model-based   \n",
              "5       model-based   \n",
              "6       model-based   \n",
              "7       model-based   \n",
              "8       model-based   \n",
              "9       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "6                                                                                                       Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. The findings suggest that MMLA can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding   \n",
              "7                                                                                                                                                                                                                                                                                                                                                              Common case of multimodal outperform unimodal models, through the addition of gaze to classify student's posttest performance and interest.   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "9                                                                                                                                                                                           Authors conclude that the feature combination of gaze and physiological MMD provide the most accurate predictions of correct answers. They also show the feasibility of early prediction of children's performance by using half (as oppose to full) data lengths to extract features and predict correctness.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  \n",
              "5             Joyce/Eduardo      1&2            NaN  \n",
              "6                     Joyce        1            NaN  \n",
              "7                   Eduardo        2            NaN  \n",
              "8             Joyce/Eduardo      1&2            NaN  \n",
              "9                     Joyce        1            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d6f8559a-b075-49b8-a4fd-d2728a26cbfe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Setting (mapped)</th>\n",
              "      <th>Environment Sub</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>physical</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>physical</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>physical</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>physical</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>physical</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>physical</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>virtual</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual</td>\n",
              "      <td>informal</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. The findings suggest that MMLA can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>virtual</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual</td>\n",
              "      <td>informal</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Common case of multimodal outperform unimodal models, through the addition of gaze to classify student's posttest performance and interest.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>virtual</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual</td>\n",
              "      <td>informal</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1877483551</td>\n",
              "      <td>motion-based educational games: using multi-modal data to predict player’s performance</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR</td>\n",
              "      <td>PULSE,TEMP,EDA,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>COG</td>\n",
              "      <td>IEEE Conference on Games</td>\n",
              "      <td>6</td>\n",
              "      <td>blended</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Authors conclude that the feature combination of gaze and physiological MMD provide the most accurate predictions of correct answers. They also show the feasibility of early prediction of children's performance by using half (as oppose to full) data lengths to extract features and predict correctness.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6f8559a-b075-49b8-a4fd-d2728a26cbfe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d6f8559a-b075-49b8-a4fd-d2728a26cbfe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d6f8559a-b075-49b8-a4fd-d2728a26cbfe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d851929c-b861-4759-a5db-c7d87dd8fd80\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d851929c-b861-4759-a5db-c7d87dd8fd80')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d851929c-b861-4759-a5db-c7d87dd8fd80 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=[\"Environment Setting\"], inplace=True)\n",
        "df.rename(columns={'Environment Setting (mapped)': 'Environment Setting'},inplace=True)\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zCtREdT_tP56",
        "outputId": "cff79d7d-e912-4d9d-8542-55220bc051aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums Mapped Modalities Mapped Analysis Methods  \\\n",
              "0                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "1                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "2                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "3             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "4             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "\n",
              "  Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0                LATE                     MLPALA   \n",
              "1                LATE                     MLPALA   \n",
              "2                LATE                     MLPALA   \n",
              "3              HYBRID                       BJET   \n",
              "4              HYBRID                       BJET   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "\n",
              "  Environment Setting Environment Sub     Participant Structure  \\\n",
              "0                PHYS            STEM  individual, multi-person   \n",
              "1                PHYS            STEM  individual, multi-person   \n",
              "2                PHYS            STEM  individual, multi-person   \n",
              "3                PHYS            STEM              multi-person   \n",
              "4                PHYS            STEM              multi-person   \n",
              "\n",
              "  Didactic Nature Level of Instruction or Training Analysis Approach  \\\n",
              "0   instructional                      unspecified       model-based   \n",
              "1   instructional                      unspecified       model-based   \n",
              "2   instructional                      unspecified       model-based   \n",
              "3   instructional                             K-12       model-based   \n",
              "4   instructional                             K-12       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7faf8d37-68e5-4ab0-af32-72458617c159\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Sub</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7faf8d37-68e5-4ab0-af32-72458617c159')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7faf8d37-68e5-4ab0-af32-72458617c159 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7faf8d37-68e5-4ab0-af32-72458617c159');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2419d79f-4823-426a-b34b-f227f020450a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2419d79f-4823-426a-b34b-f227f020450a')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2419d79f-4823-426a-b34b-f227f020450a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Subject"
      ],
      "metadata": {
        "id": "cPuO7WARtLy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_subject_vals = set()\n",
        "env_subject_list = [l.split(\", \") for l in list(df[\"Environment Sub\"])]\n",
        "for item in env_subject_list:\n",
        "  for val in item:\n",
        "    env_subject_vals.add(val.lower())\n",
        "\n",
        "env_subject_vals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec887dbf-3ff7-45b0-8ff2-a1fcea18c184",
        "id": "57z5Pp2A4H88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'humanities',\n",
              " 'language arts',\n",
              " 'other',\n",
              " 'psychomotor',\n",
              " 'psychomotor skills',\n",
              " 'stem',\n",
              " 'unspecified'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_subject_map = {\n",
        "    'humanities':\"HUM\",\n",
        "    'language arts':\"HUM\",\n",
        "    'other':\"OTH\",\n",
        "    'psychomotor':\"PSY\",\n",
        "    'psychomotor skills':\"PSY\",\n",
        "    'stem':\"STEM\",\n",
        "    'unspecified':\"UNSP\"\n",
        "}"
      ],
      "metadata": {
        "id": "1iAjUX594H8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_env_subject_list = []\n",
        "\n",
        "for item in env_subject_list:\n",
        "  new_item = set()\n",
        "  for val in item:\n",
        "    new_item.add(env_subject_map[val.lower()])\n",
        "  new_env_subject_list.append(list(new_item))\n",
        "\n",
        "new_env_subject_list[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4322c60e-6322-4129-c9bb-d7b4726dfd23",
        "id": "zQT-ez9F4H8-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['STEM'], ['STEM'], ['STEM'], ['STEM'], ['STEM']]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_env_subject_list_combined = []\n",
        "\n",
        "for item in new_env_subject_list:\n",
        "  item = sorted(item)\n",
        "  new_item = \", \".join(item)\n",
        "  new_env_subject_list_combined.append(new_item)\n",
        "\n",
        "new_env_subject_list_combined[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a90ad0-32e8-4dc0-9732-fd0c98528f6e",
        "id": "PZaYb6IW4H8-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['STEM', 'STEM', 'STEM', 'STEM', 'STEM']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.insert(14, \"Environment Subject (mapped)\", new_env_subject_list_combined)\n",
        "df[15:25]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c5bb0a7-6c91-4d7a-db3b-5cc85f1a483f",
        "id": "YKV6OmPd4H8_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          UUID  \\\n",
              "15  2070224207   \n",
              "16  2070224207   \n",
              "17  2070224207   \n",
              "18  2634033325   \n",
              "19  2634033325   \n",
              "20  2634033325   \n",
              "21  3051560548   \n",
              "22  3051560548   \n",
              "23  3051560548   \n",
              "24  3339002981   \n",
              "\n",
              "                                                                                                          Title  \\\n",
              "15                                detecting medical simulation errors with machine learning and multimodal data   \n",
              "16                                detecting medical simulation errors with machine learning and multimodal data   \n",
              "17                                detecting medical simulation errors with machine learning and multimodal data   \n",
              "18  controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "19  controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "20  controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "21                              temporal analysis of multimodal data to predict collaborative learning outcomes   \n",
              "22                              temporal analysis of multimodal data to predict collaborative learning outcomes   \n",
              "23                              temporal analysis of multimodal data to predict collaborative learning outcomes   \n",
              "24              estimation of success in collaborative learning based on multimodal learning analytics features   \n",
              "\n",
              "   Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "15    Daniele Di Mitri  2019                                Training   \n",
              "16    Daniele Di Mitri  2019                                Training   \n",
              "17    Daniele Di Mitri  2019                                Training   \n",
              "18        Xavier Ochoa  2020                                Training   \n",
              "19        Xavier Ochoa  2020                                Training   \n",
              "20        Xavier Ochoa  2020                                Training   \n",
              "21   Jennifer K. Olsen  2020                                Learning   \n",
              "22   Jennifer K. Olsen  2020                                Learning   \n",
              "23   Jennifer K. Olsen  2020                                Learning   \n",
              "24       Daniel Spikol  2017                                Learning   \n",
              "\n",
              "   Mapped Data Collection Mediums          Mapped Modalities  \\\n",
              "15              VIDEO,MOTION,LOGS                  POSE,LOGS   \n",
              "16              VIDEO,MOTION,LOGS                  POSE,LOGS   \n",
              "17              VIDEO,MOTION,LOGS                  POSE,LOGS   \n",
              "18                VIDEO,AUDIO,PPA              POSE,PROS,PPA   \n",
              "19                VIDEO,AUDIO,PPA              POSE,PROS,PPA   \n",
              "20                VIDEO,AUDIO,PPA              POSE,PROS,PPA   \n",
              "21                 LOGS,AUDIO,EYE  GAZE,LOGS,PROS,TRANS,QUAL   \n",
              "22                 LOGS,AUDIO,EYE  GAZE,LOGS,PROS,TRANS,QUAL   \n",
              "23                 LOGS,AUDIO,EYE  GAZE,LOGS,PROS,TRANS,QUAL   \n",
              "24           EYE,LOGS,VIDEO,AUDIO        GAZE,LOGS,PROS,POSE   \n",
              "\n",
              "   Mapped Analysis Methods Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "15                     CLS                 MID                       CAIM   \n",
              "16                     CLS                 MID                       CAIM   \n",
              "17                     CLS                 MID                       CAIM   \n",
              "18                   STATS                 OTH                       BJET   \n",
              "19                   STATS                 OTH                       BJET   \n",
              "20                   STATS                 OTH                       BJET   \n",
              "21                     REG                 MID                       BJET   \n",
              "22                     REG                 MID                       BJET   \n",
              "23                     REG                 MID                       BJET   \n",
              "24                     CLS                 MID                      ICALT   \n",
              "\n",
              "                                       Mapped Full Publication  Sort Number  \\\n",
              "15           Conference on Artificial Intelligence in Medicine           11   \n",
              "16           Conference on Artificial Intelligence in Medicine           11   \n",
              "17           Conference on Artificial Intelligence in Medicine           11   \n",
              "18                   British Journal of Educational Technology           12   \n",
              "19                   British Journal of Educational Technology           12   \n",
              "20                   British Journal of Educational Technology           12   \n",
              "21                   British Journal of Educational Technology           13   \n",
              "22                   British Journal of Educational Technology           13   \n",
              "23                   British Journal of Educational Technology           13   \n",
              "24  International Conference on Advanced Learning Technologies           14   \n",
              "\n",
              "   Environment Setting     Environment Sub Environment Subject (mapped)  \\\n",
              "15                BLND  psychomotor skills                          PSY   \n",
              "16                BLND  psychomotor skills                          PSY   \n",
              "17                BLND  psychomotor skills                          PSY   \n",
              "18                BLND          humanities                          HUM   \n",
              "19                BLND          humanities                          HUM   \n",
              "20                BLND          humanities                          HUM   \n",
              "21                VIRT                STEM                         STEM   \n",
              "22                VIRT                STEM                         STEM   \n",
              "23                VIRT                STEM                         STEM   \n",
              "24                VIRT                STEM                         STEM   \n",
              "\n",
              "   Participant Structure Didactic Nature Level of Instruction or Training  \\\n",
              "15            individual        training                    undergraduate   \n",
              "16            individual        training                    undergraduate   \n",
              "17            individual        training                    undergraduate   \n",
              "18            individual        informal                      unspecified   \n",
              "19            individual        training                      unspecified   \n",
              "20            individual        training                      unspecified   \n",
              "21          multi-person   instructional                             K-12   \n",
              "22          multi-person   instructional                             K-12   \n",
              "23          multi-person   instructional                             K-12   \n",
              "24          multi-person   instructional                    undergraduate   \n",
              "\n",
              "   Analysis Approach  \\\n",
              "15       model-based   \n",
              "16       model-based   \n",
              "17       model-based   \n",
              "18        model-free   \n",
              "19        model-free   \n",
              "20        model-free   \n",
              "21       model-based   \n",
              "22       model-based   \n",
              "23       model-based   \n",
              "24       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "15  Used each Chest Compression as training sample by masking/windowing of the original time series, then trained an LSTM network with all these samples and were able to classify accurately the target classes, however discarding the rest of the time-series they were not able to detect if a CC happened. Author asks Doctorial Consortium how, given the available data, could they train a classifier able to detect whether a CC happened or not.   \n",
              "16                                                                                                                                                                                                                                                                                                                               Trained an LSTM to predict ['too slow', 'on-point', 'too fast'] for Chest compression training. Achieved 70-75% accuracy.   \n",
              "17                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
              "18                      Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoidance of filled pauses)   \n",
              "19                                                                                                                                                                                                                                                                                                                                 Authors showcase that the training tool improved manually defined scores between an initial and second use of the tool.   \n",
              "20                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
              "21                                                                                                                                                Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.   \n",
              "22                                                                                                                                                                                                                                                                       Evaluating how multimodal features contribute to a model's performance to predict learning gains. Audio features introduce noise that negatively impacted the error of the model.   \n",
              "23                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
              "24                                                                                                                                          Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, authors found that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.   \n",
              "\n",
              "   Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "15                     Joyce        1            NaN  \n",
              "16                   Eduardo        2            NaN  \n",
              "17             Joyce/Eduardo      1&2            NaN  \n",
              "18                     Joyce        1            NaN  \n",
              "19                   Eduardo        2            NaN  \n",
              "20             Joyce/Eduardo      1&2            NaN  \n",
              "21                     Joyce        1            NaN  \n",
              "22                   Eduardo        2            NaN  \n",
              "23             Joyce/Eduardo      1&2            NaN  \n",
              "24                     Joyce        1            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-78759c8f-752e-4b8b-904a-11d5fa230231\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Sub</th>\n",
              "      <th>Environment Subject (mapped)</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2070224207</td>\n",
              "      <td>detecting medical simulation errors with machine learning and multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,LOGS</td>\n",
              "      <td>POSE,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>CAIM</td>\n",
              "      <td>Conference on Artificial Intelligence in Medicine</td>\n",
              "      <td>11</td>\n",
              "      <td>BLND</td>\n",
              "      <td>psychomotor skills</td>\n",
              "      <td>PSY</td>\n",
              "      <td>individual</td>\n",
              "      <td>training</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Used each Chest Compression as training sample by masking/windowing of the original time series, then trained an LSTM network with all these samples and were able to classify accurately the target classes, however discarding the rest of the time-series they were not able to detect if a CC happened. Author asks Doctorial Consortium how, given the available data, could they train a classifier able to detect whether a CC happened or not.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2070224207</td>\n",
              "      <td>detecting medical simulation errors with machine learning and multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,LOGS</td>\n",
              "      <td>POSE,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>CAIM</td>\n",
              "      <td>Conference on Artificial Intelligence in Medicine</td>\n",
              "      <td>11</td>\n",
              "      <td>BLND</td>\n",
              "      <td>psychomotor skills</td>\n",
              "      <td>PSY</td>\n",
              "      <td>individual</td>\n",
              "      <td>training</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Trained an LSTM to predict ['too slow', 'on-point', 'too fast'] for Chest compression training. Achieved 70-75% accuracy.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2070224207</td>\n",
              "      <td>detecting medical simulation errors with machine learning and multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,LOGS</td>\n",
              "      <td>POSE,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>CAIM</td>\n",
              "      <td>Conference on Artificial Intelligence in Medicine</td>\n",
              "      <td>11</td>\n",
              "      <td>BLND</td>\n",
              "      <td>psychomotor skills</td>\n",
              "      <td>PSY</td>\n",
              "      <td>individual</td>\n",
              "      <td>training</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>12</td>\n",
              "      <td>BLND</td>\n",
              "      <td>humanities</td>\n",
              "      <td>HUM</td>\n",
              "      <td>individual</td>\n",
              "      <td>informal</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-free</td>\n",
              "      <td>Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoidance of filled pauses)</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>12</td>\n",
              "      <td>BLND</td>\n",
              "      <td>humanities</td>\n",
              "      <td>HUM</td>\n",
              "      <td>individual</td>\n",
              "      <td>training</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-free</td>\n",
              "      <td>Authors showcase that the training tool improved manually defined scores between an initial and second use of the tool.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>12</td>\n",
              "      <td>BLND</td>\n",
              "      <td>humanities</td>\n",
              "      <td>HUM</td>\n",
              "      <td>individual</td>\n",
              "      <td>training</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-free</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3051560548</td>\n",
              "      <td>temporal analysis of multimodal data to predict collaborative learning outcomes</td>\n",
              "      <td>Jennifer K. Olsen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,EYE</td>\n",
              "      <td>GAZE,LOGS,PROS,TRANS,QUAL</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>13</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3051560548</td>\n",
              "      <td>temporal analysis of multimodal data to predict collaborative learning outcomes</td>\n",
              "      <td>Jennifer K. Olsen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,EYE</td>\n",
              "      <td>GAZE,LOGS,PROS,TRANS,QUAL</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>13</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Evaluating how multimodal features contribute to a model's performance to predict learning gains. Audio features introduce noise that negatively impacted the error of the model.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3051560548</td>\n",
              "      <td>temporal analysis of multimodal data to predict collaborative learning outcomes</td>\n",
              "      <td>Jennifer K. Olsen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,EYE</td>\n",
              "      <td>GAZE,LOGS,PROS,TRANS,QUAL</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>13</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>3339002981</td>\n",
              "      <td>estimation of success in collaborative learning based on multimodal learning analytics features</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,LOGS,VIDEO,AUDIO</td>\n",
              "      <td>GAZE,LOGS,PROS,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICALT</td>\n",
              "      <td>International Conference on Advanced Learning Technologies</td>\n",
              "      <td>14</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, authors found that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-78759c8f-752e-4b8b-904a-11d5fa230231')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-78759c8f-752e-4b8b-904a-11d5fa230231 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-78759c8f-752e-4b8b-904a-11d5fa230231');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-30454503-2264-43f7-8c0d-7c6a104c2499\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-30454503-2264-43f7-8c0d-7c6a104c2499')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-30454503-2264-43f7-8c0d-7c6a104c2499 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=[\"Environment Sub\"], inplace=True)\n",
        "df.rename(columns={'Environment Subject (mapped)': 'Environment Subject'},inplace=True)\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "78DzU2sltP8c",
        "outputId": "1b964c78-c4e2-4bd9-abd8-926201fa28b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums Mapped Modalities Mapped Analysis Methods  \\\n",
              "0                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "1                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "2                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "3             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "4             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "\n",
              "  Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0                LATE                     MLPALA   \n",
              "1                LATE                     MLPALA   \n",
              "2                LATE                     MLPALA   \n",
              "3              HYBRID                       BJET   \n",
              "4              HYBRID                       BJET   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "\n",
              "  Environment Setting Environment Subject     Participant Structure  \\\n",
              "0                PHYS                STEM  individual, multi-person   \n",
              "1                PHYS                STEM  individual, multi-person   \n",
              "2                PHYS                STEM  individual, multi-person   \n",
              "3                PHYS                STEM              multi-person   \n",
              "4                PHYS                STEM              multi-person   \n",
              "\n",
              "  Didactic Nature Level of Instruction or Training Analysis Approach  \\\n",
              "0   instructional                      unspecified       model-based   \n",
              "1   instructional                      unspecified       model-based   \n",
              "2   instructional                      unspecified       model-based   \n",
              "3   instructional                             K-12       model-based   \n",
              "4   instructional                             K-12       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e0a7fe8d-30de-4666-8d56-b200cc7bdecd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e0a7fe8d-30de-4666-8d56-b200cc7bdecd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e0a7fe8d-30de-4666-8d56-b200cc7bdecd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e0a7fe8d-30de-4666-8d56-b200cc7bdecd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-661205d2-b792-4545-bfa8-ca45e34d1e1a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-661205d2-b792-4545-bfa8-ca45e34d1e1a')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-661205d2-b792-4545-bfa8-ca45e34d1e1a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Participant Structure"
      ],
      "metadata": {
        "id": "N5bV35kitRJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "participant_vals = set()\n",
        "participant_list = [l.split(\", \") for l in list(df[\"Participant Structure\"])]\n",
        "for item in participant_list:\n",
        "  for val in item:\n",
        "    participant_vals.add(val.lower())\n",
        "\n",
        "participant_vals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76183a93-52fb-42df-d32e-306bd592be79",
        "id": "pztdO6nB8Yve"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'individual', 'multi-person', 'multi-student', 'mutli-person'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "participant_map = {\n",
        "    'individual':\"IND\",\n",
        "    'multi-person':\"MULTI\",\n",
        "    'multi-student':\"MULTI\",\n",
        "    'mutli-person':\"MULTI\"\n",
        "}"
      ],
      "metadata": {
        "id": "U4-JSx0k8Yvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_participant_list = []\n",
        "\n",
        "for item in participant_list:\n",
        "  new_item = set()\n",
        "  for val in item:\n",
        "    new_item.add(participant_map[val.lower()])\n",
        "  new_participant_list.append(list(new_item))\n",
        "\n",
        "new_participant_list[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73f5f071-8430-4ec2-84db-ece070e72fcc",
        "id": "Tl5T9KqP8Yvg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['IND', 'MULTI'], ['IND', 'MULTI'], ['IND', 'MULTI'], ['MULTI'], ['MULTI']]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_participant_list_combined = []\n",
        "\n",
        "for item in new_participant_list:\n",
        "  item = sorted(item)\n",
        "  new_item = \", \".join(item)\n",
        "  new_participant_list_combined.append(new_item)\n",
        "\n",
        "new_participant_list_combined[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8c58420-58eb-40f7-9b7b-719975609aae",
        "id": "IQMwaqq58Yvg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['IND, MULTI', 'IND, MULTI', 'IND, MULTI', 'MULTI', 'MULTI']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.insert(15, \"Participant Structure (mapped)\", new_participant_list_combined)\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "944ed71c-2c78-40ae-8f60-35a604becaee",
        "id": "lqQoAUD78Yvg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "5  1469065963   \n",
              "6  1598166515   \n",
              "7  1598166515   \n",
              "8  1598166515   \n",
              "9  1877483551   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "5  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "6                                                            multimodal learning analytics for game-based learning   \n",
              "7                                                            multimodal learning analytics for game-based learning   \n",
              "8                                                            multimodal learning analytics for game-based learning   \n",
              "9                           motion-based educational games: using multi-modal data to predict player’s performance   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "5            Andy Nguyen  2022                                Learning   \n",
              "6         Andrew Emerson  2020                                Learning   \n",
              "7         Andrew Emerson  2020                                Learning   \n",
              "8         Andrew Emerson  2020                                Learning   \n",
              "9     Serena Lee-Cultura  2020                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums         Mapped Modalities  \\\n",
              "0                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "1                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "2                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "3             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "4             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "5             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "6                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "7                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "8                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "9               VIDEO,EYE,SENSOR  PULSE,TEMP,EDA,GAZE,POSE   \n",
              "\n",
              "  Mapped Analysis Methods Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0               CLS,CLUST                LATE                     MLPALA   \n",
              "1               CLS,CLUST                LATE                     MLPALA   \n",
              "2               CLS,CLUST                LATE                     MLPALA   \n",
              "3          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "4          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "5          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "6               CLS,STATS                 MID                       BJET   \n",
              "7               CLS,STATS                 MID                       BJET   \n",
              "8               CLS,STATS                 MID                       BJET   \n",
              "9                     CLS                 MID                        COG   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "5                   British Journal of Educational Technology            4   \n",
              "6                   British Journal of Educational Technology            5   \n",
              "7                   British Journal of Educational Technology            5   \n",
              "8                   British Journal of Educational Technology            5   \n",
              "9                                    IEEE Conference on Games            6   \n",
              "\n",
              "  Environment Setting Environment Subject     Participant Structure  \\\n",
              "0                PHYS                STEM  individual, multi-person   \n",
              "1                PHYS                STEM  individual, multi-person   \n",
              "2                PHYS                STEM  individual, multi-person   \n",
              "3                PHYS                STEM              multi-person   \n",
              "4                PHYS                STEM              multi-person   \n",
              "5                PHYS                STEM              multi-person   \n",
              "6                VIRT                STEM                individual   \n",
              "7                VIRT                STEM                individual   \n",
              "8                VIRT                STEM                individual   \n",
              "9                BLND                STEM                individual   \n",
              "\n",
              "  Participant Structure (mapped) Didactic Nature  \\\n",
              "0                     IND, MULTI   instructional   \n",
              "1                     IND, MULTI   instructional   \n",
              "2                     IND, MULTI   instructional   \n",
              "3                          MULTI   instructional   \n",
              "4                          MULTI   instructional   \n",
              "5                          MULTI   instructional   \n",
              "6                            IND        informal   \n",
              "7                            IND        informal   \n",
              "8                            IND        informal   \n",
              "9                            IND   instructional   \n",
              "\n",
              "  Level of Instruction or Training Analysis Approach  \\\n",
              "0                      unspecified       model-based   \n",
              "1                      unspecified       model-based   \n",
              "2                      unspecified       model-based   \n",
              "3                             K-12       model-based   \n",
              "4                             K-12       model-based   \n",
              "5                             K-12       model-based   \n",
              "6                    undergraduate       model-based   \n",
              "7                    undergraduate       model-based   \n",
              "8                    undergraduate       model-based   \n",
              "9                             K-12       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "6                                                                                                       Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. The findings suggest that MMLA can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding   \n",
              "7                                                                                                                                                                                                                                                                                                                                                              Common case of multimodal outperform unimodal models, through the addition of gaze to classify student's posttest performance and interest.   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "9                                                                                                                                                                                           Authors conclude that the feature combination of gaze and physiological MMD provide the most accurate predictions of correct answers. They also show the feasibility of early prediction of children's performance by using half (as oppose to full) data lengths to extract features and predict correctness.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  \n",
              "5             Joyce/Eduardo      1&2            NaN  \n",
              "6                     Joyce        1            NaN  \n",
              "7                   Eduardo        2            NaN  \n",
              "8             Joyce/Eduardo      1&2            NaN  \n",
              "9                     Joyce        1            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1fdebf6-59fb-48e0-9d7f-9d1510e500c2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Participant Structure (mapped)</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual</td>\n",
              "      <td>IND</td>\n",
              "      <td>informal</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. The findings suggest that MMLA can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual</td>\n",
              "      <td>IND</td>\n",
              "      <td>informal</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Common case of multimodal outperform unimodal models, through the addition of gaze to classify student's posttest performance and interest.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual</td>\n",
              "      <td>IND</td>\n",
              "      <td>informal</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1877483551</td>\n",
              "      <td>motion-based educational games: using multi-modal data to predict player’s performance</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR</td>\n",
              "      <td>PULSE,TEMP,EDA,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>COG</td>\n",
              "      <td>IEEE Conference on Games</td>\n",
              "      <td>6</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual</td>\n",
              "      <td>IND</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Authors conclude that the feature combination of gaze and physiological MMD provide the most accurate predictions of correct answers. They also show the feasibility of early prediction of children's performance by using half (as oppose to full) data lengths to extract features and predict correctness.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1fdebf6-59fb-48e0-9d7f-9d1510e500c2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e1fdebf6-59fb-48e0-9d7f-9d1510e500c2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e1fdebf6-59fb-48e0-9d7f-9d1510e500c2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a4cbc984-13cd-438b-a29d-3a442048066d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a4cbc984-13cd-438b-a29d-3a442048066d')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a4cbc984-13cd-438b-a29d-3a442048066d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=[\"Participant Structure\"], inplace=True)\n",
        "df.rename(columns={'Participant Structure (mapped)': 'Participant Structure'},inplace=True)\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b2e4d6b-9377-4671-e3cd-154e1de0266c",
        "id": "AbuYCPAp8Yvh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums Mapped Modalities Mapped Analysis Methods  \\\n",
              "0                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "1                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "2                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "3             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "4             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "\n",
              "  Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0                LATE                     MLPALA   \n",
              "1                LATE                     MLPALA   \n",
              "2                LATE                     MLPALA   \n",
              "3              HYBRID                       BJET   \n",
              "4              HYBRID                       BJET   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "\n",
              "  Environment Setting Environment Subject Participant Structure  \\\n",
              "0                PHYS                STEM            IND, MULTI   \n",
              "1                PHYS                STEM            IND, MULTI   \n",
              "2                PHYS                STEM            IND, MULTI   \n",
              "3                PHYS                STEM                 MULTI   \n",
              "4                PHYS                STEM                 MULTI   \n",
              "\n",
              "  Didactic Nature Level of Instruction or Training Analysis Approach  \\\n",
              "0   instructional                      unspecified       model-based   \n",
              "1   instructional                      unspecified       model-based   \n",
              "2   instructional                      unspecified       model-based   \n",
              "3   instructional                             K-12       model-based   \n",
              "4   instructional                             K-12       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fcc6a73e-8794-4c47-b61e-094aa13ad80d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fcc6a73e-8794-4c47-b61e-094aa13ad80d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fcc6a73e-8794-4c47-b61e-094aa13ad80d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fcc6a73e-8794-4c47-b61e-094aa13ad80d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-806eb0f8-32f9-4d42-9f98-8ff088749dc7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-806eb0f8-32f9-4d42-9f98-8ff088749dc7')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-806eb0f8-32f9-4d42-9f98-8ff088749dc7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Didactic Nature"
      ],
      "metadata": {
        "id": "FnISC1-4tUsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "didactic_nature_vals = set()\n",
        "didactic_nature_list = [l.split(\", \") for l in list(df[\"Didactic Nature\"])]\n",
        "for item in didactic_nature_list:\n",
        "  for val in item:\n",
        "    didactic_nature_vals.add(val.lower())\n",
        "\n",
        "didactic_nature_vals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abeb15cc-2005-4868-850c-f8b1f3c290ef",
        "id": "qDvGwBXSBgns"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'informal',\n",
              " 'instructional',\n",
              " 'instrutional',\n",
              " 'insturctional',\n",
              " 'training',\n",
              " 'unspecified'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "didactic_nature_map = {\n",
        "    'informal':\"INF\",\n",
        "    'instructional':\"INSTR\",\n",
        "    'instrutional':\"INSTR\",\n",
        "    'insturctional':\"INSTR\",\n",
        "    'training':\"TRAIN\",\n",
        "    'unspecified':\"UNSP\"\n",
        "}"
      ],
      "metadata": {
        "id": "6Qv_m16LBgnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_didactic_nature_list = []\n",
        "\n",
        "for item in didactic_nature_list:\n",
        "  new_item = set()\n",
        "  for val in item:\n",
        "    new_item.add(didactic_nature_map[val.lower()])\n",
        "  new_didactic_nature_list.append(list(new_item))\n",
        "\n",
        "new_didactic_nature_list[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d8e5a60-1fbc-4e6b-eda9-6b8d8110cffb",
        "id": "GP9sTeVgBgnt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['INSTR'], ['INSTR'], ['INSTR'], ['INSTR'], ['INSTR']]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_didactic_nature_list_combined = []\n",
        "\n",
        "for item in new_didactic_nature_list:\n",
        "  item = sorted(item)\n",
        "  new_item = \", \".join(item)\n",
        "  new_didactic_nature_list_combined.append(new_item)\n",
        "\n",
        "new_didactic_nature_list_combined[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f300903-e573-4870-aa40-6cdda451ec17",
        "id": "V_ccHOQCBgnt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['INSTR', 'INSTR', 'INSTR', 'INSTR', 'INSTR']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.insert(16, \"Didactic Nature (mapped)\", new_didactic_nature_list_combined)\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fb5ed168-a6a4-4b82-d251-d95ca0149f35",
        "id": "rwITA26kBgnu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "5  1469065963   \n",
              "6  1598166515   \n",
              "7  1598166515   \n",
              "8  1598166515   \n",
              "9  1877483551   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "5  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "6                                                            multimodal learning analytics for game-based learning   \n",
              "7                                                            multimodal learning analytics for game-based learning   \n",
              "8                                                            multimodal learning analytics for game-based learning   \n",
              "9                           motion-based educational games: using multi-modal data to predict player’s performance   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "5            Andy Nguyen  2022                                Learning   \n",
              "6         Andrew Emerson  2020                                Learning   \n",
              "7         Andrew Emerson  2020                                Learning   \n",
              "8         Andrew Emerson  2020                                Learning   \n",
              "9     Serena Lee-Cultura  2020                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums         Mapped Modalities  \\\n",
              "0                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "1                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "2                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "3             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "4             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "5             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "6                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "7                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "8                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "9               VIDEO,EYE,SENSOR  PULSE,TEMP,EDA,GAZE,POSE   \n",
              "\n",
              "  Mapped Analysis Methods Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0               CLS,CLUST                LATE                     MLPALA   \n",
              "1               CLS,CLUST                LATE                     MLPALA   \n",
              "2               CLS,CLUST                LATE                     MLPALA   \n",
              "3          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "4          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "5          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "6               CLS,STATS                 MID                       BJET   \n",
              "7               CLS,STATS                 MID                       BJET   \n",
              "8               CLS,STATS                 MID                       BJET   \n",
              "9                     CLS                 MID                        COG   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "5                   British Journal of Educational Technology            4   \n",
              "6                   British Journal of Educational Technology            5   \n",
              "7                   British Journal of Educational Technology            5   \n",
              "8                   British Journal of Educational Technology            5   \n",
              "9                                    IEEE Conference on Games            6   \n",
              "\n",
              "  Environment Setting Environment Subject Participant Structure  \\\n",
              "0                PHYS                STEM            IND, MULTI   \n",
              "1                PHYS                STEM            IND, MULTI   \n",
              "2                PHYS                STEM            IND, MULTI   \n",
              "3                PHYS                STEM                 MULTI   \n",
              "4                PHYS                STEM                 MULTI   \n",
              "5                PHYS                STEM                 MULTI   \n",
              "6                VIRT                STEM                   IND   \n",
              "7                VIRT                STEM                   IND   \n",
              "8                VIRT                STEM                   IND   \n",
              "9                BLND                STEM                   IND   \n",
              "\n",
              "  Didactic Nature Didactic Nature (mapped) Level of Instruction or Training  \\\n",
              "0   instructional                    INSTR                      unspecified   \n",
              "1   instructional                    INSTR                      unspecified   \n",
              "2   instructional                    INSTR                      unspecified   \n",
              "3   instructional                    INSTR                             K-12   \n",
              "4   instructional                    INSTR                             K-12   \n",
              "5   instructional                    INSTR                             K-12   \n",
              "6        informal                      INF                    undergraduate   \n",
              "7        informal                      INF                    undergraduate   \n",
              "8        informal                      INF                    undergraduate   \n",
              "9   instructional                    INSTR                             K-12   \n",
              "\n",
              "  Analysis Approach  \\\n",
              "0       model-based   \n",
              "1       model-based   \n",
              "2       model-based   \n",
              "3       model-based   \n",
              "4       model-based   \n",
              "5       model-based   \n",
              "6       model-based   \n",
              "7       model-based   \n",
              "8       model-based   \n",
              "9       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "6                                                                                                       Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. The findings suggest that MMLA can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding   \n",
              "7                                                                                                                                                                                                                                                                                                                                                              Common case of multimodal outperform unimodal models, through the addition of gaze to classify student's posttest performance and interest.   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "9                                                                                                                                                                                           Authors conclude that the feature combination of gaze and physiological MMD provide the most accurate predictions of correct answers. They also show the feasibility of early prediction of children's performance by using half (as oppose to full) data lengths to extract features and predict correctness.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  \n",
              "5             Joyce/Eduardo      1&2            NaN  \n",
              "6                     Joyce        1            NaN  \n",
              "7                   Eduardo        2            NaN  \n",
              "8             Joyce/Eduardo      1&2            NaN  \n",
              "9                     Joyce        1            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-834e317a-09be-435a-a277-6d75cf00db56\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Didactic Nature (mapped)</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>informal</td>\n",
              "      <td>INF</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. The findings suggest that MMLA can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>informal</td>\n",
              "      <td>INF</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Common case of multimodal outperform unimodal models, through the addition of gaze to classify student's posttest performance and interest.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>informal</td>\n",
              "      <td>INF</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1877483551</td>\n",
              "      <td>motion-based educational games: using multi-modal data to predict player’s performance</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR</td>\n",
              "      <td>PULSE,TEMP,EDA,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>COG</td>\n",
              "      <td>IEEE Conference on Games</td>\n",
              "      <td>6</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>instructional</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Authors conclude that the feature combination of gaze and physiological MMD provide the most accurate predictions of correct answers. They also show the feasibility of early prediction of children's performance by using half (as oppose to full) data lengths to extract features and predict correctness.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-834e317a-09be-435a-a277-6d75cf00db56')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-834e317a-09be-435a-a277-6d75cf00db56 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-834e317a-09be-435a-a277-6d75cf00db56');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7a745711-3818-4193-b459-08cb48c45e36\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7a745711-3818-4193-b459-08cb48c45e36')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7a745711-3818-4193-b459-08cb48c45e36 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=[\"Didactic Nature\"], inplace=True)\n",
        "df.rename(columns={'Didactic Nature (mapped)': 'Didactic Nature'},inplace=True)\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b0f27f50-fdf6-41f6-84d4-a3bee3bf6380",
        "id": "6JeUhJEyBgnu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums Mapped Modalities Mapped Analysis Methods  \\\n",
              "0                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "1                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "2                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "3             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "4             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "\n",
              "  Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0                LATE                     MLPALA   \n",
              "1                LATE                     MLPALA   \n",
              "2                LATE                     MLPALA   \n",
              "3              HYBRID                       BJET   \n",
              "4              HYBRID                       BJET   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "\n",
              "  Environment Setting Environment Subject Participant Structure  \\\n",
              "0                PHYS                STEM            IND, MULTI   \n",
              "1                PHYS                STEM            IND, MULTI   \n",
              "2                PHYS                STEM            IND, MULTI   \n",
              "3                PHYS                STEM                 MULTI   \n",
              "4                PHYS                STEM                 MULTI   \n",
              "\n",
              "  Didactic Nature Level of Instruction or Training Analysis Approach  \\\n",
              "0           INSTR                      unspecified       model-based   \n",
              "1           INSTR                      unspecified       model-based   \n",
              "2           INSTR                      unspecified       model-based   \n",
              "3           INSTR                             K-12       model-based   \n",
              "4           INSTR                             K-12       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cd4ba329-32d5-4722-bb41-7305106380dc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd4ba329-32d5-4722-bb41-7305106380dc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cd4ba329-32d5-4722-bb41-7305106380dc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cd4ba329-32d5-4722-bb41-7305106380dc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9208adb2-0eb8-44f9-8a6b-b03b54048838\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9208adb2-0eb8-44f9-8a6b-b03b54048838')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9208adb2-0eb8-44f9-8a6b-b03b54048838 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Level of Instruction or Training"
      ],
      "metadata": {
        "id": "bWm_2QFGtZMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "level_vals = set()\n",
        "level_list = [l.split(\", \") for l in list(df[\"Level of Instruction or Training\"])]\n",
        "for item in level_list:\n",
        "  for val in item:\n",
        "    level_vals.add(val.lower())\n",
        "\n",
        "level_vals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93058f7a-53d8-4dc9-fc26-b600aedd67db",
        "id": "g2apl3flFGtL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'graduate',\n",
              " 'k-12',\n",
              " 'k12',\n",
              " 'professional',\n",
              " 'professional development',\n",
              " 'undergraduate',\n",
              " 'undisclosed',\n",
              " 'university',\n",
              " 'unspecified'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "level_map = {\n",
        "    'graduate':\"UNI\",\n",
        "    'k-12':\"K12\",\n",
        "    'k12':\"K12\",\n",
        "    'professional':\"PROF\",\n",
        "    'professional development':\"PROF\",\n",
        "    'undergraduate':\"UNI\",\n",
        "    'undisclosed':\"UNSP\",\n",
        "    'university':\"UNI\",\n",
        "    'unspecified':\"UNSP\"\n",
        "}"
      ],
      "metadata": {
        "id": "-HO79ErmFGtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_level_list = []\n",
        "\n",
        "for item in level_list:\n",
        "  new_item = set()\n",
        "  for val in item:\n",
        "    new_item.add(level_map[val.lower()])\n",
        "  new_level_list.append(list(new_item))\n",
        "\n",
        "new_level_list[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "098bbcfd-604f-4c98-8ec1-ae70fa5ab144",
        "id": "lh0wbfmAFGtM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['UNSP'], ['UNSP'], ['UNSP'], ['K12'], ['K12']]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_level_list_combined = []\n",
        "\n",
        "for item in new_level_list:\n",
        "  item = sorted(item)\n",
        "  new_item = \", \".join(item)\n",
        "  new_level_list_combined.append(new_item)\n",
        "\n",
        "new_level_list_combined[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f9892cc-89d9-45de-d0c3-3a848057cfcd",
        "id": "V5kcXZSvFGtN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['UNSP', 'UNSP', 'UNSP', 'K12', 'K12']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.insert(17, \"Level of Instruction or Training (mapped)\", new_level_list_combined)\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f87ad26a-beaf-4c18-9037-a90c7658436b",
        "id": "u1RL2FJeFGtN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "5  1469065963   \n",
              "6  1598166515   \n",
              "7  1598166515   \n",
              "8  1598166515   \n",
              "9  1877483551   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "5  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "6                                                            multimodal learning analytics for game-based learning   \n",
              "7                                                            multimodal learning analytics for game-based learning   \n",
              "8                                                            multimodal learning analytics for game-based learning   \n",
              "9                           motion-based educational games: using multi-modal data to predict player’s performance   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "5            Andy Nguyen  2022                                Learning   \n",
              "6         Andrew Emerson  2020                                Learning   \n",
              "7         Andrew Emerson  2020                                Learning   \n",
              "8         Andrew Emerson  2020                                Learning   \n",
              "9     Serena Lee-Cultura  2020                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums         Mapped Modalities  \\\n",
              "0                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "1                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "2                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "3             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "4             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "5             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "6                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "7                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "8                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "9               VIDEO,EYE,SENSOR  PULSE,TEMP,EDA,GAZE,POSE   \n",
              "\n",
              "  Mapped Analysis Methods Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0               CLS,CLUST                LATE                     MLPALA   \n",
              "1               CLS,CLUST                LATE                     MLPALA   \n",
              "2               CLS,CLUST                LATE                     MLPALA   \n",
              "3          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "4          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "5          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "6               CLS,STATS                 MID                       BJET   \n",
              "7               CLS,STATS                 MID                       BJET   \n",
              "8               CLS,STATS                 MID                       BJET   \n",
              "9                     CLS                 MID                        COG   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "5                   British Journal of Educational Technology            4   \n",
              "6                   British Journal of Educational Technology            5   \n",
              "7                   British Journal of Educational Technology            5   \n",
              "8                   British Journal of Educational Technology            5   \n",
              "9                                    IEEE Conference on Games            6   \n",
              "\n",
              "  Environment Setting Environment Subject Participant Structure  \\\n",
              "0                PHYS                STEM            IND, MULTI   \n",
              "1                PHYS                STEM            IND, MULTI   \n",
              "2                PHYS                STEM            IND, MULTI   \n",
              "3                PHYS                STEM                 MULTI   \n",
              "4                PHYS                STEM                 MULTI   \n",
              "5                PHYS                STEM                 MULTI   \n",
              "6                VIRT                STEM                   IND   \n",
              "7                VIRT                STEM                   IND   \n",
              "8                VIRT                STEM                   IND   \n",
              "9                BLND                STEM                   IND   \n",
              "\n",
              "  Didactic Nature Level of Instruction or Training  \\\n",
              "0           INSTR                      unspecified   \n",
              "1           INSTR                      unspecified   \n",
              "2           INSTR                      unspecified   \n",
              "3           INSTR                             K-12   \n",
              "4           INSTR                             K-12   \n",
              "5           INSTR                             K-12   \n",
              "6             INF                    undergraduate   \n",
              "7             INF                    undergraduate   \n",
              "8             INF                    undergraduate   \n",
              "9           INSTR                             K-12   \n",
              "\n",
              "  Level of Instruction or Training (mapped) Analysis Approach  \\\n",
              "0                                      UNSP       model-based   \n",
              "1                                      UNSP       model-based   \n",
              "2                                      UNSP       model-based   \n",
              "3                                       K12       model-based   \n",
              "4                                       K12       model-based   \n",
              "5                                       K12       model-based   \n",
              "6                                       UNI       model-based   \n",
              "7                                       UNI       model-based   \n",
              "8                                       UNI       model-based   \n",
              "9                                       K12       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "6                                                                                                       Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. The findings suggest that MMLA can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding   \n",
              "7                                                                                                                                                                                                                                                                                                                                                              Common case of multimodal outperform unimodal models, through the addition of gaze to classify student's posttest performance and interest.   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "9                                                                                                                                                                                           Authors conclude that the feature combination of gaze and physiological MMD provide the most accurate predictions of correct answers. They also show the feasibility of early prediction of children's performance by using half (as oppose to full) data lengths to extract features and predict correctness.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  \n",
              "5             Joyce/Eduardo      1&2            NaN  \n",
              "6                     Joyce        1            NaN  \n",
              "7                   Eduardo        2            NaN  \n",
              "8             Joyce/Eduardo      1&2            NaN  \n",
              "9                     Joyce        1            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a6c49d4b-8b08-45ff-9cb9-fffe3d2b047c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Level of Instruction or Training (mapped)</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K-12</td>\n",
              "      <td>K12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K-12</td>\n",
              "      <td>K12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K-12</td>\n",
              "      <td>K12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>UNI</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. The findings suggest that MMLA can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>UNI</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Common case of multimodal outperform unimodal models, through the addition of gaze to classify student's posttest performance and interest.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>UNI</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1877483551</td>\n",
              "      <td>motion-based educational games: using multi-modal data to predict player’s performance</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR</td>\n",
              "      <td>PULSE,TEMP,EDA,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>COG</td>\n",
              "      <td>IEEE Conference on Games</td>\n",
              "      <td>6</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K-12</td>\n",
              "      <td>K12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Authors conclude that the feature combination of gaze and physiological MMD provide the most accurate predictions of correct answers. They also show the feasibility of early prediction of children's performance by using half (as oppose to full) data lengths to extract features and predict correctness.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a6c49d4b-8b08-45ff-9cb9-fffe3d2b047c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a6c49d4b-8b08-45ff-9cb9-fffe3d2b047c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a6c49d4b-8b08-45ff-9cb9-fffe3d2b047c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-817b33e9-ee06-48a9-b460-43bd0e3c9699\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-817b33e9-ee06-48a9-b460-43bd0e3c9699')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-817b33e9-ee06-48a9-b460-43bd0e3c9699 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=[\"Level of Instruction or Training\"], inplace=True)\n",
        "df.rename(columns={'Level of Instruction or Training (mapped)': 'Level of Instruction or Training'},inplace=True)\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "287e932e-47c1-4d04-d97b-bb437044aeb2",
        "id": "0AjtquG0FGtO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums Mapped Modalities Mapped Analysis Methods  \\\n",
              "0                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "1                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "2                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "3             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "4             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "\n",
              "  Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0                LATE                     MLPALA   \n",
              "1                LATE                     MLPALA   \n",
              "2                LATE                     MLPALA   \n",
              "3              HYBRID                       BJET   \n",
              "4              HYBRID                       BJET   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "\n",
              "  Environment Setting Environment Subject Participant Structure  \\\n",
              "0                PHYS                STEM            IND, MULTI   \n",
              "1                PHYS                STEM            IND, MULTI   \n",
              "2                PHYS                STEM            IND, MULTI   \n",
              "3                PHYS                STEM                 MULTI   \n",
              "4                PHYS                STEM                 MULTI   \n",
              "\n",
              "  Didactic Nature Level of Instruction or Training Analysis Approach  \\\n",
              "0           INSTR                             UNSP       model-based   \n",
              "1           INSTR                             UNSP       model-based   \n",
              "2           INSTR                             UNSP       model-based   \n",
              "3           INSTR                              K12       model-based   \n",
              "4           INSTR                              K12       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d1ddf645-546f-4852-93ce-8e712bc8a3c6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d1ddf645-546f-4852-93ce-8e712bc8a3c6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d1ddf645-546f-4852-93ce-8e712bc8a3c6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d1ddf645-546f-4852-93ce-8e712bc8a3c6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9bd3fcf2-8dbd-45ab-a8b2-601ce07a19f1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9bd3fcf2-8dbd-45ab-a8b2-601ce07a19f1')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9bd3fcf2-8dbd-45ab-a8b2-601ce07a19f1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis Approach"
      ],
      "metadata": {
        "id": "cTliFsnntcM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analysis_vals = set()\n",
        "analysis_list = [l.split(\", \") for l in list(df[\"Analysis Approach\"])]\n",
        "for item in analysis_list:\n",
        "  for val in item:\n",
        "    analysis_vals.add(val.lower())\n",
        "\n",
        "analysis_vals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d8763a2-533c-4c27-fed7-ded339a362b5",
        "id": "aBHrBPl8LRGv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mixed',\n",
              " 'model based',\n",
              " 'model free',\n",
              " 'model-based',\n",
              " 'model-free',\n",
              " 'uses 3rd party model but for augmenting qualitative'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analysis_map = {\n",
        "    'mixed':\"MB, MF\",\n",
        "    'model based':\"MB\",\n",
        "    'model free':\"MF\",\n",
        "    'model-based':\"MB\",\n",
        "    'model-free':\"MF\",\n",
        "    'uses 3rd party model but for augmenting qualitative':\"MB\"\n",
        "}"
      ],
      "metadata": {
        "id": "UdTjLge3LRGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_analysis_list = []\n",
        "\n",
        "for item in analysis_list:\n",
        "  new_item = set()\n",
        "  for val in item:\n",
        "    new_item.add(analysis_map[val.lower()])\n",
        "  new_analysis_list.append(list(new_item))\n",
        "\n",
        "new_analysis_list[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2bf4b01-0873-44f3-b31c-4d263ef65c4b",
        "id": "yUT2MuH4LRG0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['MB'], ['MB'], ['MB'], ['MB'], ['MB']]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_analysis_list_combined = []\n",
        "\n",
        "for item in new_analysis_list:\n",
        "  item = sorted(item)\n",
        "  new_item = \", \".join(item)\n",
        "  new_analysis_list_combined.append(new_item)\n",
        "\n",
        "new_analysis_list_combined[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ca949f8-c2e3-4824-d0b2-2cc12a73d699",
        "id": "kPJRVFRpLRG0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['MB', 'MB', 'MB', 'MB', 'MB']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.insert(18, \"Analysis Approach (mapped)\", new_analysis_list_combined)\n",
        "df[15:25]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7f8c15ef-d0c8-461c-fa56-027a87dda533",
        "id": "ApmfgKOvLRG1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          UUID  \\\n",
              "15  2070224207   \n",
              "16  2070224207   \n",
              "17  2070224207   \n",
              "18  2634033325   \n",
              "19  2634033325   \n",
              "20  2634033325   \n",
              "21  3051560548   \n",
              "22  3051560548   \n",
              "23  3051560548   \n",
              "24  3339002981   \n",
              "\n",
              "                                                                                                          Title  \\\n",
              "15                                detecting medical simulation errors with machine learning and multimodal data   \n",
              "16                                detecting medical simulation errors with machine learning and multimodal data   \n",
              "17                                detecting medical simulation errors with machine learning and multimodal data   \n",
              "18  controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "19  controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "20  controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "21                              temporal analysis of multimodal data to predict collaborative learning outcomes   \n",
              "22                              temporal analysis of multimodal data to predict collaborative learning outcomes   \n",
              "23                              temporal analysis of multimodal data to predict collaborative learning outcomes   \n",
              "24              estimation of success in collaborative learning based on multimodal learning analytics features   \n",
              "\n",
              "   Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "15    Daniele Di Mitri  2019                                Training   \n",
              "16    Daniele Di Mitri  2019                                Training   \n",
              "17    Daniele Di Mitri  2019                                Training   \n",
              "18        Xavier Ochoa  2020                                Training   \n",
              "19        Xavier Ochoa  2020                                Training   \n",
              "20        Xavier Ochoa  2020                                Training   \n",
              "21   Jennifer K. Olsen  2020                                Learning   \n",
              "22   Jennifer K. Olsen  2020                                Learning   \n",
              "23   Jennifer K. Olsen  2020                                Learning   \n",
              "24       Daniel Spikol  2017                                Learning   \n",
              "\n",
              "   Mapped Data Collection Mediums          Mapped Modalities  \\\n",
              "15              VIDEO,MOTION,LOGS                  POSE,LOGS   \n",
              "16              VIDEO,MOTION,LOGS                  POSE,LOGS   \n",
              "17              VIDEO,MOTION,LOGS                  POSE,LOGS   \n",
              "18                VIDEO,AUDIO,PPA              POSE,PROS,PPA   \n",
              "19                VIDEO,AUDIO,PPA              POSE,PROS,PPA   \n",
              "20                VIDEO,AUDIO,PPA              POSE,PROS,PPA   \n",
              "21                 LOGS,AUDIO,EYE  GAZE,LOGS,PROS,TRANS,QUAL   \n",
              "22                 LOGS,AUDIO,EYE  GAZE,LOGS,PROS,TRANS,QUAL   \n",
              "23                 LOGS,AUDIO,EYE  GAZE,LOGS,PROS,TRANS,QUAL   \n",
              "24           EYE,LOGS,VIDEO,AUDIO        GAZE,LOGS,PROS,POSE   \n",
              "\n",
              "   Mapped Analysis Methods Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "15                     CLS                 MID                       CAIM   \n",
              "16                     CLS                 MID                       CAIM   \n",
              "17                     CLS                 MID                       CAIM   \n",
              "18                   STATS                 OTH                       BJET   \n",
              "19                   STATS                 OTH                       BJET   \n",
              "20                   STATS                 OTH                       BJET   \n",
              "21                     REG                 MID                       BJET   \n",
              "22                     REG                 MID                       BJET   \n",
              "23                     REG                 MID                       BJET   \n",
              "24                     CLS                 MID                      ICALT   \n",
              "\n",
              "                                       Mapped Full Publication  Sort Number  \\\n",
              "15           Conference on Artificial Intelligence in Medicine           11   \n",
              "16           Conference on Artificial Intelligence in Medicine           11   \n",
              "17           Conference on Artificial Intelligence in Medicine           11   \n",
              "18                   British Journal of Educational Technology           12   \n",
              "19                   British Journal of Educational Technology           12   \n",
              "20                   British Journal of Educational Technology           12   \n",
              "21                   British Journal of Educational Technology           13   \n",
              "22                   British Journal of Educational Technology           13   \n",
              "23                   British Journal of Educational Technology           13   \n",
              "24  International Conference on Advanced Learning Technologies           14   \n",
              "\n",
              "   Environment Setting Environment Subject Participant Structure  \\\n",
              "15                BLND                 PSY                   IND   \n",
              "16                BLND                 PSY                   IND   \n",
              "17                BLND                 PSY                   IND   \n",
              "18                BLND                 HUM                   IND   \n",
              "19                BLND                 HUM                   IND   \n",
              "20                BLND                 HUM                   IND   \n",
              "21                VIRT                STEM                 MULTI   \n",
              "22                VIRT                STEM                 MULTI   \n",
              "23                VIRT                STEM                 MULTI   \n",
              "24                VIRT                STEM                 MULTI   \n",
              "\n",
              "   Didactic Nature Level of Instruction or Training Analysis Approach  \\\n",
              "15           TRAIN                              UNI       model-based   \n",
              "16           TRAIN                              UNI       model-based   \n",
              "17           TRAIN                              UNI       model-based   \n",
              "18             INF                             UNSP        model-free   \n",
              "19           TRAIN                             UNSP        model-free   \n",
              "20           TRAIN                             UNSP        model-free   \n",
              "21           INSTR                              K12       model-based   \n",
              "22           INSTR                              K12       model-based   \n",
              "23           INSTR                              K12       model-based   \n",
              "24           INSTR                              UNI       model-based   \n",
              "\n",
              "   Analysis Approach (mapped)  \\\n",
              "15                         MB   \n",
              "16                         MB   \n",
              "17                         MB   \n",
              "18                         MF   \n",
              "19                         MF   \n",
              "20                         MF   \n",
              "21                         MB   \n",
              "22                         MB   \n",
              "23                         MB   \n",
              "24                         MB   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "15  Used each Chest Compression as training sample by masking/windowing of the original time series, then trained an LSTM network with all these samples and were able to classify accurately the target classes, however discarding the rest of the time-series they were not able to detect if a CC happened. Author asks Doctorial Consortium how, given the available data, could they train a classifier able to detect whether a CC happened or not.   \n",
              "16                                                                                                                                                                                                                                                                                                                               Trained an LSTM to predict ['too slow', 'on-point', 'too fast'] for Chest compression training. Achieved 70-75% accuracy.   \n",
              "17                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
              "18                      Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoidance of filled pauses)   \n",
              "19                                                                                                                                                                                                                                                                                                                                 Authors showcase that the training tool improved manually defined scores between an initial and second use of the tool.   \n",
              "20                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
              "21                                                                                                                                                Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.   \n",
              "22                                                                                                                                                                                                                                                                       Evaluating how multimodal features contribute to a model's performance to predict learning gains. Audio features introduce noise that negatively impacted the error of the model.   \n",
              "23                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
              "24                                                                                                                                          Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, authors found that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.   \n",
              "\n",
              "   Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "15                     Joyce        1            NaN  \n",
              "16                   Eduardo        2            NaN  \n",
              "17             Joyce/Eduardo      1&2            NaN  \n",
              "18                     Joyce        1            NaN  \n",
              "19                   Eduardo        2            NaN  \n",
              "20             Joyce/Eduardo      1&2            NaN  \n",
              "21                     Joyce        1            NaN  \n",
              "22                   Eduardo        2            NaN  \n",
              "23             Joyce/Eduardo      1&2            NaN  \n",
              "24                     Joyce        1            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4bc5b1e3-83c1-4ed0-ac70-61c3bc9b07ac\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Approach (mapped)</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2070224207</td>\n",
              "      <td>detecting medical simulation errors with machine learning and multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,LOGS</td>\n",
              "      <td>POSE,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>CAIM</td>\n",
              "      <td>Conference on Artificial Intelligence in Medicine</td>\n",
              "      <td>11</td>\n",
              "      <td>BLND</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>model-based</td>\n",
              "      <td>MB</td>\n",
              "      <td>Used each Chest Compression as training sample by masking/windowing of the original time series, then trained an LSTM network with all these samples and were able to classify accurately the target classes, however discarding the rest of the time-series they were not able to detect if a CC happened. Author asks Doctorial Consortium how, given the available data, could they train a classifier able to detect whether a CC happened or not.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2070224207</td>\n",
              "      <td>detecting medical simulation errors with machine learning and multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,LOGS</td>\n",
              "      <td>POSE,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>CAIM</td>\n",
              "      <td>Conference on Artificial Intelligence in Medicine</td>\n",
              "      <td>11</td>\n",
              "      <td>BLND</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>model-based</td>\n",
              "      <td>MB</td>\n",
              "      <td>Trained an LSTM to predict ['too slow', 'on-point', 'too fast'] for Chest compression training. Achieved 70-75% accuracy.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2070224207</td>\n",
              "      <td>detecting medical simulation errors with machine learning and multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,LOGS</td>\n",
              "      <td>POSE,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>CAIM</td>\n",
              "      <td>Conference on Artificial Intelligence in Medicine</td>\n",
              "      <td>11</td>\n",
              "      <td>BLND</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>model-based</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>12</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>model-free</td>\n",
              "      <td>MF</td>\n",
              "      <td>Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoidance of filled pauses)</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>12</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>model-free</td>\n",
              "      <td>MF</td>\n",
              "      <td>Authors showcase that the training tool improved manually defined scores between an initial and second use of the tool.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>12</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>model-free</td>\n",
              "      <td>MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3051560548</td>\n",
              "      <td>temporal analysis of multimodal data to predict collaborative learning outcomes</td>\n",
              "      <td>Jennifer K. Olsen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,EYE</td>\n",
              "      <td>GAZE,LOGS,PROS,TRANS,QUAL</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>13</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>MB</td>\n",
              "      <td>Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3051560548</td>\n",
              "      <td>temporal analysis of multimodal data to predict collaborative learning outcomes</td>\n",
              "      <td>Jennifer K. Olsen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,EYE</td>\n",
              "      <td>GAZE,LOGS,PROS,TRANS,QUAL</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>13</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>MB</td>\n",
              "      <td>Evaluating how multimodal features contribute to a model's performance to predict learning gains. Audio features introduce noise that negatively impacted the error of the model.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3051560548</td>\n",
              "      <td>temporal analysis of multimodal data to predict collaborative learning outcomes</td>\n",
              "      <td>Jennifer K. Olsen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,EYE</td>\n",
              "      <td>GAZE,LOGS,PROS,TRANS,QUAL</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>13</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>3339002981</td>\n",
              "      <td>estimation of success in collaborative learning based on multimodal learning analytics features</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,LOGS,VIDEO,AUDIO</td>\n",
              "      <td>GAZE,LOGS,PROS,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICALT</td>\n",
              "      <td>International Conference on Advanced Learning Technologies</td>\n",
              "      <td>14</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>model-based</td>\n",
              "      <td>MB</td>\n",
              "      <td>Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, authors found that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4bc5b1e3-83c1-4ed0-ac70-61c3bc9b07ac')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4bc5b1e3-83c1-4ed0-ac70-61c3bc9b07ac button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4bc5b1e3-83c1-4ed0-ac70-61c3bc9b07ac');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d8107261-70ba-4ae8-9a98-548ed8e1db3e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d8107261-70ba-4ae8-9a98-548ed8e1db3e')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d8107261-70ba-4ae8-9a98-548ed8e1db3e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=[\"Analysis Approach\"], inplace=True)\n",
        "df.rename(columns={'Analysis Approach (mapped)': 'Analysis Approach'},inplace=True)\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7766f02-50c2-4af1-acab-4d6b968143cc",
        "id": "MrxpbYtRLRG2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums Mapped Modalities Mapped Analysis Methods  \\\n",
              "0                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "1                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "2                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "3             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "4             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "\n",
              "  Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0                LATE                     MLPALA   \n",
              "1                LATE                     MLPALA   \n",
              "2                LATE                     MLPALA   \n",
              "3              HYBRID                       BJET   \n",
              "4              HYBRID                       BJET   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "\n",
              "  Environment Setting Environment Subject Participant Structure  \\\n",
              "0                PHYS                STEM            IND, MULTI   \n",
              "1                PHYS                STEM            IND, MULTI   \n",
              "2                PHYS                STEM            IND, MULTI   \n",
              "3                PHYS                STEM                 MULTI   \n",
              "4                PHYS                STEM                 MULTI   \n",
              "\n",
              "  Didactic Nature Level of Instruction or Training Analysis Approach  \\\n",
              "0           INSTR                             UNSP                MB   \n",
              "1           INSTR                             UNSP                MB   \n",
              "2           INSTR                             UNSP                MB   \n",
              "3           INSTR                              K12                MB   \n",
              "4           INSTR                              K12                MB   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-df970ff7-b8a0-46ee-a3c2-be75e03d8dc5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df970ff7-b8a0-46ee-a3c2-be75e03d8dc5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-df970ff7-b8a0-46ee-a3c2-be75e03d8dc5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-df970ff7-b8a0-46ee-a3c2-be75e03d8dc5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f6942cb1-9fe5-4b97-98fb-a963b83fbafa\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f6942cb1-9fe5-4b97-98fb-a963b83fbafa')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f6942cb1-9fe5-4b97-98fb-a963b83fbafa button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify New Columns"
      ],
      "metadata": {
        "id": "GZ2_pJfuLDct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Environment Setting:\", set(df[\"Environment Setting\"]))\n",
        "print(\"Environment Subject\", set(df[\"Environment Subject\"]))\n",
        "print(\"Participant Structure\", set(df[\"Participant Structure\"]))\n",
        "print(\"Didactic Nature\", set(df[\"Didactic Nature\"]))\n",
        "print(\"Level of Instruction or Training\", set(df[\"Level of Instruction or Training\"]))\n",
        "print(\"Analysis Approach\", set(df[\"Analysis Approach\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKTz6-vRLGEB",
        "outputId": "b36af630-524f-4205-8a11-ac262d88345e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment Setting: {'BLND', 'PHYS', 'VIRT', 'UNSP', 'PHYS, VIRT'}\n",
            "Environment Subject {'OTH', 'HUM, OTH, STEM', 'HUM, STEM', 'PSY', 'UNSP', 'HUM', 'STEM'}\n",
            "Participant Structure {'IND', 'MULTI', 'IND, MULTI'}\n",
            "Didactic Nature {'INF', 'TRAIN', 'INSTR, TRAIN', 'INSTR', 'UNSP'}\n",
            "Level of Instruction or Training {'K12, UNI', 'PROF', 'UNI, UNSP', 'UNSP', 'UNI', 'K12', 'PROF, UNI'}\n",
            "Analysis Approach {'MB, MF', 'MB', 'MF'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save IRR/Consensus File"
      ],
      "metadata": {
        "id": "9dteGBJ6LGXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S18_CONSENSUS_PATH = \"drive/My Drive/Clayton/20230420_MMLTE/S18_IRR_and_Consensus.csv\"\n",
        "# df.to_csv(S18_CONSENSUS_PATH, index=False)"
      ],
      "metadata": {
        "id": "RZA6eF36PGFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify export\n",
        "df_import = pd.read_csv(S18_CONSENSUS_PATH)\n",
        "\n",
        "assert df.equals(df_import)\n",
        "assert len(df) == len(df_import)"
      ],
      "metadata": {
        "id": "y-AVp8uTogwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify UUIDs relative to last version\n",
        "PREV_VERSION_PATH = \"drive/My Drive/Clayton/20230420_MMLTE/S17.csv\"\n",
        "\n",
        "df_import = pd.read_csv(PREV_VERSION_PATH,header=0)\n",
        "\n",
        "assert set(df.UUID) == set(df_import.UUID)\n",
        "assert len(set(df.UUID)) == len(set(df_import.UUID)) == 73"
      ],
      "metadata": {
        "id": "rmpLheEzo4zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fVzWWklAtEGv",
        "outputId": "e05989a3-f4ac-4f86-ff4d-3237194c774a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           UUID  \\\n",
              "0    1326191931   \n",
              "1    1326191931   \n",
              "2    1326191931   \n",
              "3    1469065963   \n",
              "4    1469065963   \n",
              "5    1469065963   \n",
              "6    1598166515   \n",
              "7    1598166515   \n",
              "8    1598166515   \n",
              "9    1877483551   \n",
              "10   1877483551   \n",
              "11   1877483551   \n",
              "12   2000036002   \n",
              "13   2000036002   \n",
              "14   2000036002   \n",
              "15   2070224207   \n",
              "16   2070224207   \n",
              "17   2070224207   \n",
              "18   2634033325   \n",
              "19   2634033325   \n",
              "20   2634033325   \n",
              "21   3051560548   \n",
              "22   3051560548   \n",
              "23   3051560548   \n",
              "24   3339002981   \n",
              "25   3339002981   \n",
              "26   3339002981   \n",
              "27   3408664396   \n",
              "28   3408664396   \n",
              "29   3408664396   \n",
              "30    804659204   \n",
              "31    804659204   \n",
              "32    804659204   \n",
              "33   1581261659   \n",
              "34   1581261659   \n",
              "35   1581261659   \n",
              "36   2836996318   \n",
              "37   2836996318   \n",
              "38   2836996318   \n",
              "39     32184286   \n",
              "40     32184286   \n",
              "41     32184286   \n",
              "42    433919853   \n",
              "43    433919853   \n",
              "44    433919853   \n",
              "45    483140962   \n",
              "46    483140962   \n",
              "47    483140962   \n",
              "48   3308658121   \n",
              "49   3308658121   \n",
              "50   3308658121   \n",
              "51   1847468084   \n",
              "52   1847468084   \n",
              "53   1847468084   \n",
              "54   2345021698   \n",
              "55   2345021698   \n",
              "56   2345021698   \n",
              "57   3796643912   \n",
              "58   3796643912   \n",
              "59   3796643912   \n",
              "60    205660768   \n",
              "61    205660768   \n",
              "62    205660768   \n",
              "63   2181637610   \n",
              "64   2181637610   \n",
              "65   2181637610   \n",
              "66   4035649049   \n",
              "67   4035649049   \n",
              "68   4035649049   \n",
              "69   1019093033   \n",
              "70   1019093033   \n",
              "71   1019093033   \n",
              "72   2936220551   \n",
              "73   2936220551   \n",
              "74   2936220551   \n",
              "75   1886134458   \n",
              "76   1886134458   \n",
              "77   1886134458   \n",
              "78   2879332689   \n",
              "79   2879332689   \n",
              "80   2879332689   \n",
              "81   3146393211   \n",
              "82   3146393211   \n",
              "83   3146393211   \n",
              "84   3809293172   \n",
              "85   3809293172   \n",
              "86   3809293172   \n",
              "87   4019205162   \n",
              "88   4019205162   \n",
              "89   4019205162   \n",
              "90   4277812050   \n",
              "91   4277812050   \n",
              "92   4277812050   \n",
              "93   1609706685   \n",
              "94   1609706685   \n",
              "95   1609706685   \n",
              "96   3398902089   \n",
              "97   3398902089   \n",
              "98   3398902089   \n",
              "99   3093310941   \n",
              "100  3093310941   \n",
              "101  3093310941   \n",
              "102  1576545447   \n",
              "103  1576545447   \n",
              "104  1576545447   \n",
              "105  3796180663   \n",
              "106  3796180663   \n",
              "107  3796180663   \n",
              "108  1770989706   \n",
              "109  1770989706   \n",
              "110  1770989706   \n",
              "111  2456887548   \n",
              "112  2456887548   \n",
              "113  2456887548   \n",
              "114   518268671   \n",
              "115   518268671   \n",
              "116   518268671   \n",
              "117   957160695   \n",
              "118   957160695   \n",
              "119   957160695   \n",
              "120  3009548670   \n",
              "121  3009548670   \n",
              "122  3009548670   \n",
              "123  3448122334   \n",
              "124  3448122334   \n",
              "125  3448122334   \n",
              "126  2497456347   \n",
              "127  2497456347   \n",
              "128  2497456347   \n",
              "129  3660066725   \n",
              "130  3660066725   \n",
              "131  3660066725   \n",
              "132  3783339081   \n",
              "133  3783339081   \n",
              "134  3783339081   \n",
              "135  3095923626   \n",
              "136  3095923626   \n",
              "137  3095923626   \n",
              "138    85990093   \n",
              "139    85990093   \n",
              "140    85990093   \n",
              "141    86191824   \n",
              "142    86191824   \n",
              "143    86191824   \n",
              "144   818492192   \n",
              "145   818492192   \n",
              "146   818492192   \n",
              "147   147203129   \n",
              "148   147203129   \n",
              "149   147203129   \n",
              "150   123412197   \n",
              "151   123412197   \n",
              "152   123412197   \n",
              "153  1118315889   \n",
              "154  1118315889   \n",
              "155  1118315889   \n",
              "156  1315379489   \n",
              "157  1315379489   \n",
              "158  1315379489   \n",
              "159  1374035721   \n",
              "160  1374035721   \n",
              "161  1374035721   \n",
              "162  1426267857   \n",
              "163  1426267857   \n",
              "164  1426267857   \n",
              "165  2055153191   \n",
              "166  2055153191   \n",
              "167  2055153191   \n",
              "168  2273914836   \n",
              "169  2273914836   \n",
              "170  2273914836   \n",
              "171  1763513559   \n",
              "172  1763513559   \n",
              "173  1763513559   \n",
              "174  1345598079   \n",
              "175  1345598079   \n",
              "176  1345598079   \n",
              "177  3135645357   \n",
              "178  3135645357   \n",
              "179  3135645357   \n",
              "180  3856280479   \n",
              "181  3856280479   \n",
              "182  3856280479   \n",
              "183  2609260641   \n",
              "184  2609260641   \n",
              "185  2609260641   \n",
              "186   666050348   \n",
              "187   666050348   \n",
              "188   666050348   \n",
              "189  1637690235   \n",
              "190  1637690235   \n",
              "191  1637690235   \n",
              "192  2155422499   \n",
              "193  2155422499   \n",
              "194  2155422499   \n",
              "195  3309250332   \n",
              "196  3309250332   \n",
              "197  3309250332   \n",
              "198  3625722965   \n",
              "199  3625722965   \n",
              "200  3625722965   \n",
              "201  4278392816   \n",
              "202  4278392816   \n",
              "203  4278392816   \n",
              "204   566043228   \n",
              "205   566043228   \n",
              "206   566043228   \n",
              "207   853680639   \n",
              "208   853680639   \n",
              "209   853680639   \n",
              "210  3637456466   \n",
              "211  3637456466   \n",
              "212  3637456466   \n",
              "213  3754172825   \n",
              "214  3754172825   \n",
              "215  3754172825   \n",
              "216  1296637108   \n",
              "217  1296637108   \n",
              "218  1296637108   \n",
              "\n",
              "                                                                                                                                                          Title  \\\n",
              "0                                                                                                       multimodal learning analytics in a laboratory classroom   \n",
              "1                                                                                                       multimodal learning analytics in a laboratory classroom   \n",
              "2                                                                                                       multimodal learning analytics in a laboratory classroom   \n",
              "3                                               examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4                                               examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "5                                               examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "6                                                                                                         multimodal learning analytics for game-based learning   \n",
              "7                                                                                                         multimodal learning analytics for game-based learning   \n",
              "8                                                                                                         multimodal learning analytics for game-based learning   \n",
              "9                                                                        motion-based educational games: using multi-modal data to predict player’s performance   \n",
              "10                                                                       motion-based educational games: using multi-modal data to predict player’s performance   \n",
              "11                                                                       motion-based educational games: using multi-modal data to predict player’s performance   \n",
              "12                                                                        predicting learners’ effortful behaviour in adaptive assessment using multimodal data   \n",
              "13                                                                        predicting learners’ effortful behaviour in adaptive assessment using multimodal data   \n",
              "14                                                                        predicting learners’ effortful behaviour in adaptive assessment using multimodal data   \n",
              "15                                                                                detecting medical simulation errors with machine learning and multimodal data   \n",
              "16                                                                                detecting medical simulation errors with machine learning and multimodal data   \n",
              "17                                                                                detecting medical simulation errors with machine learning and multimodal data   \n",
              "18                                                  controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "19                                                  controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "20                                                  controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "21                                                                              temporal analysis of multimodal data to predict collaborative learning outcomes   \n",
              "22                                                                              temporal analysis of multimodal data to predict collaborative learning outcomes   \n",
              "23                                                                              temporal analysis of multimodal data to predict collaborative learning outcomes   \n",
              "24                                                              estimation of success in collaborative learning based on multimodal learning analytics features   \n",
              "25                                                              estimation of success in collaborative learning based on multimodal learning analytics features   \n",
              "26                                                              estimation of success in collaborative learning based on multimodal learning analytics features   \n",
              "27                                                                                                 multimodal student engagement recognition in prosocial games   \n",
              "28                                                                                                 multimodal student engagement recognition in prosocial games   \n",
              "29                                                                                                 multimodal student engagement recognition in prosocial games   \n",
              "30                                                                           towards smart educational recommendations with reinforcement learning in classroom   \n",
              "31                                                                           towards smart educational recommendations with reinforcement learning in classroom   \n",
              "32                                                                           towards smart educational recommendations with reinforcement learning in classroom   \n",
              "33                                                                 early prediction of visitor engagement in science museums with multimodal learning analytics   \n",
              "34                                                                 early prediction of visitor engagement in science museums with multimodal learning analytics   \n",
              "35                                                                 early prediction of visitor engagement in science museums with multimodal learning analytics   \n",
              "36                                                                     predicting learners' emotions in mobile mooc learning via a multimodal intelligent tutor   \n",
              "37                                                                     predicting learners' emotions in mobile mooc learning via a multimodal intelligent tutor   \n",
              "38                                                                     predicting learners' emotions in mobile mooc learning via a multimodal intelligent tutor   \n",
              "39                                                                                            once more with feeling: emotions in multimodal learning analytics   \n",
              "40                                                                                            once more with feeling: emotions in multimodal learning analytics   \n",
              "41                                                                                            once more with feeling: emotions in multimodal learning analytics   \n",
              "42                                                                                           understanding fun in learning to code: a multi-modal data approach   \n",
              "43                                                                                           understanding fun in learning to code: a multi-modal data approach   \n",
              "44                                                                                           understanding fun in learning to code: a multi-modal data approach   \n",
              "45                                                            investigating multimodal affect sensing in an affective tutoring system using unobtrusive sensors   \n",
              "46                                                            investigating multimodal affect sensing in an affective tutoring system using unobtrusive sensors   \n",
              "47                                                            investigating multimodal affect sensing in an affective tutoring system using unobtrusive sensors   \n",
              "48                                                                              exploring collaboration using motion sensors and multi-modal learning analytics   \n",
              "49                                                                              exploring collaboration using motion sensors and multi-modal learning analytics   \n",
              "50                                                                              exploring collaboration using motion sensors and multi-modal learning analytics   \n",
              "51                                                                         computationally augmented ethnography: emotion tracking and learning in museum games   \n",
              "52                                                                         computationally augmented ethnography: emotion tracking and learning in museum games   \n",
              "53                                                                         computationally augmented ethnography: emotion tracking and learning in museum games   \n",
              "54                            exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course   \n",
              "55                            exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course   \n",
              "56                            exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course   \n",
              "57                              an evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities   \n",
              "58                              an evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities   \n",
              "59                              an evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities   \n",
              "60                                                                    multimodal learning analytics to investigate cognitive load during online problem solving   \n",
              "61                                                                    multimodal learning analytics to investigate cognitive load during online problem solving   \n",
              "62                                                                    multimodal learning analytics to investigate cognitive load during online problem solving   \n",
              "63                                                         toward using multi-modal learning analytics to support and measure collaboration in co-located dyads   \n",
              "64                                                         toward using multi-modal learning analytics to support and measure collaboration in co-located dyads   \n",
              "65                                                         toward using multi-modal learning analytics to support and measure collaboration in co-located dyads   \n",
              "66                                                                           storytelling with learner data: guiding student reflection on multimodal team data   \n",
              "67                                                                           storytelling with learner data: guiding student reflection on multimodal team data   \n",
              "68                                                                           storytelling with learner data: guiding student reflection on multimodal team data   \n",
              "69                                                                  prime: block-wise missingness handling for multi-modalities in intelligent tutoring systems   \n",
              "70                                                                  prime: block-wise missingness handling for multi-modalities in intelligent tutoring systems   \n",
              "71                                                                  prime: block-wise missingness handling for multi-modalities in intelligent tutoring systems   \n",
              "72                                           multi-source and multimodal data fusion for predicting academic performance in blended learning university courses   \n",
              "73                                           multi-source and multimodal data fusion for predicting academic performance in blended learning university courses   \n",
              "74                                           multi-source and multimodal data fusion for predicting academic performance in blended learning university courses   \n",
              "75                                                                         personalizing computer science education by leveraging multimodal learning analytics   \n",
              "76                                                                         personalizing computer science education by leveraging multimodal learning analytics   \n",
              "77                                                                         personalizing computer science education by leveraging multimodal learning analytics   \n",
              "78                                                                     from data to insights: a layered storytelling approach for multimodal learning analytics   \n",
              "79                                                                     from data to insights: a layered storytelling approach for multimodal learning analytics   \n",
              "80                                                                     from data to insights: a layered storytelling approach for multimodal learning analytics   \n",
              "81                                                       mobile mixed reality for experiential learning and simulation in medical and health sciences education   \n",
              "82                                                       mobile mixed reality for experiential learning and simulation in medical and health sciences education   \n",
              "83                                                       mobile mixed reality for experiential learning and simulation in medical and health sciences education   \n",
              "84                    blending learning analytics and embodied design to model students' comprehension of measurement using their actions, speech, and gestures   \n",
              "85                    blending learning analytics and embodied design to model students' comprehension of measurement using their actions, speech, and gestures   \n",
              "86                    blending learning analytics and embodied design to model students' comprehension of measurement using their actions, speech, and gestures   \n",
              "87                     introducing low-cost sensors into the classroom settings: improving the assessment in agile practices with multimodal learning analytics   \n",
              "88                     introducing low-cost sensors into the classroom settings: improving the assessment in agile practices with multimodal learning analytics   \n",
              "89                     introducing low-cost sensors into the classroom settings: improving the assessment in agile practices with multimodal learning analytics   \n",
              "90   improving prediction of students' performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources   \n",
              "91   improving prediction of students' performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources   \n",
              "92   improving prediction of students' performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources   \n",
              "93                                      learning pulse: a machine learning approach for predicting performance in self-regulated learning using multimodal data   \n",
              "94                                      learning pulse: a machine learning approach for predicting performance in self-regulated learning using multimodal data   \n",
              "95                                      learning pulse: a machine learning approach for predicting performance in self-regulated learning using multimodal data   \n",
              "96                                                                   what multimodal data can tell us about the students’ regulation of their learning process?   \n",
              "97                                                                   what multimodal data can tell us about the students’ regulation of their learning process?   \n",
              "98                                                                   what multimodal data can tell us about the students’ regulation of their learning process?   \n",
              "99                                      embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders   \n",
              "100                                     embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders   \n",
              "101                                     embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders   \n",
              "102                                        artificial intelligence and multimodal data in the service of human decision-making: a case study in debate tutoring   \n",
              "103                                        artificial intelligence and multimodal data in the service of human decision-making: a case study in debate tutoring   \n",
              "104                                        artificial intelligence and multimodal data in the service of human decision-making: a case study in debate tutoring   \n",
              "105                                                                           learning linkages: integrating data streams of multiple modalities and timescales   \n",
              "106                                                                           learning linkages: integrating data streams of multiple modalities and timescales   \n",
              "107                                                                           learning linkages: integrating data streams of multiple modalities and timescales   \n",
              "108                                                  focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving   \n",
              "109                                                  focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving   \n",
              "110                                                  focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving   \n",
              "111                                                                      an unobtrusive and multimodal approach for behavioral engagement detection of students   \n",
              "112                                                                      an unobtrusive and multimodal approach for behavioral engagement detection of students   \n",
              "113                                                                      an unobtrusive and multimodal approach for behavioral engagement detection of students   \n",
              "114                                                   using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game   \n",
              "115                                                   using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game   \n",
              "116                                                   using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game   \n",
              "117                                                                                 virtual debate coach design: assessing multimodal argumentation performance   \n",
              "118                                                                                 virtual debate coach design: assessing multimodal argumentation performance   \n",
              "119                                                                                 virtual debate coach design: assessing multimodal argumentation performance   \n",
              "120                                                                                                            real-time multimodal feedback with the cpr tutor   \n",
              "121                                                                                                            real-time multimodal feedback with the cpr tutor   \n",
              "122                                                                                                            real-time multimodal feedback with the cpr tutor   \n",
              "123                                         investigating the impact of a real-time, multimodal student engagement analytics technology in authentic classrooms   \n",
              "124                                         investigating the impact of a real-time, multimodal student engagement analytics technology in authentic classrooms   \n",
              "125                                         investigating the impact of a real-time, multimodal student engagement analytics technology in authentic classrooms   \n",
              "126                                               the rap system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors   \n",
              "127                                               the rap system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors   \n",
              "128                                               the rap system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors   \n",
              "129                             children's play and problem solving in motion-based educational games: synergies between human annotations and multi-modal data   \n",
              "130                             children's play and problem solving in motion-based educational games: synergies between human annotations and multi-modal data   \n",
              "131                             children's play and problem solving in motion-based educational games: synergies between human annotations and multi-modal data   \n",
              "132                                        a novel method for the in-depth multimodal analysis of student learning trajectories in intelligent tutoring systems   \n",
              "133                                        a novel method for the in-depth multimodal analysis of student learning trajectories in intelligent tutoring systems   \n",
              "134                                        a novel method for the in-depth multimodal analysis of student learning trajectories in intelligent tutoring systems   \n",
              "135                                                                                                                             a multimodal analysis of making   \n",
              "136                                                                                                                             a multimodal analysis of making   \n",
              "137                                                                                                                             a multimodal analysis of making   \n",
              "138                                                                                  multimodal markers of persuasive speech : designing a virtual debate coach   \n",
              "139                                                                                  multimodal markers of persuasive speech : designing a virtual debate coach   \n",
              "140                                                                                  multimodal markers of persuasive speech : designing a virtual debate coach   \n",
              "141                                   examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes   \n",
              "142                                   examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes   \n",
              "143                                   examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes   \n",
              "144                         understanding student learning trajectories using multimodal learning analytics within an embodied-interaction learning environment   \n",
              "145                         understanding student learning trajectories using multimodal learning analytics within an embodied-interaction learning environment   \n",
              "146                         understanding student learning trajectories using multimodal learning analytics within an embodied-interaction learning environment   \n",
              "147                                                           multimodal learning analytics to inform learning design: lessons learned from computing education   \n",
              "148                                                           multimodal learning analytics to inform learning design: lessons learned from computing education   \n",
              "149                                                           multimodal learning analytics to inform learning design: lessons learned from computing education   \n",
              "150                                                                          utilizing multimodal data through fsqca to explain engagement in adaptive learning   \n",
              "151                                                                          utilizing multimodal data through fsqca to explain engagement in adaptive learning   \n",
              "152                                                                          utilizing multimodal data through fsqca to explain engagement in adaptive learning   \n",
              "153                                                          using multimodal learning analytics to identify aspects of collaboration in project-based learning   \n",
              "154                                                          using multimodal learning analytics to identify aspects of collaboration in project-based learning   \n",
              "155                                                          using multimodal learning analytics to identify aspects of collaboration in project-based learning   \n",
              "156                                                                                          multimodal engagement analysis from facial videos in the classroom   \n",
              "157                                                                                          multimodal engagement analysis from facial videos in the classroom   \n",
              "158                                                                                          multimodal engagement analysis from facial videos in the classroom   \n",
              "159                                                                      attentivelearner2: a multimodal approach for improving mooc learning on mobile devices   \n",
              "160                                                                      attentivelearner2: a multimodal approach for improving mooc learning on mobile devices   \n",
              "161                                                                      attentivelearner2: a multimodal approach for improving mooc learning on mobile devices   \n",
              "162                                                                      affect, support, and personal factors: multimodal causal models of one-on-one coaching   \n",
              "163                                                                      affect, support, and personal factors: multimodal causal models of one-on-one coaching   \n",
              "164                                                                      affect, support, and personal factors: multimodal causal models of one-on-one coaching   \n",
              "165                                                        round or rectangular tables for collaborative problem solving? a multimodal learning analytics study   \n",
              "166                                                        round or rectangular tables for collaborative problem solving? a multimodal learning analytics study   \n",
              "167                                                        round or rectangular tables for collaborative problem solving? a multimodal learning analytics study   \n",
              "168                               many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities   \n",
              "169                               many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities   \n",
              "170                               many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities   \n",
              "171                                                                                                keep me in the loop: real-time feedback with multimodal data   \n",
              "172                                                                                                keep me in the loop: real-time feedback with multimodal data   \n",
              "173                                                                                                keep me in the loop: real-time feedback with multimodal data   \n",
              "174                       intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning   \n",
              "175                       intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning   \n",
              "176                       intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning   \n",
              "177                                                       multimodal teaching analytics: automated extraction of orchestration graphs from wearable sensor data   \n",
              "178                                                       multimodal teaching analytics: automated extraction of orchestration graphs from wearable sensor data   \n",
              "179                                                       multimodal teaching analytics: automated extraction of orchestration graphs from wearable sensor data   \n",
              "180                                        children's play and problem-solving in motion-based learning technologies using a multi-modal mixed methods approach   \n",
              "181                                        children's play and problem-solving in motion-based learning technologies using a multi-modal mixed methods approach   \n",
              "182                                        children's play and problem-solving in motion-based learning technologies using a multi-modal mixed methods approach   \n",
              "183                                                visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication   \n",
              "184                                                visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication   \n",
              "185                                                visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication   \n",
              "186                                                                        multicraft: a multimodal interface for supporting and studying learning in minecraft   \n",
              "187                                                                        multicraft: a multimodal interface for supporting and studying learning in minecraft   \n",
              "188                                                                        multicraft: a multimodal interface for supporting and studying learning in minecraft   \n",
              "189                                               supervised machine learning in multimodal learning analytics for estimating success in project-based learning   \n",
              "190                                               supervised machine learning in multimodal learning analytics for estimating success in project-based learning   \n",
              "191                                               supervised machine learning in multimodal learning analytics for estimating success in project-based learning   \n",
              "192                                                              a multimodal analysis of pair work engagement episodes: implications for emi lecturer training   \n",
              "193                                                              a multimodal analysis of pair work engagement episodes: implications for emi lecturer training   \n",
              "194                                                              a multimodal analysis of pair work engagement episodes: implications for emi lecturer training   \n",
              "195                                                      (dis)engagement matters: identifying efficacious learning practices with multimodal learning analytics   \n",
              "196                                                      (dis)engagement matters: identifying efficacious learning practices with multimodal learning analytics   \n",
              "197                                                      (dis)engagement matters: identifying efficacious learning practices with multimodal learning analytics   \n",
              "198                                                            table tennis tutor: forehand strokes classification based on multimodal data and neural networks   \n",
              "199                                                            table tennis tutor: forehand strokes classification based on multimodal data and neural networks   \n",
              "200                                                            table tennis tutor: forehand strokes classification based on multimodal data and neural networks   \n",
              "201                                                                                            multimodal data as a means to understand the learning experience   \n",
              "202                                                                                            multimodal data as a means to understand the learning experience   \n",
              "203                                                                                            multimodal data as a means to understand the learning experience   \n",
              "204                                                                  automatic student engagement in online learning environment based on neural turing machine   \n",
              "205                                                                  automatic student engagement in online learning environment based on neural turing machine   \n",
              "206                                                                  automatic student engagement in online learning environment based on neural turing machine   \n",
              "207                                                                sensor-based data fusion for multimodal affect detection in game-based learning environments   \n",
              "208                                                                sensor-based data fusion for multimodal affect detection in game-based learning environments   \n",
              "209                                                                sensor-based data fusion for multimodal affect detection in game-based learning environments   \n",
              "210                                    impact of inquiry interventions on students in e-learning and classroom environments using affective computing framework   \n",
              "211                                    impact of inquiry interventions on students in e-learning and classroom environments using affective computing framework   \n",
              "212                                    impact of inquiry interventions on students in e-learning and classroom environments using affective computing framework   \n",
              "213                                                                   detecting impasse during collaborative problem solving with multimodal learning analytics   \n",
              "214                                                                   detecting impasse during collaborative problem solving with multimodal learning analytics   \n",
              "215                                                                   detecting impasse during collaborative problem solving with multimodal learning analytics   \n",
              "216                                                                                 towards collaboration translucence: giving meaning to multimodal group data   \n",
              "217                                                                                 towards collaboration translucence: giving meaning to multimodal group data   \n",
              "218                                                                                 towards collaboration translucence: giving meaning to multimodal group data   \n",
              "\n",
              "            Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0         Man Ching Esther Chan  2019                                Learning   \n",
              "1         Man Ching Esther Chan  2019                                Learning   \n",
              "2         Man Ching Esther Chan  2019                                Learning   \n",
              "3                   Andy Nguyen  2022                                Learning   \n",
              "4                   Andy Nguyen  2022                                Learning   \n",
              "5                   Andy Nguyen  2022                                Learning   \n",
              "6                Andrew Emerson  2020                                Learning   \n",
              "7                Andrew Emerson  2020                                Learning   \n",
              "8                Andrew Emerson  2020                                Learning   \n",
              "9            Serena Lee-Cultura  2020                                Learning   \n",
              "10           Serena Lee-Cultura  2020                                Learning   \n",
              "11           Serena Lee-Cultura  2020                                Learning   \n",
              "12               Kshitij Sharma  2020                                Learning   \n",
              "13               Kshitij Sharma  2020                                Learning   \n",
              "14               Kshitij Sharma  2020                                Learning   \n",
              "15             Daniele Di Mitri  2019                                Training   \n",
              "16             Daniele Di Mitri  2019                                Training   \n",
              "17             Daniele Di Mitri  2019                                Training   \n",
              "18                 Xavier Ochoa  2020                                Training   \n",
              "19                 Xavier Ochoa  2020                                Training   \n",
              "20                 Xavier Ochoa  2020                                Training   \n",
              "21            Jennifer K. Olsen  2020                                Learning   \n",
              "22            Jennifer K. Olsen  2020                                Learning   \n",
              "23            Jennifer K. Olsen  2020                                Learning   \n",
              "24                Daniel Spikol  2017                                Learning   \n",
              "25                Daniel Spikol  2017                                Learning   \n",
              "26                Daniel Spikol  2017                                Learning   \n",
              "27           Athanasios Psaltis  2017                                Learning   \n",
              "28           Athanasios Psaltis  2017                                Learning   \n",
              "29           Athanasios Psaltis  2017                                Learning   \n",
              "30                       Su Liu  2018                                Learning   \n",
              "31                       Su Liu  2018                                Learning   \n",
              "32                       Su Liu  2018                                Learning   \n",
              "33               Andrew Emerson  2020                                Learning   \n",
              "34               Andrew Emerson  2020                                Learning   \n",
              "35               Andrew Emerson  2020                                Learning   \n",
              "36                  Phuong Pham  2018                                Learning   \n",
              "37                  Phuong Pham  2018                                Learning   \n",
              "38                  Phuong Pham  2018                                Learning   \n",
              "39                Marcus Kubsch  2022                                Learning   \n",
              "40                Marcus Kubsch  2022                                Learning   \n",
              "41                Marcus Kubsch  2022                                Learning   \n",
              "42              Gabriella Tisza  2022                                Learning   \n",
              "43              Gabriella Tisza  2022                                Learning   \n",
              "44              Gabriella Tisza  2022                                Learning   \n",
              "45                Hua Leong Fwa  2018                                Learning   \n",
              "46                Hua Leong Fwa  2018                                Learning   \n",
              "47                Hua Leong Fwa  2018                                Learning   \n",
              "48             Joseph M. Reilly  2018                                Learning   \n",
              "49             Joseph M. Reilly  2018                                Learning   \n",
              "50             Joseph M. Reilly  2018                                Learning   \n",
              "51                   Kit Martin  2019                                Learning   \n",
              "52                   Kit Martin  2019                                Learning   \n",
              "53                   Kit Martin  2019                                Learning   \n",
              "54                    René Noël  2018                                Learning   \n",
              "55                    René Noël  2018                                Learning   \n",
              "56                    René Noël  2018                                Learning   \n",
              "57          Penelope J. Standen  2020                                Learning   \n",
              "58          Penelope J. Standen  2020                                Learning   \n",
              "59          Penelope J. Standen  2020                                Learning   \n",
              "60          Charlotte Larmuseau  2020                                Learning   \n",
              "61          Charlotte Larmuseau  2020                                Learning   \n",
              "62          Charlotte Larmuseau  2020                                Learning   \n",
              "63                Emma L. Starr  2018                                Learning   \n",
              "64                Emma L. Starr  2018                                Learning   \n",
              "65                Emma L. Starr  2018                                Learning   \n",
              "66       Gloria Fernández-Nieto  2021                                Training   \n",
              "67       Gloria Fernández-Nieto  2021                                Training   \n",
              "68       Gloria Fernández-Nieto  2021                                Training   \n",
              "69                      Xi Yang  2019                                Learning   \n",
              "70                      Xi Yang  2019                                Learning   \n",
              "71                      Xi Yang  2019                                Learning   \n",
              "72                Wilson Chango  2020                                Learning   \n",
              "73                Wilson Chango  2020                                Learning   \n",
              "74                Wilson Chango  2020                                Learning   \n",
              "75                 David Azcona  2018                                Learning   \n",
              "76                 David Azcona  2018                                Learning   \n",
              "77                 David Azcona  2018                                Learning   \n",
              "78   Roberto Martinez-Maldonado  2020                                Training   \n",
              "79   Roberto Martinez-Maldonado  2020                                Training   \n",
              "80   Roberto Martinez-Maldonado  2020                                Training   \n",
              "81                   James Birt  2018                                Learning   \n",
              "82                   James Birt  2018                                Learning   \n",
              "83                   James Birt  2018                                Learning   \n",
              "84             Avery H. Closser  2021                                Learning   \n",
              "85             Avery H. Closser  2021                                Learning   \n",
              "86             Avery H. Closser  2021                                Learning   \n",
              "87         Hector Cornide-Reyes  2019                                Learning   \n",
              "88         Hector Cornide-Reyes  2019                                Learning   \n",
              "89         Hector Cornide-Reyes  2019                                Learning   \n",
              "90                Wilson Chango  2021                                Learning   \n",
              "91                Wilson Chango  2021                                Learning   \n",
              "92                Wilson Chango  2021                                Learning   \n",
              "93             Daniele Di Mitri  2017                                Training   \n",
              "94             Daniele Di Mitri  2017                                Training   \n",
              "95             Daniele Di Mitri  2017                                Training   \n",
              "96                Sanna Järvelä  2019                                Learning   \n",
              "97                Sanna Järvelä  2019                                Learning   \n",
              "98                Sanna Järvelä  2019                                Learning   \n",
              "99                Hiroki Tanaka  2017                                Training   \n",
              "100               Hiroki Tanaka  2017                                Training   \n",
              "101               Hiroki Tanaka  2017                                Training   \n",
              "102              Mutlu Cukurova  2019                                Learning   \n",
              "103              Mutlu Cukurova  2019                                Learning   \n",
              "104              Mutlu Cukurova  2019                                Learning   \n",
              "105                     Ran Liu  2018                                Learning   \n",
              "106                     Ran Liu  2018                                Learning   \n",
              "107                     Ran Liu  2018                                Learning   \n",
              "108               Hana Vrzakova  2020                                Learning   \n",
              "109               Hana Vrzakova  2020                                Learning   \n",
              "110               Hana Vrzakova  2020                                Learning   \n",
              "111                  Nese Alyuz  2017                                Learning   \n",
              "112                  Nese Alyuz  2017                                Learning   \n",
              "113                  Nese Alyuz  2017                                Learning   \n",
              "114          María Ximena López  2021                                Learning   \n",
              "115          María Ximena López  2021                                Learning   \n",
              "116          María Ximena López  2021                                Learning   \n",
              "117             Volha Petukhova  2017                                Training   \n",
              "118             Volha Petukhova  2017                                Training   \n",
              "119             Volha Petukhova  2017                                Training   \n",
              "120            Daniele Di Mitri  2020                                Training   \n",
              "121            Daniele Di Mitri  2020                                Training   \n",
              "122            Daniele Di Mitri  2020                                Training   \n",
              "123                 Sinem Aslan  2019                                Learning   \n",
              "124                 Sinem Aslan  2019                                Learning   \n",
              "125                 Sinem Aslan  2019                                Learning   \n",
              "126                Xavier Ochoa  2018                                Training   \n",
              "127                Xavier Ochoa  2018                                Training   \n",
              "128                Xavier Ochoa  2018                                Training   \n",
              "129          Serena Lee-Cultura  2021                                Learning   \n",
              "130          Serena Lee-Cultura  2021                                Learning   \n",
              "131          Serena Lee-Cultura  2021                                Learning   \n",
              "132                     Ran Liu  2018                                Learning   \n",
              "133                     Ran Liu  2018                                Learning   \n",
              "134                     Ran Liu  2018                                Learning   \n",
              "135             Marcelo Worsley  2017                                Learning   \n",
              "136             Marcelo Worsley  2017                                Learning   \n",
              "137             Marcelo Worsley  2017                                Learning   \n",
              "138             Volha Petukhova  2017                                Training   \n",
              "139             Volha Petukhova  2017                                Training   \n",
              "140             Volha Petukhova  2017                                Training   \n",
              "141                Shiyan Jiang  2019                                Learning   \n",
              "142                Shiyan Jiang  2019                                Learning   \n",
              "143                Shiyan Jiang  2019                                Learning   \n",
              "144           Alejandro Andrade  2017                                Learning   \n",
              "145           Alejandro Andrade  2017                                Learning   \n",
              "146           Alejandro Andrade  2017                                Learning   \n",
              "147         Katerina Mangaroska  2020                                Learning   \n",
              "148         Katerina Mangaroska  2020                                Learning   \n",
              "149         Katerina Mangaroska  2020                                Learning   \n",
              "150      Zacharoula Papamitsiou  2020                                Learning   \n",
              "151      Zacharoula Papamitsiou  2020                                Learning   \n",
              "152      Zacharoula Papamitsiou  2020                                Learning   \n",
              "153               Daniel Spikol  2017                                Learning   \n",
              "154               Daniel Spikol  2017                                Learning   \n",
              "155               Daniel Spikol  2017                                Learning   \n",
              "156                  Ömer Sümer  2021                                Learning   \n",
              "157                  Ömer Sümer  2021                                Learning   \n",
              "158                  Ömer Sümer  2021                                Learning   \n",
              "159                 Phuong Pham  2017                                Learning   \n",
              "160                 Phuong Pham  2017                                Learning   \n",
              "161                 Phuong Pham  2017                                Learning   \n",
              "162            Lujie Karen Chen  2021                                Learning   \n",
              "163            Lujie Karen Chen  2021                                Learning   \n",
              "164            Lujie Karen Chen  2021                                Learning   \n",
              "165              Milica Vujovic  2020                                Learning   \n",
              "166              Milica Vujovic  2020                                Learning   \n",
              "167              Milica Vujovic  2020                                Learning   \n",
              "168             Jauwairia Nasir  2022                                Learning   \n",
              "169             Jauwairia Nasir  2022                                Learning   \n",
              "170             Jauwairia Nasir  2022                                Learning   \n",
              "171            Daniele Di Mitri  2021                                Training   \n",
              "172            Daniele Di Mitri  2021                                Training   \n",
              "173            Daniele Di Mitri  2021                                Training   \n",
              "174              Sofia Tancredi  2022                                Learning   \n",
              "175              Sofia Tancredi  2022                                Learning   \n",
              "176              Sofia Tancredi  2022                                Learning   \n",
              "177              Luis P. Prieto  2018                                Learning   \n",
              "178              Luis P. Prieto  2018                                Learning   \n",
              "179              Luis P. Prieto  2018                                Learning   \n",
              "180          Serena Lee-Cultura  2021                                Learning   \n",
              "181          Serena Lee-Cultura  2021                                Learning   \n",
              "182          Serena Lee-Cultura  2021                                Learning   \n",
              "183                   René Noël  2022                                Learning   \n",
              "184                   René Noël  2022                                Learning   \n",
              "185                   René Noël  2022                                Learning   \n",
              "186             Marcelo Worsley  2021                                Learning   \n",
              "187             Marcelo Worsley  2021                                Learning   \n",
              "188             Marcelo Worsley  2021                                Learning   \n",
              "189               Daniel Spikol  2018                                Learning   \n",
              "190               Daniel Spikol  2018                                Learning   \n",
              "191               Daniel Spikol  2018                                Learning   \n",
              "192               Teresa Morell  2022                                Training   \n",
              "193               Teresa Morell  2022                                Training   \n",
              "194               Teresa Morell  2022                                Training   \n",
              "195             Marcelo Worsley  2018                                Learning   \n",
              "196             Marcelo Worsley  2018                                Learning   \n",
              "197             Marcelo Worsley  2018                                Learning   \n",
              "198  Khaleel Asyraaf Mat Sanusi  2021                                Training   \n",
              "199  Khaleel Asyraaf Mat Sanusi  2021                                Training   \n",
              "200  Khaleel Asyraaf Mat Sanusi  2021                                Training   \n",
              "201           Michail Giannakos  2019                                Training   \n",
              "202           Michail Giannakos  2019                                Training   \n",
              "203           Michail Giannakos  2019                                Training   \n",
              "204                 Xiaoyang Ma  2021                                Learning   \n",
              "205                 Xiaoyang Ma  2021                                Learning   \n",
              "206                 Xiaoyang Ma  2021                                Learning   \n",
              "207            Nathan Henderson  2019                                Training   \n",
              "208            Nathan Henderson  2019                                Training   \n",
              "209            Nathan Henderson  2019                                Training   \n",
              "210                T. S. Ashwin  2020                                Learning   \n",
              "211                T. S. Ashwin  2020                                Learning   \n",
              "212                T. S. Ashwin  2020                                Learning   \n",
              "213                   Yingbo Ma  2022                                Learning   \n",
              "214                   Yingbo Ma  2022                                Learning   \n",
              "215                   Yingbo Ma  2022                                Learning   \n",
              "216          Vanessa Echeverria  2019                                Training   \n",
              "217          Vanessa Echeverria  2019                                Training   \n",
              "218          Vanessa Echeverria  2019                                Training   \n",
              "\n",
              "                    Mapped Data Collection Mediums  \\\n",
              "0                                      VIDEO,AUDIO   \n",
              "1                                      VIDEO,AUDIO   \n",
              "2                                      VIDEO,AUDIO   \n",
              "3                               VIDEO,AUDIO,SENSOR   \n",
              "4                               VIDEO,AUDIO,SENSOR   \n",
              "5                               VIDEO,AUDIO,SENSOR   \n",
              "6                                   VIDEO,LOGS,EYE   \n",
              "7                                   VIDEO,LOGS,EYE   \n",
              "8                                   VIDEO,LOGS,EYE   \n",
              "9                                 VIDEO,EYE,SENSOR   \n",
              "10                                VIDEO,EYE,SENSOR   \n",
              "11                                VIDEO,EYE,SENSOR   \n",
              "12                                VIDEO,EYE,SENSOR   \n",
              "13                                VIDEO,EYE,SENSOR   \n",
              "14                                VIDEO,EYE,SENSOR   \n",
              "15                               VIDEO,MOTION,LOGS   \n",
              "16                               VIDEO,MOTION,LOGS   \n",
              "17                               VIDEO,MOTION,LOGS   \n",
              "18                                 VIDEO,AUDIO,PPA   \n",
              "19                                 VIDEO,AUDIO,PPA   \n",
              "20                                 VIDEO,AUDIO,PPA   \n",
              "21                                  LOGS,AUDIO,EYE   \n",
              "22                                  LOGS,AUDIO,EYE   \n",
              "23                                  LOGS,AUDIO,EYE   \n",
              "24                            EYE,LOGS,VIDEO,AUDIO   \n",
              "25                            EYE,LOGS,VIDEO,AUDIO   \n",
              "26                            EYE,LOGS,VIDEO,AUDIO   \n",
              "27                                      VIDEO,LOGS   \n",
              "28                                      VIDEO,LOGS   \n",
              "29                                      VIDEO,LOGS   \n",
              "30                                    VIDEO,SENSOR   \n",
              "31                                    VIDEO,SENSOR   \n",
              "32                                    VIDEO,SENSOR   \n",
              "33                                  VIDEO,EYE,LOGS   \n",
              "34                                  VIDEO,EYE,LOGS   \n",
              "35                                  VIDEO,EYE,LOGS   \n",
              "36                                           VIDEO   \n",
              "37                                           VIDEO   \n",
              "38                                           VIDEO   \n",
              "39                                SURVEY,PPA,AUDIO   \n",
              "40                                SURVEY,PPA,AUDIO   \n",
              "41                                SURVEY,PPA,AUDIO   \n",
              "42                                SENSOR,VIDEO,PPA   \n",
              "43                                SENSOR,VIDEO,PPA   \n",
              "44                                SENSOR,VIDEO,PPA   \n",
              "45                                      VIDEO,LOGS   \n",
              "46                                      VIDEO,LOGS   \n",
              "47                                      VIDEO,LOGS   \n",
              "48                                 VIDEO,AUDIO,PPA   \n",
              "49                                 VIDEO,AUDIO,PPA   \n",
              "50                                 VIDEO,AUDIO,PPA   \n",
              "51                             VIDEO,AUDIO,PPA,RPA   \n",
              "52                             VIDEO,AUDIO,PPA,RPA   \n",
              "53                             VIDEO,AUDIO,PPA,RPA   \n",
              "54                                   AUDIO,PPA,RPA   \n",
              "55                                   AUDIO,PPA,RPA   \n",
              "56                                   AUDIO,PPA,RPA   \n",
              "57                            VIDEO,AUDIO,LOGS,RPA   \n",
              "58                            VIDEO,AUDIO,LOGS,RPA   \n",
              "59                            VIDEO,AUDIO,LOGS,RPA   \n",
              "60                                      PPA,SENSOR   \n",
              "61                                      PPA,SENSOR   \n",
              "62                                      PPA,SENSOR   \n",
              "63                        VIDEO,AUDIO,PPA,RPA,LOGS   \n",
              "64                        VIDEO,AUDIO,PPA,RPA,LOGS   \n",
              "65                        VIDEO,AUDIO,PPA,RPA,LOGS   \n",
              "66                                 SENSOR,LOGS,RPA   \n",
              "67                                 SENSOR,LOGS,RPA   \n",
              "68                                 SENSOR,LOGS,RPA   \n",
              "69                                   PPA,VIDEO,EYE   \n",
              "70                                   PPA,VIDEO,EYE   \n",
              "71                                   PPA,VIDEO,EYE   \n",
              "72                                  VIDEO,LOGS,PPA   \n",
              "73                                  VIDEO,LOGS,PPA   \n",
              "74                                  VIDEO,LOGS,PPA   \n",
              "75                                        LOGS,PPA   \n",
              "76                                        LOGS,PPA   \n",
              "77                                        LOGS,PPA   \n",
              "78                    LOGS,MOTION,SENSOR,RPA,VIDEO   \n",
              "79                    LOGS,MOTION,SENSOR,RPA,VIDEO   \n",
              "80                    LOGS,MOTION,SENSOR,RPA,VIDEO   \n",
              "81                                       PPA,INTER   \n",
              "82                                       PPA,INTER   \n",
              "83                                       PPA,INTER   \n",
              "84                                     VIDEO,AUDIO   \n",
              "85                                     VIDEO,AUDIO   \n",
              "86                                     VIDEO,AUDIO   \n",
              "87                            AUDIO,SURVEY,PPA,RPA   \n",
              "88                            AUDIO,SURVEY,PPA,RPA   \n",
              "89                            AUDIO,SURVEY,PPA,RPA   \n",
              "90                              LOGS,VIDEO,EYE,PPA   \n",
              "91                              LOGS,VIDEO,EYE,PPA   \n",
              "92                              LOGS,VIDEO,EYE,PPA   \n",
              "93                          SENSOR,LOGS,MOTION,PPA   \n",
              "94                          SENSOR,LOGS,MOTION,PPA   \n",
              "95                          SENSOR,LOGS,MOTION,PPA   \n",
              "96                              SENSOR,VIDEO,AUDIO   \n",
              "97                              SENSOR,VIDEO,AUDIO   \n",
              "98                              SENSOR,VIDEO,AUDIO   \n",
              "99                                 AUDIO,VIDEO,PPA   \n",
              "100                                AUDIO,VIDEO,PPA   \n",
              "101                                AUDIO,VIDEO,PPA   \n",
              "102                                   AUDIO,SURVEY   \n",
              "103                                   AUDIO,SURVEY   \n",
              "104                                   AUDIO,SURVEY   \n",
              "105                    VIDEO,AUDIO,LOGS,SCREEN,PPA   \n",
              "106                    VIDEO,AUDIO,LOGS,SCREEN,PPA   \n",
              "107                    VIDEO,AUDIO,LOGS,SCREEN,PPA   \n",
              "108                  AUDIO,VIDEO,SCREEN,SURVEY,PPA   \n",
              "109                  AUDIO,VIDEO,SCREEN,SURVEY,PPA   \n",
              "110                  AUDIO,VIDEO,SCREEN,SURVEY,PPA   \n",
              "111                          LOGS,VIDEO,SCREEN,PPA   \n",
              "112                          LOGS,VIDEO,SCREEN,PPA   \n",
              "113                          LOGS,VIDEO,SCREEN,PPA   \n",
              "114                        SURVEY,LOGS,AUDIO,VIDEO   \n",
              "115                        SURVEY,LOGS,AUDIO,VIDEO   \n",
              "116                        SURVEY,LOGS,AUDIO,VIDEO   \n",
              "117                                    VIDEO,AUDIO   \n",
              "118                                    VIDEO,AUDIO   \n",
              "119                                    VIDEO,AUDIO   \n",
              "120                       LOGS,VIDEO,SENSOR,MOTION   \n",
              "121                       LOGS,VIDEO,SENSOR,MOTION   \n",
              "122                       LOGS,VIDEO,SENSOR,MOTION   \n",
              "123   VIDEO,LOGS,SCREEN,AUDIO,SURVEY,RPA,PPA,INTER   \n",
              "124   VIDEO,LOGS,SCREEN,AUDIO,SURVEY,RPA,PPA,INTER   \n",
              "125   VIDEO,LOGS,SCREEN,AUDIO,SURVEY,RPA,PPA,INTER   \n",
              "126                         AUDIO,VIDEO,PPA,SURVEY   \n",
              "127                         AUDIO,VIDEO,PPA,SURVEY   \n",
              "128                         AUDIO,VIDEO,PPA,SURVEY   \n",
              "129                          VIDEO,SENSOR,EYE,LOGS   \n",
              "130                          VIDEO,SENSOR,EYE,LOGS   \n",
              "131                          VIDEO,SENSOR,EYE,LOGS   \n",
              "132                          LOGS,AUDIO,SCREEN,PPA   \n",
              "133                          LOGS,AUDIO,SCREEN,PPA   \n",
              "134                          LOGS,AUDIO,SCREEN,PPA   \n",
              "135                   VIDEO,AUDIO,SENSOR,PPA,INTER   \n",
              "136                   VIDEO,AUDIO,SENSOR,PPA,INTER   \n",
              "137                   VIDEO,AUDIO,SENSOR,PPA,INTER   \n",
              "138                                    VIDEO,AUDIO   \n",
              "139                                    VIDEO,AUDIO   \n",
              "140                                    VIDEO,AUDIO   \n",
              "141                         SCREEN,INTER,PPA,AUDIO   \n",
              "142                         SCREEN,INTER,PPA,AUDIO   \n",
              "143                         SCREEN,INTER,PPA,AUDIO   \n",
              "144                           VIDEO,LOGS,INTER,PPA   \n",
              "145                           VIDEO,LOGS,INTER,PPA   \n",
              "146                           VIDEO,LOGS,INTER,PPA   \n",
              "147                          VIDEO,EYE,SENSOR,LOGS   \n",
              "148                          VIDEO,EYE,SENSOR,LOGS   \n",
              "149                          VIDEO,EYE,SENSOR,LOGS   \n",
              "150                   SURVEY,LOGS,EYE,SENSOR,VIDEO   \n",
              "151                   SURVEY,LOGS,EYE,SENSOR,VIDEO   \n",
              "152                   SURVEY,LOGS,EYE,SENSOR,VIDEO   \n",
              "153                               VIDEO,AUDIO,LOGS   \n",
              "154                               VIDEO,AUDIO,LOGS   \n",
              "155                               VIDEO,AUDIO,LOGS   \n",
              "156                                          VIDEO   \n",
              "157                                          VIDEO   \n",
              "158                                          VIDEO   \n",
              "159                                   VIDEO,SURVEY   \n",
              "160                                   VIDEO,SURVEY   \n",
              "161                                   VIDEO,SURVEY   \n",
              "162                             AUDIO,VIDEO,SURVEY   \n",
              "163                             AUDIO,VIDEO,SURVEY   \n",
              "164                             AUDIO,VIDEO,SURVEY   \n",
              "165                                   VIDEO,MOTION   \n",
              "166                                   VIDEO,MOTION   \n",
              "167                                   VIDEO,MOTION   \n",
              "168                    VIDEO,AUDIO,LOGS,PPA,SURVEY   \n",
              "169                    VIDEO,AUDIO,LOGS,PPA,SURVEY   \n",
              "170                    VIDEO,AUDIO,LOGS,PPA,SURVEY   \n",
              "171                SURVEY,LOGS,VIDEO,SENSOR,MOTION   \n",
              "172                SURVEY,LOGS,VIDEO,SENSOR,MOTION   \n",
              "173                SURVEY,LOGS,VIDEO,SENSOR,MOTION   \n",
              "174                          EYE,VIDEO,AUDIO,INTER   \n",
              "175                          EYE,VIDEO,AUDIO,INTER   \n",
              "176                          EYE,VIDEO,AUDIO,INTER   \n",
              "177                         EYE,VIDEO,AUDIO,MOTION   \n",
              "178                         EYE,VIDEO,AUDIO,MOTION   \n",
              "179                         EYE,VIDEO,AUDIO,MOTION   \n",
              "180                          VIDEO,SENSOR,EYE,LOGS   \n",
              "181                          VIDEO,SENSOR,EYE,LOGS   \n",
              "182                          VIDEO,SENSOR,EYE,LOGS   \n",
              "183                          AUDIO,VIDEO,RPA,INTER   \n",
              "184                          AUDIO,VIDEO,RPA,INTER   \n",
              "185                          AUDIO,VIDEO,RPA,INTER   \n",
              "186  AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS   \n",
              "187  AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS   \n",
              "188  AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS   \n",
              "189                       VIDEO,AUDIO,LOGS,PPA,RPA   \n",
              "190                       VIDEO,AUDIO,LOGS,PPA,RPA   \n",
              "191                       VIDEO,AUDIO,LOGS,PPA,RPA   \n",
              "192                                VIDEO,AUDIO,PPA   \n",
              "193                                VIDEO,AUDIO,PPA   \n",
              "194                                VIDEO,AUDIO,PPA   \n",
              "195                                      VIDEO,PPA   \n",
              "196                                      VIDEO,PPA   \n",
              "197                                      VIDEO,PPA   \n",
              "198                             VIDEO,MOTION,INTER   \n",
              "199                             VIDEO,MOTION,INTER   \n",
              "200                             VIDEO,MOTION,INTER   \n",
              "201                          LOGS,EYE,SENSOR,VIDEO   \n",
              "202                          LOGS,EYE,SENSOR,VIDEO   \n",
              "203                          LOGS,EYE,SENSOR,VIDEO   \n",
              "204                                          VIDEO   \n",
              "205                                          VIDEO   \n",
              "206                                          VIDEO   \n",
              "207                               VIDEO,SENSOR,RPA   \n",
              "208                               VIDEO,SENSOR,RPA   \n",
              "209                               VIDEO,SENSOR,RPA   \n",
              "210                                      VIDEO,PPA   \n",
              "211                                      VIDEO,PPA   \n",
              "212                                      VIDEO,PPA   \n",
              "213                                    VIDEO,AUDIO   \n",
              "214                                    VIDEO,AUDIO   \n",
              "215                                    VIDEO,AUDIO   \n",
              "216           VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER   \n",
              "217           VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER   \n",
              "218           VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER   \n",
              "\n",
              "                                     Mapped Modalities  \\\n",
              "0                                       POSE,GAZE,PROS   \n",
              "1                                       POSE,GAZE,PROS   \n",
              "2                                       POSE,GAZE,PROS   \n",
              "3                                             QUAL,EDA   \n",
              "4                                             QUAL,EDA   \n",
              "5                                             QUAL,EDA   \n",
              "6                                 AFFECT,GAZE,LOGS,PPA   \n",
              "7                                 AFFECT,GAZE,LOGS,PPA   \n",
              "8                                 AFFECT,GAZE,LOGS,PPA   \n",
              "9                             PULSE,TEMP,EDA,GAZE,POSE   \n",
              "10                            PULSE,TEMP,EDA,GAZE,POSE   \n",
              "11                            PULSE,TEMP,EDA,GAZE,POSE   \n",
              "12                      EDA,TEMP,PULSE,EEG,GAZE,AFFECT   \n",
              "13                      EDA,TEMP,PULSE,EEG,GAZE,AFFECT   \n",
              "14                      EDA,TEMP,PULSE,EEG,GAZE,AFFECT   \n",
              "15                                           POSE,LOGS   \n",
              "16                                           POSE,LOGS   \n",
              "17                                           POSE,LOGS   \n",
              "18                                       POSE,PROS,PPA   \n",
              "19                                       POSE,PROS,PPA   \n",
              "20                                       POSE,PROS,PPA   \n",
              "21                           GAZE,LOGS,PROS,TRANS,QUAL   \n",
              "22                           GAZE,LOGS,PROS,TRANS,QUAL   \n",
              "23                           GAZE,LOGS,PROS,TRANS,QUAL   \n",
              "24                                 GAZE,LOGS,PROS,POSE   \n",
              "25                                 GAZE,LOGS,PROS,POSE   \n",
              "26                                 GAZE,LOGS,PROS,POSE   \n",
              "27                                    POSE,AFFECT,LOGS   \n",
              "28                                    POSE,AFFECT,LOGS   \n",
              "29                                    POSE,AFFECT,LOGS   \n",
              "30                                   PULSE,AFFECT,GAZE   \n",
              "31                                   PULSE,AFFECT,GAZE   \n",
              "32                                   PULSE,AFFECT,GAZE   \n",
              "33                          POSE,GEST,AFFECT,GAZE,LOGS   \n",
              "34                          POSE,GEST,AFFECT,GAZE,LOGS   \n",
              "35                          POSE,GEST,AFFECT,GAZE,LOGS   \n",
              "36                                        AFFECT,PULSE   \n",
              "37                                        AFFECT,PULSE   \n",
              "38                                        AFFECT,PULSE   \n",
              "39                      INTER,SURVEY,TRANS,PROS,AFFECT   \n",
              "40                      INTER,SURVEY,TRANS,PROS,AFFECT   \n",
              "41                      INTER,SURVEY,TRANS,PROS,AFFECT   \n",
              "42                            TEMP,PULSE,EDA,BP,AFFECT   \n",
              "43                            TEMP,PULSE,EDA,BP,AFFECT   \n",
              "44                            TEMP,PULSE,EDA,BP,AFFECT   \n",
              "45                                       POSE,ACT,LOGS   \n",
              "46                                       POSE,ACT,LOGS   \n",
              "47                                       POSE,ACT,LOGS   \n",
              "48                                   PPA,RPA,POSE,GEST   \n",
              "49                                   PPA,RPA,POSE,GEST   \n",
              "50                                   PPA,RPA,POSE,GEST   \n",
              "51                                   TRANS,AFFECT,QUAL   \n",
              "52                                   TRANS,AFFECT,QUAL   \n",
              "53                                   TRANS,AFFECT,QUAL   \n",
              "54                                            RPA,PROS   \n",
              "55                                            RPA,PROS   \n",
              "56                                            RPA,PROS   \n",
              "57                 AFFECT,POSE,LOGS,RPA,GAZE,PROS,GEST   \n",
              "58                 AFFECT,POSE,LOGS,RPA,GAZE,PROS,GEST   \n",
              "59                 AFFECT,POSE,LOGS,RPA,GAZE,PROS,GEST   \n",
              "60                                       PPA,PULSE,EDA   \n",
              "61                                       PPA,PULSE,EDA   \n",
              "62                                       PPA,PULSE,EDA   \n",
              "63                              POSE,PROS,PPA,RPA,LOGS   \n",
              "64                              POSE,PROS,PPA,RPA,LOGS   \n",
              "65                              POSE,PROS,PPA,RPA,LOGS   \n",
              "66                                 EDA,LOGS,RPA,AFFECT   \n",
              "67                                 EDA,LOGS,RPA,AFFECT   \n",
              "68                                 EDA,LOGS,RPA,AFFECT   \n",
              "69                                    LOGS,AFFECT,GAZE   \n",
              "70                                    LOGS,AFFECT,GAZE   \n",
              "71                                    LOGS,AFFECT,GAZE   \n",
              "72                                   LOGS,POSE,RPA,ACT   \n",
              "73                                   LOGS,POSE,RPA,ACT   \n",
              "74                                   LOGS,POSE,RPA,ACT   \n",
              "75                                            LOGS,PPA   \n",
              "76                                            LOGS,PPA   \n",
              "77                                            LOGS,PPA   \n",
              "78                                   POSE,EDA,LOGS,RPA   \n",
              "79                                   POSE,EDA,LOGS,RPA   \n",
              "80                                   POSE,EDA,LOGS,RPA   \n",
              "81                                           PPA,TRANS   \n",
              "82                                           PPA,TRANS   \n",
              "83                                           PPA,TRANS   \n",
              "84                                                 RPA   \n",
              "85                                                 RPA   \n",
              "86                                                 RPA   \n",
              "87                                SURVEY,TRANS,PPA,RPA   \n",
              "88                                SURVEY,TRANS,PPA,RPA   \n",
              "89                                SURVEY,TRANS,PPA,RPA   \n",
              "90                                AFFECT,LOGS,GAZE,PPA   \n",
              "91                                AFFECT,LOGS,GAZE,PPA   \n",
              "92                                AFFECT,LOGS,GAZE,PPA   \n",
              "93                                    PULSE,ACT,AFFECT   \n",
              "94                                    PULSE,ACT,AFFECT   \n",
              "95                                    PULSE,ACT,AFFECT   \n",
              "96                                     EDA,AFFECT,QUAL   \n",
              "97                                     EDA,AFFECT,QUAL   \n",
              "98                                     EDA,AFFECT,QUAL   \n",
              "99                                    POSE,PROS,AFFECT   \n",
              "100                                   POSE,PROS,AFFECT   \n",
              "101                                   POSE,PROS,AFFECT   \n",
              "102                                        AFFECT,LOGS   \n",
              "103                                        AFFECT,LOGS   \n",
              "104                                        AFFECT,LOGS   \n",
              "105                                     TRANS,QUAL,PPA   \n",
              "106                                     TRANS,QUAL,PPA   \n",
              "107                                     TRANS,QUAL,PPA   \n",
              "108                                  PROS,ACT,GEST,PPA   \n",
              "109                                  PROS,ACT,GEST,PPA   \n",
              "110                                  PROS,ACT,GEST,PPA   \n",
              "111                               AFFECT,POSE,LOGS,PPA   \n",
              "112                               AFFECT,POSE,LOGS,PPA   \n",
              "113                               AFFECT,POSE,LOGS,PPA   \n",
              "114                              LOGS,SURVEY,GAZE,PROS   \n",
              "115                              LOGS,SURVEY,GAZE,PROS   \n",
              "116                              LOGS,SURVEY,GAZE,PROS   \n",
              "117                        GEST,TRANS,PROS,SURVEY,GAZE   \n",
              "118                        GEST,TRANS,PROS,SURVEY,GAZE   \n",
              "119                        GEST,TRANS,PROS,SURVEY,GAZE   \n",
              "120                                      POSE,EMG,GEST   \n",
              "121                                      POSE,EMG,GEST   \n",
              "122                                      POSE,EMG,GEST   \n",
              "123             AFFECT,LOGS,POSE,QUAL,INTER,SURVEY,RPA   \n",
              "124             AFFECT,LOGS,POSE,QUAL,INTER,SURVEY,RPA   \n",
              "125             AFFECT,LOGS,POSE,QUAL,INTER,SURVEY,RPA   \n",
              "126                                 PPA,GAZE,POSE,PROS   \n",
              "127                                 PPA,GAZE,POSE,PROS   \n",
              "128                                 PPA,GAZE,POSE,PROS   \n",
              "129     ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP   \n",
              "130     ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP   \n",
              "131     ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP   \n",
              "132                            LOGS,TRANS,ACT,QUAL,PPA   \n",
              "133                            LOGS,TRANS,ACT,QUAL,PPA   \n",
              "134                            LOGS,TRANS,ACT,QUAL,PPA   \n",
              "135                   GEST,PPA,EDA,ACT,PROS,QUAL,INTER   \n",
              "136                   GEST,PPA,EDA,ACT,PROS,QUAL,INTER   \n",
              "137                   GEST,PPA,EDA,ACT,PROS,QUAL,INTER   \n",
              "138                                          PROS,GEST   \n",
              "139                                          PROS,GEST   \n",
              "140                                          PROS,GEST   \n",
              "141                                   INTER,QUAL,TRANS   \n",
              "142                                   INTER,QUAL,TRANS   \n",
              "143                                   INTER,QUAL,TRANS   \n",
              "144                           GAZE,LOGS,INTER,PPA,GEST   \n",
              "145                           GAZE,LOGS,INTER,PPA,GEST   \n",
              "146                           GAZE,LOGS,INTER,PPA,GEST   \n",
              "147                    LOGS,GAZE,EDA,PULSE,AFFECT,TEMP   \n",
              "148                    LOGS,GAZE,EDA,PULSE,AFFECT,TEMP   \n",
              "149                    LOGS,GAZE,EDA,PULSE,AFFECT,TEMP   \n",
              "150      PULSE,AFFECT,EEG,GAZE,LOGS,BP,TEMP,EDA,SURVEY   \n",
              "151      PULSE,AFFECT,EEG,GAZE,LOGS,BP,TEMP,EDA,SURVEY   \n",
              "152      PULSE,AFFECT,EEG,GAZE,LOGS,BP,TEMP,EDA,SURVEY   \n",
              "153                                          POSE,PROS   \n",
              "154                                          POSE,PROS   \n",
              "155                                          POSE,PROS   \n",
              "156                                        POSE,AFFECT   \n",
              "157                                        POSE,AFFECT   \n",
              "158                                        POSE,AFFECT   \n",
              "159                                PULSE,AFFECT,SURVEY   \n",
              "160                                PULSE,AFFECT,SURVEY   \n",
              "161                                PULSE,AFFECT,SURVEY   \n",
              "162                      PROS,GAZE,TRANS,AFFECT,SURVEY   \n",
              "163                      PROS,GAZE,TRANS,AFFECT,SURVEY   \n",
              "164                      PROS,GAZE,TRANS,AFFECT,SURVEY   \n",
              "165                                      POSE,GEST,ACT   \n",
              "166                                      POSE,GEST,ACT   \n",
              "167                                      POSE,GEST,ACT   \n",
              "168                        PROS,AFFECT,GAZE,TRANS,LOGS   \n",
              "169                        PROS,AFFECT,GAZE,TRANS,LOGS   \n",
              "170                        PROS,AFFECT,GAZE,TRANS,LOGS   \n",
              "171                               POSE,EMG,GEST,SURVEY   \n",
              "172                               POSE,EMG,GEST,SURVEY   \n",
              "173                               POSE,EMG,GEST,SURVEY   \n",
              "174                         GAZE,GEST,TRANS,POSE,INTER   \n",
              "175                         GAZE,GEST,TRANS,POSE,INTER   \n",
              "176                         GAZE,GEST,TRANS,POSE,INTER   \n",
              "177                                GAZE,PROS,ACT,PIXEL   \n",
              "178                                GAZE,PROS,ACT,PIXEL   \n",
              "179                                GAZE,PROS,ACT,PIXEL   \n",
              "180  ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP,BP   \n",
              "181  ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP,BP   \n",
              "182  ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP,BP   \n",
              "183                           PROS,POSE,RPA,INTER,QUAL   \n",
              "184                           PROS,POSE,RPA,INTER,QUAL   \n",
              "185                           PROS,POSE,RPA,INTER,QUAL   \n",
              "186             PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS   \n",
              "187             PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS   \n",
              "188             PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS   \n",
              "189                        POSE,GEST,PROS,LOGS,PPA,RPA   \n",
              "190                        POSE,GEST,PROS,LOGS,PPA,RPA   \n",
              "191                        POSE,GEST,PROS,LOGS,PPA,RPA   \n",
              "192                            TRANS,PPA,QUAL,POSE,ACT   \n",
              "193                            TRANS,PPA,QUAL,POSE,ACT   \n",
              "194                            TRANS,PPA,QUAL,POSE,ACT   \n",
              "195                                      GEST,QUAL,PPA   \n",
              "196                                      GEST,QUAL,PPA   \n",
              "197                                      GEST,QUAL,PPA   \n",
              "198                                POSE,GEST,ACT,INTER   \n",
              "199                                POSE,GEST,ACT,INTER   \n",
              "200                                POSE,GEST,ACT,INTER   \n",
              "201               EEG,GAZE,LOGS,PULSE,EDA,TEMP,BP,POSE   \n",
              "202               EEG,GAZE,LOGS,PULSE,EDA,TEMP,BP,POSE   \n",
              "203               EEG,GAZE,LOGS,PULSE,EDA,TEMP,BP,POSE   \n",
              "204                                          POSE,GAZE   \n",
              "205                                          POSE,GAZE   \n",
              "206                                          POSE,GAZE   \n",
              "207                                    POSE,EDA,AFFECT   \n",
              "208                                    POSE,EDA,AFFECT   \n",
              "209                                    POSE,EDA,AFFECT   \n",
              "210                                   AFFECT,POSE,GEST   \n",
              "211                                   AFFECT,POSE,GEST   \n",
              "212                                   AFFECT,POSE,GEST   \n",
              "213                         TRANS,PROS,SPECT,GAZE,POSE   \n",
              "214                         TRANS,PROS,SPECT,GAZE,POSE   \n",
              "215                         TRANS,PROS,SPECT,GAZE,POSE   \n",
              "216                 POSE,LOGS,TRANS,EDA,ACT,PROS,INTER   \n",
              "217                 POSE,LOGS,TRANS,EDA,ACT,PROS,INTER   \n",
              "218                 POSE,LOGS,TRANS,EDA,ACT,PROS,INTER   \n",
              "\n",
              "     Mapped Analysis Methods Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0                  CLS,CLUST                LATE                     MLPALA   \n",
              "1                  CLS,CLUST                LATE                     MLPALA   \n",
              "2                  CLS,CLUST                LATE                     MLPALA   \n",
              "3             PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "4             PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "5             PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "6                  CLS,STATS                 MID                       BJET   \n",
              "7                  CLS,STATS                 MID                       BJET   \n",
              "8                  CLS,STATS                 MID                       BJET   \n",
              "9                        CLS                 MID                        COG   \n",
              "10                       CLS                 MID                        COG   \n",
              "11                       CLS                 MID                        COG   \n",
              "12            CLUST,CLS,PATT                 MID                        LAK   \n",
              "13            CLUST,CLS,PATT                 MID                        LAK   \n",
              "14            CLUST,CLS,PATT                 MID                        LAK   \n",
              "15                       CLS                 MID                       CAIM   \n",
              "16                       CLS                 MID                       CAIM   \n",
              "17                       CLS                 MID                       CAIM   \n",
              "18                     STATS                 OTH                       BJET   \n",
              "19                     STATS                 OTH                       BJET   \n",
              "20                     STATS                 OTH                       BJET   \n",
              "21                       REG                 MID                       BJET   \n",
              "22                       REG                 MID                       BJET   \n",
              "23                       REG                 MID                       BJET   \n",
              "24                       CLS                 MID                      ICALT   \n",
              "25                       CLS                 MID                      ICALT   \n",
              "26                       CLS                 MID                      ICALT   \n",
              "27                       CLS                LATE                    T-CIAIG   \n",
              "28                       CLS                LATE                    T-CIAIG   \n",
              "29                       CLS                LATE                    T-CIAIG   \n",
              "30                       CLS                 MID                       TALE   \n",
              "31                       CLS                 MID                       TALE   \n",
              "32                       CLS                 MID                       TALE   \n",
              "33                       REG                 MID                       ICMI   \n",
              "34                       REG                 MID                       ICMI   \n",
              "35                       REG                 MID                       ICMI   \n",
              "36                       REG                LATE                        ITS   \n",
              "37                       REG                LATE                        ITS   \n",
              "38                       REG                LATE                        ITS   \n",
              "39             CLS,REG,STATS                 OTH              MMLA Handbook   \n",
              "40             CLS,REG,STATS                 OTH              MMLA Handbook   \n",
              "41             CLS,REG,STATS                 OTH              MMLA Handbook   \n",
              "42                 REG,STATS                 MID                        IDC   \n",
              "43                 REG,STATS                 MID                        IDC   \n",
              "44                 REG,STATS                 MID                        IDC   \n",
              "45                       CLS                 MID                       PPIG   \n",
              "46                       CLS                 MID                       PPIG   \n",
              "47                       CLS                 MID                       PPIG   \n",
              "48          CLUST,PATT,STATS                 OTH                        EDM   \n",
              "49          CLUST,PATT,STATS                 OTH                        EDM   \n",
              "50          CLUST,PATT,STATS                 OTH                        EDM   \n",
              "51                      QUAL                 OTH                       ICQE   \n",
              "52                      QUAL                 OTH                       ICQE   \n",
              "53                      QUAL                 OTH                       ICQE   \n",
              "54            QUAL,NET,STATS                 OTH                     Access   \n",
              "55            QUAL,NET,STATS                 OTH                     Access   \n",
              "56            QUAL,NET,STATS                 OTH                     Access   \n",
              "57                 CLS,STATS              HYBRID                       BJET   \n",
              "58                 CLS,STATS              HYBRID                       BJET   \n",
              "59                 CLS,STATS              HYBRID                       BJET   \n",
              "60                 STATS,CLS               EARLY                       BJET   \n",
              "61                 STATS,CLS               EARLY                       BJET   \n",
              "62                 STATS,CLS               EARLY                       BJET   \n",
              "63                STATS,QUAL                 OTH                       ICLS   \n",
              "64                STATS,QUAL                 OTH                       ICLS   \n",
              "65                STATS,QUAL                 OTH                       ICLS   \n",
              "66                      QUAL                 OTH                        TLT   \n",
              "67                      QUAL                 OTH                        TLT   \n",
              "68                      QUAL                 OTH                        TLT   \n",
              "69                 CLS,STATS              HYBRID                        MMM   \n",
              "70                 CLS,STATS              HYBRID                        MMM   \n",
              "71                 CLS,STATS              HYBRID                        MMM   \n",
              "72            CLS,QUAL,STATS            MID,LATE                        CEE   \n",
              "73            CLS,QUAL,STATS            MID,LATE                        CEE   \n",
              "74            CLS,QUAL,STATS            MID,LATE                        CEE   \n",
              "75                 CLS,STATS                 MID                        FIE   \n",
              "76                 CLS,STATS                 MID                        FIE   \n",
              "77                 CLS,STATS                 MID                        FIE   \n",
              "78                      QUAL                 OTH                        CHI   \n",
              "79                      QUAL                 OTH                        CHI   \n",
              "80                      QUAL                 OTH                        CHI   \n",
              "81                QUAL,STATS                 OTH                Information   \n",
              "82                QUAL,STATS                 OTH                Information   \n",
              "83                QUAL,STATS                 OTH                Information   \n",
              "84                 CLUST,REG                 MID                      IJCCI   \n",
              "85                 CLUST,REG                 MID                      IJCCI   \n",
              "86                 CLUST,REG                 MID                      IJCCI   \n",
              "87                 NET,STATS             MID,OTH                    Sensors   \n",
              "88                 NET,STATS             MID,OTH                    Sensors   \n",
              "89                 NET,STATS             MID,OTH                    Sensors   \n",
              "90                       CLS         HYBRID,LATE                       JCHE   \n",
              "91                       CLS         HYBRID,LATE                       JCHE   \n",
              "92                       CLS         HYBRID,LATE                       JCHE   \n",
              "93                       REG                 MID                        LAK   \n",
              "94                       REG                 MID                        LAK   \n",
              "95                       REG                 MID                        LAK   \n",
              "96                      QUAL                 OTH                        LAI   \n",
              "97                      QUAL                 OTH                        LAI   \n",
              "98                      QUAL                 OTH                        LAI   \n",
              "99                 REG,STATS                 MID                       PLOS   \n",
              "100                REG,STATS                 MID                       PLOS   \n",
              "101                REG,STATS                 MID                       PLOS   \n",
              "102                      CLS                 MID                       BJET   \n",
              "103                      CLS                 MID                       BJET   \n",
              "104                      CLS                 MID                       BJET   \n",
              "105                CLS,STATS                 MID                       JCAL   \n",
              "106                CLS,STATS                 MID                       JCAL   \n",
              "107                CLS,STATS                 MID                       JCAL   \n",
              "108               STATS,PATT                 MID                        LAK   \n",
              "109               STATS,PATT                 MID                        LAK   \n",
              "110               STATS,PATT                 MID                        LAK   \n",
              "111                      CLS              HYBRID                        MIE   \n",
              "112                      CLS              HYBRID                        MIE   \n",
              "113                      CLS              HYBRID                        MIE   \n",
              "114                    STATS                 OTH                      ECGBL   \n",
              "115                    STATS                 OTH                      ECGBL   \n",
              "116                    STATS                 OTH                      ECGBL   \n",
              "117           STATS,CLS,QUAL                 MID                       ICMI   \n",
              "118           STATS,CLS,QUAL                 MID                       ICMI   \n",
              "119           STATS,CLS,QUAL                 MID                       ICMI   \n",
              "120                      CLS              HYBRID                       AIED   \n",
              "121                      CLS              HYBRID                       AIED   \n",
              "122                      CLS              HYBRID                       AIED   \n",
              "123           QUAL,STATS,CLS                LATE                        CHI   \n",
              "124           QUAL,STATS,CLS                LATE                        CHI   \n",
              "125           QUAL,STATS,CLS                LATE                        CHI   \n",
              "126           CLS,STATS,QUAL                LATE                        LAK   \n",
              "127           CLS,STATS,QUAL                LATE                        LAK   \n",
              "128           CLS,STATS,QUAL                LATE                        LAK   \n",
              "129               STATS,QUAL                 OTH                        IDC   \n",
              "130               STATS,QUAL                 OTH                        IDC   \n",
              "131               STATS,QUAL                 OTH                        IDC   \n",
              "132           STATS,REG,QUAL                 MID                        JLA   \n",
              "133           STATS,REG,QUAL                 MID                        JLA   \n",
              "134           STATS,REG,QUAL                 MID                        JLA   \n",
              "135    STATS,CLUST,QUAL,PATT               EARLY                     IJAIED   \n",
              "136    STATS,CLUST,QUAL,PATT               EARLY                     IJAIED   \n",
              "137    STATS,CLUST,QUAL,PATT               EARLY                     IJAIED   \n",
              "138           CLS,QUAL,STATS                 MID                INTERSPEECH   \n",
              "139           CLS,QUAL,STATS                 MID                INTERSPEECH   \n",
              "140           CLS,QUAL,STATS                 MID                INTERSPEECH   \n",
              "141                     QUAL                 OTH                        ILE   \n",
              "142                     QUAL                 OTH                        ILE   \n",
              "143                     QUAL                 OTH                        ILE   \n",
              "144               CLUST,QUAL              HYBRID                        LAK   \n",
              "145               CLUST,QUAL              HYBRID                        LAK   \n",
              "146               CLUST,QUAL              HYBRID                        LAK   \n",
              "147                      CLS              HYBRID                        JLA   \n",
              "148                      CLS              HYBRID                        JLA   \n",
              "149                      CLS              HYBRID                        JLA   \n",
              "150                     PATT              HYBRID                        TLT   \n",
              "151                     PATT              HYBRID                        TLT   \n",
              "152                     PATT              HYBRID                        TLT   \n",
              "153                      REG                 MID                       CSCL   \n",
              "154                      REG                 MID                       CSCL   \n",
              "155                      REG                 MID                       CSCL   \n",
              "156                      CLS          EARLY,LATE                        TAC   \n",
              "157                      CLS          EARLY,LATE                        TAC   \n",
              "158                      CLS          EARLY,LATE                        TAC   \n",
              "159                      CLS                 MID                       AIED   \n",
              "160                      CLS                 MID                       AIED   \n",
              "161                      CLS                 MID                       AIED   \n",
              "162                STATS,NET              HYBRID                       JEDM   \n",
              "163                STATS,NET              HYBRID                       JEDM   \n",
              "164                STATS,NET              HYBRID                       JEDM   \n",
              "165               STATS,QUAL                 OTH                       BJET   \n",
              "166               STATS,QUAL                 OTH                       BJET   \n",
              "167               STATS,QUAL                 OTH                       BJET   \n",
              "168     STATS,QUAL,CLUST,CLS              HYBRID                     IJCSCL   \n",
              "169     STATS,QUAL,CLUST,CLS              HYBRID                     IJCSCL   \n",
              "170     STATS,QUAL,CLUST,CLS              HYBRID                     IJCSCL   \n",
              "171           CLS,QUAL,STATS              HYBRID                     IJAIED   \n",
              "172           CLS,QUAL,STATS              HYBRID                     IJAIED   \n",
              "173           CLS,QUAL,STATS              HYBRID                     IJAIED   \n",
              "174          PATT,QUAL,STATS                 OTH              MMLA Handbook   \n",
              "175          PATT,QUAL,STATS                 OTH              MMLA Handbook   \n",
              "176          PATT,QUAL,STATS                 OTH              MMLA Handbook   \n",
              "177  NET,CLS,STATS,PATT,QUAL              HYBRID                       JCAL   \n",
              "178  NET,CLS,STATS,PATT,QUAL              HYBRID                       JCAL   \n",
              "179  NET,CLS,STATS,PATT,QUAL              HYBRID                       JCAL   \n",
              "180           STATS,QUAL,CLS              HYBRID                      IJCCI   \n",
              "181           STATS,QUAL,CLS              HYBRID                      IJCCI   \n",
              "182           STATS,QUAL,CLS              HYBRID                      IJCCI   \n",
              "183                     QUAL                 OTH                      DAMLE   \n",
              "184                     QUAL                 OTH                      DAMLE   \n",
              "185                     QUAL                 OTH                      DAMLE   \n",
              "186                     QUAL                 OTH                       HCII   \n",
              "187                     QUAL                 OTH                       HCII   \n",
              "188                     QUAL                 OTH                       HCII   \n",
              "189                  REG,CLS                 MID                       JCAL   \n",
              "190                  REG,CLS                 MID                       JCAL   \n",
              "191                  REG,CLS                 MID                       JCAL   \n",
              "192                     QUAL                 OTH                       JEAP   \n",
              "193                     QUAL                 OTH                       JEAP   \n",
              "194                     QUAL                 OTH                       JEAP   \n",
              "195           CLUST,CLS,QUAL              HYBRID                        LAK   \n",
              "196           CLUST,CLS,QUAL              HYBRID                        LAK   \n",
              "197           CLUST,CLS,QUAL              HYBRID                        LAK   \n",
              "198                 CLS,QUAL              HYBRID                    Sensors   \n",
              "199                 CLS,QUAL              HYBRID                    Sensors   \n",
              "200                 CLS,QUAL              HYBRID                    Sensors   \n",
              "201                REG,STATS              HYBRID                       IJIM   \n",
              "202                REG,STATS              HYBRID                       IJIM   \n",
              "203                REG,STATS              HYBRID                       IJIM   \n",
              "204                      CLS                 MID                      IJIET   \n",
              "205                      CLS                 MID                      IJIET   \n",
              "206                      CLS                 MID                      IJIET   \n",
              "207                      CLS              HYBRID                        EDM   \n",
              "208                      CLS              HYBRID                        EDM   \n",
              "209                      CLS              HYBRID                        EDM   \n",
              "210           CLS,STATS,PATT                 MID                      UMUAI   \n",
              "211           CLS,STATS,PATT                 MID                      UMUAI   \n",
              "212           CLS,STATS,PATT                 MID                      UMUAI   \n",
              "213                      CLS              HYBRID                        LAK   \n",
              "214                      CLS              HYBRID                        LAK   \n",
              "215                      CLS              HYBRID                        LAK   \n",
              "216                     QUAL                 OTH                        CHI   \n",
              "217                     QUAL                 OTH                        CHI   \n",
              "218                     QUAL                 OTH                        CHI   \n",
              "\n",
              "                                                                Mapped Full Publication  \\\n",
              "0                            Machine Learning Paradigms: Advances in Learning Analytics   \n",
              "1                            Machine Learning Paradigms: Advances in Learning Analytics   \n",
              "2                            Machine Learning Paradigms: Advances in Learning Analytics   \n",
              "3                                             British Journal of Educational Technology   \n",
              "4                                             British Journal of Educational Technology   \n",
              "5                                             British Journal of Educational Technology   \n",
              "6                                             British Journal of Educational Technology   \n",
              "7                                             British Journal of Educational Technology   \n",
              "8                                             British Journal of Educational Technology   \n",
              "9                                                              IEEE Conference on Games   \n",
              "10                                                             IEEE Conference on Games   \n",
              "11                                                             IEEE Conference on Games   \n",
              "12                           International Conference on Learning Analytics & Knowledge   \n",
              "13                           International Conference on Learning Analytics & Knowledge   \n",
              "14                           International Conference on Learning Analytics & Knowledge   \n",
              "15                                    Conference on Artificial Intelligence in Medicine   \n",
              "16                                    Conference on Artificial Intelligence in Medicine   \n",
              "17                                    Conference on Artificial Intelligence in Medicine   \n",
              "18                                            British Journal of Educational Technology   \n",
              "19                                            British Journal of Educational Technology   \n",
              "20                                            British Journal of Educational Technology   \n",
              "21                                            British Journal of Educational Technology   \n",
              "22                                            British Journal of Educational Technology   \n",
              "23                                            British Journal of Educational Technology   \n",
              "24                           International Conference on Advanced Learning Technologies   \n",
              "25                           International Conference on Advanced Learning Technologies   \n",
              "26                           International Conference on Advanced Learning Technologies   \n",
              "27                           Transactions on Computational Intelligence and AI in Games   \n",
              "28                           Transactions on Computational Intelligence and AI in Games   \n",
              "29                           Transactions on Computational Intelligence and AI in Games   \n",
              "30        International Conference on Teaching, Assessment and Learning for Engineering   \n",
              "31        International Conference on Teaching, Assessment and Learning for Engineering   \n",
              "32        International Conference on Teaching, Assessment and Learning for Engineering   \n",
              "33                                   International Conference on Multimodal Interaction   \n",
              "34                                   International Conference on Multimodal Interaction   \n",
              "35                                   International Conference on Multimodal Interaction   \n",
              "36                             International Conference on Intelligent Tutoring Systems   \n",
              "37                             International Conference on Intelligent Tutoring Systems   \n",
              "38                             International Conference on Intelligent Tutoring Systems   \n",
              "39                                           The Multimodal Learning Analytics Handbook   \n",
              "40                                           The Multimodal Learning Analytics Handbook   \n",
              "41                                           The Multimodal Learning Analytics Handbook   \n",
              "42                                           Interaction Design and Children Conference   \n",
              "43                                           Interaction Design and Children Conference   \n",
              "44                                           Interaction Design and Children Conference   \n",
              "45                                    Workshop Psychology of Programming Interest Group   \n",
              "46                                    Workshop Psychology of Programming Interest Group   \n",
              "47                                    Workshop Psychology of Programming Interest Group   \n",
              "48                                  International Conference on Educational Data Mining   \n",
              "49                                  International Conference on Educational Data Mining   \n",
              "50                                  International Conference on Educational Data Mining   \n",
              "51                                 International Conference on Quantitative Ethnography   \n",
              "52                                 International Conference on Quantitative Ethnography   \n",
              "53                                 International Conference on Quantitative Ethnography   \n",
              "54                                                                          IEEE Access   \n",
              "55                                                                          IEEE Access   \n",
              "56                                                                          IEEE Access   \n",
              "57                                            British Journal of Educational Technology   \n",
              "58                                            British Journal of Educational Technology   \n",
              "59                                            British Journal of Educational Technology   \n",
              "60                                            British Journal of Educational Technology   \n",
              "61                                            British Journal of Educational Technology   \n",
              "62                                            British Journal of Educational Technology   \n",
              "63                                    International Conference of the Learning Sciences   \n",
              "64                                    International Conference of the Learning Sciences   \n",
              "65                                    International Conference of the Learning Sciences   \n",
              "66                                                Transactions on Learning Technologies   \n",
              "67                                                Transactions on Learning Technologies   \n",
              "68                                                Transactions on Learning Technologies   \n",
              "69                                      International Conference on Multimedia Modeling   \n",
              "70                                      International Conference on Multimedia Modeling   \n",
              "71                                      International Conference on Multimedia Modeling   \n",
              "72                                        Elsevier Computers and Electrical Engineering   \n",
              "73                                        Elsevier Computers and Electrical Engineering   \n",
              "74                                        Elsevier Computers and Electrical Engineering   \n",
              "75                                                    Frontiers in Education Conference   \n",
              "76                                                    Frontiers in Education Conference   \n",
              "77                                                    Frontiers in Education Conference   \n",
              "78                                     Conference on Human Factors in Computing Systems   \n",
              "79                                     Conference on Human Factors in Computing Systems   \n",
              "80                                     Conference on Human Factors in Computing Systems   \n",
              "81                                                                     MDPI Information   \n",
              "82                                                                     MDPI Information   \n",
              "83                                                                     MDPI Information   \n",
              "84                                  International Journal of Child-Computer Interaction   \n",
              "85                                  International Journal of Child-Computer Interaction   \n",
              "86                                  International Journal of Child-Computer Interaction   \n",
              "87                                                                         MDPI Sensors   \n",
              "88                                                                         MDPI Sensors   \n",
              "89                                                                         MDPI Sensors   \n",
              "90                                             Journal of Computing in Higher Education   \n",
              "91                                             Journal of Computing in Higher Education   \n",
              "92                                             Journal of Computing in Higher Education   \n",
              "93                           International Conference on Learning Analytics & Knowledge   \n",
              "94                           International Conference on Learning Analytics & Knowledge   \n",
              "95                           International Conference on Learning Analytics & Knowledge   \n",
              "96                                                    Elsevier Learning and Instruction   \n",
              "97                                                    Elsevier Learning and Instruction   \n",
              "98                                                    Elsevier Learning and Instruction   \n",
              "99                                                                             PLOS ONE   \n",
              "100                                                                            PLOS ONE   \n",
              "101                                                                            PLOS ONE   \n",
              "102                                           British Journal of Educational Technology   \n",
              "103                                           British Journal of Educational Technology   \n",
              "104                                           British Journal of Educational Technology   \n",
              "105                                               Journal of Computer Assisted Learning   \n",
              "106                                               Journal of Computer Assisted Learning   \n",
              "107                                               Journal of Computer Assisted Learning   \n",
              "108                          International Conference on Learning Analytics & Knowledge   \n",
              "109                          International Conference on Learning Analytics & Knowledge   \n",
              "110                          International Conference on Learning Analytics & Knowledge   \n",
              "111                      International Workshop on Multimodal Interaction for Education   \n",
              "112                      International Workshop on Multimodal Interaction for Education   \n",
              "113                      International Workshop on Multimodal Interaction for Education   \n",
              "114                                         European Conference on Games Based Learning   \n",
              "115                                         European Conference on Games Based Learning   \n",
              "116                                         European Conference on Games Based Learning   \n",
              "117                                  International Conference on Multimodal Interaction   \n",
              "118                                  International Conference on Multimodal Interaction   \n",
              "119                                  International Conference on Multimodal Interaction   \n",
              "120                    International Conference on Artificial Intelligence in Education   \n",
              "121                    International Conference on Artificial Intelligence in Education   \n",
              "122                    International Conference on Artificial Intelligence in Education   \n",
              "123                                    Conference on Human Factors in Computing Systems   \n",
              "124                                    Conference on Human Factors in Computing Systems   \n",
              "125                                    Conference on Human Factors in Computing Systems   \n",
              "126                          International Conference on Learning Analytics & Knowledge   \n",
              "127                          International Conference on Learning Analytics & Knowledge   \n",
              "128                          International Conference on Learning Analytics & Knowledge   \n",
              "129                                          Interaction Design and Children Conference   \n",
              "130                                          Interaction Design and Children Conference   \n",
              "131                                          Interaction Design and Children Conference   \n",
              "132                                                       Journal of Learning Analytics   \n",
              "133                                                       Journal of Learning Analytics   \n",
              "134                                                       Journal of Learning Analytics   \n",
              "135                       International Journal of Artificial Intelligence in Education   \n",
              "136                       International Journal of Artificial Intelligence in Education   \n",
              "137                       International Journal of Artificial Intelligence in Education   \n",
              "138                                                              INTERSPEECH Conference   \n",
              "139                                                              INTERSPEECH Conference   \n",
              "140                                                              INTERSPEECH Conference   \n",
              "141                                                   Interactive Learning Environments   \n",
              "142                                                   Interactive Learning Environments   \n",
              "143                                                   Interactive Learning Environments   \n",
              "144                          International Conference on Learning Analytics & Knowledge   \n",
              "145                          International Conference on Learning Analytics & Knowledge   \n",
              "146                          International Conference on Learning Analytics & Knowledge   \n",
              "147                                                       Journal of Learning Analytics   \n",
              "148                                                       Journal of Learning Analytics   \n",
              "149                                                       Journal of Learning Analytics   \n",
              "150                                               Transactions on Learning Technologies   \n",
              "151                                               Transactions on Learning Technologies   \n",
              "152                                               Transactions on Learning Technologies   \n",
              "153                             Conference on Computer Supported Collaborative Learning   \n",
              "154                             Conference on Computer Supported Collaborative Learning   \n",
              "155                             Conference on Computer Supported Collaborative Learning   \n",
              "156                                                 Transactions on Affective Computing   \n",
              "157                                                 Transactions on Affective Computing   \n",
              "158                                                 Transactions on Affective Computing   \n",
              "159                    International Conference on Artificial Intelligence in Education   \n",
              "160                    International Conference on Artificial Intelligence in Education   \n",
              "161                    International Conference on Artificial Intelligence in Education   \n",
              "162                                                  Journal of Educational Data Mining   \n",
              "163                                                  Journal of Educational Data Mining   \n",
              "164                                                  Journal of Educational Data Mining   \n",
              "165                                           British Journal of Educational Technology   \n",
              "166                                           British Journal of Educational Technology   \n",
              "167                                           British Journal of Educational Technology   \n",
              "168                  International Journal of Computer-Supported Collaborative Learning   \n",
              "169                  International Journal of Computer-Supported Collaborative Learning   \n",
              "170                  International Journal of Computer-Supported Collaborative Learning   \n",
              "171                       International Journal of Artificial Intelligence in Education   \n",
              "172                       International Journal of Artificial Intelligence in Education   \n",
              "173                       International Journal of Artificial Intelligence in Education   \n",
              "174                                          The Multimodal Learning Analytics Handbook   \n",
              "175                                          The Multimodal Learning Analytics Handbook   \n",
              "176                                          The Multimodal Learning Analytics Handbook   \n",
              "177                                               Journal of Computer Assisted Learning   \n",
              "178                                               Journal of Computer Assisted Learning   \n",
              "179                                               Journal of Computer Assisted Learning   \n",
              "180                                 International Journal of Child-Computer Interaction   \n",
              "181                                 International Journal of Child-Computer Interaction   \n",
              "182                                 International Journal of Child-Computer Interaction   \n",
              "183  Applied Sciences, Special Issue \"Data Analytics and Machine Learning in Education\"   \n",
              "184  Applied Sciences, Special Issue \"Data Analytics and Machine Learning in Education\"   \n",
              "185  Applied Sciences, Special Issue \"Data Analytics and Machine Learning in Education\"   \n",
              "186                              International Conference on Human-Computer Interaction   \n",
              "187                              International Conference on Human-Computer Interaction   \n",
              "188                              International Conference on Human-Computer Interaction   \n",
              "189                                               Journal of Computer Assisted Learning   \n",
              "190                                               Journal of Computer Assisted Learning   \n",
              "191                                               Journal of Computer Assisted Learning   \n",
              "192                                            Journal of English for Academic Purposes   \n",
              "193                                            Journal of English for Academic Purposes   \n",
              "194                                            Journal of English for Academic Purposes   \n",
              "195                          International Conference on Learning Analytics & Knowledge   \n",
              "196                          International Conference on Learning Analytics & Knowledge   \n",
              "197                          International Conference on Learning Analytics & Knowledge   \n",
              "198                                                                        MDPI Sensors   \n",
              "199                                                                        MDPI Sensors   \n",
              "200                                                                        MDPI Sensors   \n",
              "201                                     International Journal of Information Management   \n",
              "202                                     International Journal of Information Management   \n",
              "203                                     International Journal of Information Management   \n",
              "204                       International Journal of Information and Education Technology   \n",
              "205                       International Journal of Information and Education Technology   \n",
              "206                       International Journal of Information and Education Technology   \n",
              "207                                 International Conference on Educational Data Mining   \n",
              "208                                 International Conference on Educational Data Mining   \n",
              "209                                 International Conference on Educational Data Mining   \n",
              "210                                          User Modeling and User-Adapted Interaction   \n",
              "211                                          User Modeling and User-Adapted Interaction   \n",
              "212                                          User Modeling and User-Adapted Interaction   \n",
              "213                          International Conference on Learning Analytics & Knowledge   \n",
              "214                          International Conference on Learning Analytics & Knowledge   \n",
              "215                          International Conference on Learning Analytics & Knowledge   \n",
              "216                                    Conference on Human Factors in Computing Systems   \n",
              "217                                    Conference on Human Factors in Computing Systems   \n",
              "218                                    Conference on Human Factors in Computing Systems   \n",
              "\n",
              "     Sort Number Environment Setting Environment Subject  \\\n",
              "0              3                PHYS                STEM   \n",
              "1              3                PHYS                STEM   \n",
              "2              3                PHYS                STEM   \n",
              "3              4                PHYS                STEM   \n",
              "4              4                PHYS                STEM   \n",
              "5              4                PHYS                STEM   \n",
              "6              5                VIRT                STEM   \n",
              "7              5                VIRT                STEM   \n",
              "8              5                VIRT                STEM   \n",
              "9              6                BLND                STEM   \n",
              "10             6                BLND                STEM   \n",
              "11             6                BLND                STEM   \n",
              "12             7                VIRT                STEM   \n",
              "13             7                VIRT                STEM   \n",
              "14             7                VIRT                STEM   \n",
              "15            11                BLND                 PSY   \n",
              "16            11                BLND                 PSY   \n",
              "17            11                BLND                 PSY   \n",
              "18            12                BLND                 HUM   \n",
              "19            12                BLND                 HUM   \n",
              "20            12                BLND                 HUM   \n",
              "21            13                VIRT                STEM   \n",
              "22            13                VIRT                STEM   \n",
              "23            13                VIRT                STEM   \n",
              "24            14                VIRT                STEM   \n",
              "25            14                VIRT                STEM   \n",
              "26            14                VIRT                STEM   \n",
              "27            15                BLND                 HUM   \n",
              "28            15                BLND                 HUM   \n",
              "29            15                BLND                 HUM   \n",
              "30            48                UNSP                UNSP   \n",
              "31            48                UNSP                UNSP   \n",
              "32            48                UNSP                UNSP   \n",
              "33            49                VIRT                STEM   \n",
              "34            49                VIRT                STEM   \n",
              "35            49                VIRT                STEM   \n",
              "36            50                VIRT                STEM   \n",
              "37            50                VIRT                STEM   \n",
              "38            50                VIRT                STEM   \n",
              "39            51                PHYS                STEM   \n",
              "40            51                PHYS                STEM   \n",
              "41            51                PHYS                STEM   \n",
              "42            52                VIRT                STEM   \n",
              "43            52                VIRT                STEM   \n",
              "44            52                VIRT                STEM   \n",
              "45            53                VIRT                STEM   \n",
              "46            53                VIRT                STEM   \n",
              "47            53                VIRT                STEM   \n",
              "48            54                BLND                STEM   \n",
              "49            54                BLND                STEM   \n",
              "50            54                BLND                STEM   \n",
              "51            55                VIRT                STEM   \n",
              "52            55                BLND                STEM   \n",
              "53            55                VIRT                STEM   \n",
              "54            56                PHYS                STEM   \n",
              "55            56                PHYS                STEM   \n",
              "56            56                PHYS                STEM   \n",
              "57            57                VIRT                UNSP   \n",
              "58            57                VIRT      HUM, OTH, STEM   \n",
              "59            57                VIRT      HUM, OTH, STEM   \n",
              "60            58                VIRT                 OTH   \n",
              "61            58                VIRT                STEM   \n",
              "62            58                VIRT                STEM   \n",
              "63            59                BLND                STEM   \n",
              "64            59                BLND                STEM   \n",
              "65            59                BLND                STEM   \n",
              "66            60                BLND                STEM   \n",
              "67            60                BLND                STEM   \n",
              "68            60                BLND                STEM   \n",
              "69            61                VIRT                STEM   \n",
              "70            61                VIRT                STEM   \n",
              "71            61                VIRT                STEM   \n",
              "72            62                BLND                STEM   \n",
              "73            62                BLND                STEM   \n",
              "74            62                BLND                STEM   \n",
              "75            63                VIRT                STEM   \n",
              "76            63                VIRT                STEM   \n",
              "77            63                VIRT                STEM   \n",
              "78            64                BLND                STEM   \n",
              "79            64                BLND                STEM   \n",
              "80            64                BLND                STEM   \n",
              "81            65                BLND                STEM   \n",
              "82            65                BLND                STEM   \n",
              "83            65                BLND                STEM   \n",
              "84            66                PHYS                STEM   \n",
              "85            66                PHYS                STEM   \n",
              "86            66                PHYS                STEM   \n",
              "87            67                PHYS                STEM   \n",
              "88            67                PHYS                STEM   \n",
              "89            67                PHYS                STEM   \n",
              "90            68                VIRT                STEM   \n",
              "91            68                VIRT                STEM   \n",
              "92            68                VIRT                STEM   \n",
              "93            69                BLND                UNSP   \n",
              "94            69                BLND                UNSP   \n",
              "95            69                BLND                UNSP   \n",
              "96            70                PHYS                STEM   \n",
              "97            70                BLND                STEM   \n",
              "98            70                BLND                STEM   \n",
              "99            71                VIRT                 HUM   \n",
              "100           71                VIRT                 HUM   \n",
              "101           71                VIRT                 HUM   \n",
              "102           72                UNSP                 HUM   \n",
              "103           72                PHYS                 HUM   \n",
              "104           72                UNSP                 HUM   \n",
              "105           73                VIRT                STEM   \n",
              "106           73                VIRT                STEM   \n",
              "107           73                VIRT                STEM   \n",
              "108            1                VIRT                STEM   \n",
              "109            1                VIRT                STEM   \n",
              "110            1                VIRT                STEM   \n",
              "111            2                VIRT                STEM   \n",
              "112            2                VIRT                STEM   \n",
              "113            2                VIRT                STEM   \n",
              "114            8                BLND                STEM   \n",
              "115            8                BLND                STEM   \n",
              "116            8                BLND                STEM   \n",
              "117            9                PHYS                 HUM   \n",
              "118            9                PHYS                 HUM   \n",
              "119            9                PHYS                 HUM   \n",
              "120           10                PHYS                 PSY   \n",
              "121           10                PHYS                 PSY   \n",
              "122           10                PHYS                 PSY   \n",
              "123           16                VIRT                STEM   \n",
              "124           16                VIRT                STEM   \n",
              "125           16                VIRT                STEM   \n",
              "126           17                BLND                 HUM   \n",
              "127           17                BLND                 HUM   \n",
              "128           17                BLND                 HUM   \n",
              "129           18                BLND                STEM   \n",
              "130           18                BLND                STEM   \n",
              "131           18                BLND                STEM   \n",
              "132           19                VIRT                STEM   \n",
              "133           19                VIRT                STEM   \n",
              "134           19                VIRT                STEM   \n",
              "135           20                PHYS                STEM   \n",
              "136           20                PHYS                STEM   \n",
              "137           20                PHYS                STEM   \n",
              "138           21                PHYS                 HUM   \n",
              "139           21                PHYS                 HUM   \n",
              "140           21                PHYS                 HUM   \n",
              "141           22                VIRT                STEM   \n",
              "142           22                PHYS                STEM   \n",
              "143           22                VIRT                STEM   \n",
              "144           23                BLND                STEM   \n",
              "145           23                BLND                STEM   \n",
              "146           23                BLND                STEM   \n",
              "147           24                VIRT                STEM   \n",
              "148           24                VIRT                STEM   \n",
              "149           24                VIRT                STEM   \n",
              "150           25                VIRT                STEM   \n",
              "151           25                VIRT                STEM   \n",
              "152           25                VIRT                STEM   \n",
              "153           26                PHYS                STEM   \n",
              "154           26                BLND                STEM   \n",
              "155           26                PHYS                STEM   \n",
              "156           27                PHYS           HUM, STEM   \n",
              "157           27                PHYS           HUM, STEM   \n",
              "158           27                PHYS           HUM, STEM   \n",
              "159           28                VIRT                STEM   \n",
              "160           28                VIRT                STEM   \n",
              "161           28                VIRT                STEM   \n",
              "162           29                PHYS                STEM   \n",
              "163           29                PHYS                STEM   \n",
              "164           29                PHYS                STEM   \n",
              "165           30                BLND           HUM, STEM   \n",
              "166           30                BLND           HUM, STEM   \n",
              "167           30                BLND           HUM, STEM   \n",
              "168           31                BLND                STEM   \n",
              "169           31                BLND                STEM   \n",
              "170           31                BLND                STEM   \n",
              "171           32                PHYS                 PSY   \n",
              "172           32                PHYS                 PSY   \n",
              "173           32                PHYS                 PSY   \n",
              "174           33                VIRT                STEM   \n",
              "175           33                VIRT                STEM   \n",
              "176           33                VIRT                STEM   \n",
              "177           34                BLND                STEM   \n",
              "178           34                PHYS                STEM   \n",
              "179           34                PHYS                STEM   \n",
              "180           35                BLND                STEM   \n",
              "181           35                BLND                STEM   \n",
              "182           35                BLND                STEM   \n",
              "183           36                PHYS                 OTH   \n",
              "184           36                PHYS                 HUM   \n",
              "185           36                PHYS                 HUM   \n",
              "186           37                VIRT                 PSY   \n",
              "187           37                VIRT                STEM   \n",
              "188           37                VIRT                STEM   \n",
              "189           38                BLND                STEM   \n",
              "190           38                BLND                STEM   \n",
              "191           38                BLND                STEM   \n",
              "192           39                PHYS                 OTH   \n",
              "193           39                PHYS                 OTH   \n",
              "194           39                PHYS                 OTH   \n",
              "195           40                PHYS                STEM   \n",
              "196           40                PHYS                STEM   \n",
              "197           40                PHYS                STEM   \n",
              "198           41                PHYS                 PSY   \n",
              "199           41                PHYS                 PSY   \n",
              "200           41                PHYS                 PSY   \n",
              "201           42                VIRT                 PSY   \n",
              "202           42                VIRT                 PSY   \n",
              "203           42                VIRT                 PSY   \n",
              "204           43                VIRT                UNSP   \n",
              "205           43                VIRT                UNSP   \n",
              "206           43                VIRT                UNSP   \n",
              "207           44                VIRT                 OTH   \n",
              "208           44                VIRT                STEM   \n",
              "209           44                VIRT                STEM   \n",
              "210           45          PHYS, VIRT                UNSP   \n",
              "211           45          PHYS, VIRT                UNSP   \n",
              "212           45          PHYS, VIRT                UNSP   \n",
              "213           46                VIRT                STEM   \n",
              "214           46                VIRT                STEM   \n",
              "215           46                VIRT                STEM   \n",
              "216           47                PHYS                 OTH   \n",
              "217           47                PHYS                STEM   \n",
              "218           47                PHYS                STEM   \n",
              "\n",
              "    Participant Structure Didactic Nature Level of Instruction or Training  \\\n",
              "0              IND, MULTI           INSTR                             UNSP   \n",
              "1              IND, MULTI           INSTR                             UNSP   \n",
              "2              IND, MULTI           INSTR                             UNSP   \n",
              "3                   MULTI           INSTR                              K12   \n",
              "4                   MULTI           INSTR                              K12   \n",
              "5                   MULTI           INSTR                              K12   \n",
              "6                     IND             INF                              UNI   \n",
              "7                     IND             INF                              UNI   \n",
              "8                     IND             INF                              UNI   \n",
              "9                     IND           INSTR                              K12   \n",
              "10                    IND             INF                              K12   \n",
              "11                    IND           INSTR                              K12   \n",
              "12                    IND           INSTR                              UNI   \n",
              "13                    IND           INSTR                              UNI   \n",
              "14                    IND           INSTR                              UNI   \n",
              "15                    IND           TRAIN                              UNI   \n",
              "16                    IND           TRAIN                              UNI   \n",
              "17                    IND           TRAIN                              UNI   \n",
              "18                    IND             INF                             UNSP   \n",
              "19                    IND           TRAIN                             UNSP   \n",
              "20                    IND           TRAIN                             UNSP   \n",
              "21                  MULTI           INSTR                              K12   \n",
              "22                  MULTI           INSTR                              K12   \n",
              "23                  MULTI           INSTR                              K12   \n",
              "24                  MULTI           INSTR                              UNI   \n",
              "25                  MULTI           INSTR                              UNI   \n",
              "26                  MULTI           INSTR                              UNI   \n",
              "27                    IND             INF                              K12   \n",
              "28                    IND             INF                              K12   \n",
              "29                    IND             INF                              K12   \n",
              "30                    IND           INSTR                             UNSP   \n",
              "31                    IND           INSTR                             UNSP   \n",
              "32                    IND           INSTR                             UNSP   \n",
              "33                    IND             INF                              K12   \n",
              "34                    IND             INF                              K12   \n",
              "35                    IND             INF                              K12   \n",
              "36                    IND           INSTR                              UNI   \n",
              "37                    IND           INSTR                              UNI   \n",
              "38                    IND           INSTR                              UNI   \n",
              "39                    IND           INSTR                              K12   \n",
              "40                    IND           INSTR                              K12   \n",
              "41                    IND           INSTR                              K12   \n",
              "42                    IND           INSTR                              K12   \n",
              "43                    IND           INSTR                              K12   \n",
              "44                    IND           INSTR                              K12   \n",
              "45                    IND           INSTR                              UNI   \n",
              "46                    IND           INSTR                              UNI   \n",
              "47                    IND           INSTR                              UNI   \n",
              "48                  MULTI           INSTR                              UNI   \n",
              "49                  MULTI             INF                              UNI   \n",
              "50                  MULTI           INSTR                              UNI   \n",
              "51                  MULTI             INF                             UNSP   \n",
              "52                  MULTI             INF                             UNSP   \n",
              "53                  MULTI             INF                             UNSP   \n",
              "54                  MULTI             INF                              UNI   \n",
              "55                  MULTI             INF                              UNI   \n",
              "56                  MULTI             INF                              UNI   \n",
              "57                    IND           INSTR                              K12   \n",
              "58                    IND           INSTR                              K12   \n",
              "59                    IND           INSTR                              K12   \n",
              "60                    IND            UNSP                              K12   \n",
              "61                    IND           INSTR                         K12, UNI   \n",
              "62                    IND           INSTR                         K12, UNI   \n",
              "63                  MULTI             INF                              UNI   \n",
              "64                  MULTI             INF                              UNI   \n",
              "65                  MULTI             INF                              UNI   \n",
              "66                  MULTI             INF                              UNI   \n",
              "67                  MULTI    INSTR, TRAIN                              UNI   \n",
              "68                  MULTI           TRAIN                              UNI   \n",
              "69                    IND            UNSP                              UNI   \n",
              "70                    IND           INSTR                              UNI   \n",
              "71                    IND           INSTR                              UNI   \n",
              "72                    IND           INSTR                              UNI   \n",
              "73                    IND           INSTR                              UNI   \n",
              "74                    IND           INSTR                              UNI   \n",
              "75                    IND           INSTR                              UNI   \n",
              "76                    IND           INSTR                              UNI   \n",
              "77                    IND           INSTR                              UNI   \n",
              "78                  MULTI             INF                              UNI   \n",
              "79                  MULTI    INSTR, TRAIN                              UNI   \n",
              "80                  MULTI           TRAIN                              UNI   \n",
              "81                    IND           INSTR                              UNI   \n",
              "82                    IND           INSTR                              UNI   \n",
              "83                    IND           INSTR                              UNI   \n",
              "84                    IND             INF                         K12, UNI   \n",
              "85                    IND             INF                         K12, UNI   \n",
              "86                    IND             INF                         K12, UNI   \n",
              "87                  MULTI             INF                              UNI   \n",
              "88                  MULTI           INSTR                              UNI   \n",
              "89                  MULTI           INSTR                              UNI   \n",
              "90                    IND             INF                              UNI   \n",
              "91                    IND           INSTR                              UNI   \n",
              "92                    IND           INSTR                              UNI   \n",
              "93                    IND            UNSP                              UNI   \n",
              "94                    IND            UNSP                              UNI   \n",
              "95                    IND            UNSP                              UNI   \n",
              "96                  MULTI             INF                              K12   \n",
              "97                  MULTI           INSTR                              K12   \n",
              "98                  MULTI           INSTR                              K12   \n",
              "99                    IND           TRAIN                         K12, UNI   \n",
              "100                   IND           TRAIN                         K12, UNI   \n",
              "101                   IND           TRAIN                         K12, UNI   \n",
              "102                   IND           TRAIN                             UNSP   \n",
              "103                   IND           TRAIN                             UNSP   \n",
              "104                   IND           TRAIN                             UNSP   \n",
              "105                   IND             INF                              K12   \n",
              "106            IND, MULTI           INSTR                              K12   \n",
              "107            IND, MULTI           INSTR                              K12   \n",
              "108                 MULTI             INF                              UNI   \n",
              "109                 MULTI           INSTR                              UNI   \n",
              "110                 MULTI           INSTR                              UNI   \n",
              "111                   IND           INSTR                              K12   \n",
              "112                   IND           INSTR                              K12   \n",
              "113                   IND           INSTR                              K12   \n",
              "114                 MULTI           INSTR                              UNI   \n",
              "115                 MULTI           INSTR                              UNI   \n",
              "116                 MULTI           INSTR                              UNI   \n",
              "117                 MULTI             INF                              K12   \n",
              "118                 MULTI           TRAIN                              K12   \n",
              "119                 MULTI           TRAIN                              K12   \n",
              "120                   IND           TRAIN                             PROF   \n",
              "121                   IND           TRAIN                             PROF   \n",
              "122                   IND           TRAIN                             PROF   \n",
              "123                   IND           INSTR                              K12   \n",
              "124                   IND           INSTR                              K12   \n",
              "125                   IND           INSTR                              K12   \n",
              "126                   IND             INF                              UNI   \n",
              "127                   IND           TRAIN                              UNI   \n",
              "128                   IND           TRAIN                              UNI   \n",
              "129                 MULTI           INSTR                              K12   \n",
              "130                   IND           INSTR                              K12   \n",
              "131                   IND           INSTR                              K12   \n",
              "132                   IND           INSTR                              K12   \n",
              "133                   IND           INSTR                              K12   \n",
              "134                   IND           INSTR                              K12   \n",
              "135                 MULTI             INF                              K12   \n",
              "136                 MULTI           INSTR                         K12, UNI   \n",
              "137                 MULTI             INF                         K12, UNI   \n",
              "138                 MULTI           TRAIN                              K12   \n",
              "139                 MULTI           TRAIN                              K12   \n",
              "140                 MULTI           TRAIN                              K12   \n",
              "141                 MULTI           INSTR                              K12   \n",
              "142                 MULTI           INSTR                              K12   \n",
              "143                 MULTI           INSTR                              K12   \n",
              "144                   IND           INSTR                              K12   \n",
              "145                   IND           INSTR                              K12   \n",
              "146                   IND           INSTR                              K12   \n",
              "147                   IND           INSTR                              UNI   \n",
              "148                   IND           INSTR                              UNI   \n",
              "149                   IND           INSTR                              UNI   \n",
              "150                   IND             INF                              UNI   \n",
              "151                   IND           INSTR                              UNI   \n",
              "152                   IND             INF                              UNI   \n",
              "153                 MULTI             INF                              UNI   \n",
              "154                 MULTI           INSTR                              UNI   \n",
              "155                 MULTI           INSTR                              UNI   \n",
              "156                   IND           INSTR                              K12   \n",
              "157                 MULTI           INSTR                              K12   \n",
              "158                 MULTI           INSTR                              K12   \n",
              "159                   IND           INSTR                             UNSP   \n",
              "160                   IND           INSTR                             UNSP   \n",
              "161                   IND           INSTR                             UNSP   \n",
              "162                   IND           INSTR                              K12   \n",
              "163                   IND           INSTR                              K12   \n",
              "164                   IND           INSTR                              K12   \n",
              "165                 MULTI           INSTR                              K12   \n",
              "166                 MULTI           INSTR                              K12   \n",
              "167                 MULTI           INSTR                              K12   \n",
              "168                 MULTI           INSTR                              K12   \n",
              "169                 MULTI           INSTR                              K12   \n",
              "170                 MULTI           INSTR                              K12   \n",
              "171                   IND           TRAIN                             UNSP   \n",
              "172                   IND           TRAIN                              UNI   \n",
              "173                   IND           TRAIN                             PROF   \n",
              "174                   IND           INSTR                              K12   \n",
              "175                   IND           INSTR                              K12   \n",
              "176                   IND           INSTR                              K12   \n",
              "177                 MULTI           INSTR                              K12   \n",
              "178                 MULTI           INSTR                             PROF   \n",
              "179                 MULTI           INSTR                             PROF   \n",
              "180                   IND           INSTR                              K12   \n",
              "181                   IND           INSTR                              K12   \n",
              "182                   IND           INSTR                              K12   \n",
              "183                 MULTI             INF                             PROF   \n",
              "184                 MULTI             INF                              UNI   \n",
              "185                 MULTI             INF                        PROF, UNI   \n",
              "186                   IND             INF                              K12   \n",
              "187                   IND             INF                              K12   \n",
              "188                   IND             INF                              K12   \n",
              "189                 MULTI           INSTR                              UNI   \n",
              "190                 MULTI           INSTR                              UNI   \n",
              "191                 MULTI           INSTR                              UNI   \n",
              "192                 MULTI           TRAIN                             PROF   \n",
              "193                 MULTI           TRAIN                             PROF   \n",
              "194                 MULTI           TRAIN                             PROF   \n",
              "195                 MULTI           INSTR                              UNI   \n",
              "196                 MULTI           INSTR                              UNI   \n",
              "197                 MULTI           INSTR                              UNI   \n",
              "198                 MULTI           TRAIN                              UNI   \n",
              "199            IND, MULTI           TRAIN                        UNI, UNSP   \n",
              "200                   IND           TRAIN                              UNI   \n",
              "201                   IND             INF                              UNI   \n",
              "202                   IND             INF                              UNI   \n",
              "203                   IND             INF                              UNI   \n",
              "204                   IND           INSTR                              UNI   \n",
              "205                   IND           INSTR                             UNSP   \n",
              "206                   IND           INSTR                             UNSP   \n",
              "207                   IND           TRAIN                              UNI   \n",
              "208                   IND           TRAIN                              UNI   \n",
              "209                   IND           TRAIN                              UNI   \n",
              "210            IND, MULTI           INSTR                             UNSP   \n",
              "211            IND, MULTI           INSTR                              UNI   \n",
              "212            IND, MULTI           INSTR                              UNI   \n",
              "213                 MULTI           INSTR                              K12   \n",
              "214                 MULTI           INSTR                              K12   \n",
              "215                 MULTI           INSTR                              K12   \n",
              "216                 MULTI           TRAIN                              UNI   \n",
              "217                 MULTI           TRAIN                              UNI   \n",
              "218                 MULTI           TRAIN                              UNI   \n",
              "\n",
              "    Analysis Approach  \\\n",
              "0                  MB   \n",
              "1                  MB   \n",
              "2                  MB   \n",
              "3                  MB   \n",
              "4                  MB   \n",
              "5                  MB   \n",
              "6                  MB   \n",
              "7                  MB   \n",
              "8                  MB   \n",
              "9                  MB   \n",
              "10                 MB   \n",
              "11                 MB   \n",
              "12                 MB   \n",
              "13                 MB   \n",
              "14                 MB   \n",
              "15                 MB   \n",
              "16                 MB   \n",
              "17                 MB   \n",
              "18                 MF   \n",
              "19                 MF   \n",
              "20                 MF   \n",
              "21                 MB   \n",
              "22                 MB   \n",
              "23                 MB   \n",
              "24                 MB   \n",
              "25                 MB   \n",
              "26                 MB   \n",
              "27                 MB   \n",
              "28                 MB   \n",
              "29                 MB   \n",
              "30                 MB   \n",
              "31                 MB   \n",
              "32                 MB   \n",
              "33                 MB   \n",
              "34                 MB   \n",
              "35                 MB   \n",
              "36                 MB   \n",
              "37                 MB   \n",
              "38                 MB   \n",
              "39                 MB   \n",
              "40             MB, MF   \n",
              "41             MB, MF   \n",
              "42                 MB   \n",
              "43                 MF   \n",
              "44             MB, MF   \n",
              "45                 MB   \n",
              "46                 MB   \n",
              "47                 MB   \n",
              "48                 MF   \n",
              "49             MB, MF   \n",
              "50             MB, MF   \n",
              "51                 MB   \n",
              "52                 MF   \n",
              "53             MB, MF   \n",
              "54             MB, MF   \n",
              "55             MB, MF   \n",
              "56             MB, MF   \n",
              "57                 MF   \n",
              "58                 MB   \n",
              "59                 MB   \n",
              "60                 MF   \n",
              "61             MB, MF   \n",
              "62             MB, MF   \n",
              "63                 MF   \n",
              "64                 MF   \n",
              "65                 MF   \n",
              "66                 MF   \n",
              "67                 MF   \n",
              "68                 MF   \n",
              "69                 MB   \n",
              "70                 MB   \n",
              "71                 MB   \n",
              "72                 MB   \n",
              "73                 MB   \n",
              "74                 MB   \n",
              "75                 MB   \n",
              "76                 MB   \n",
              "77                 MB   \n",
              "78                 MF   \n",
              "79                 MF   \n",
              "80                 MF   \n",
              "81                 MF   \n",
              "82                 MF   \n",
              "83                 MF   \n",
              "84             MB, MF   \n",
              "85                 MB   \n",
              "86             MB, MF   \n",
              "87             MB, MF   \n",
              "88                 MF   \n",
              "89             MB, MF   \n",
              "90                 MB   \n",
              "91                 MB   \n",
              "92                 MB   \n",
              "93                 MB   \n",
              "94                 MB   \n",
              "95                 MB   \n",
              "96                 MF   \n",
              "97                 MB   \n",
              "98                 MF   \n",
              "99                 MF   \n",
              "100                MF   \n",
              "101                MF   \n",
              "102                MF   \n",
              "103                MB   \n",
              "104            MB, MF   \n",
              "105                MF   \n",
              "106            MB, MF   \n",
              "107            MB, MF   \n",
              "108                MB   \n",
              "109                MF   \n",
              "110                MF   \n",
              "111                MB   \n",
              "112                MB   \n",
              "113                MB   \n",
              "114                MF   \n",
              "115                MF   \n",
              "116                MF   \n",
              "117                MF   \n",
              "118                MB   \n",
              "119                MB   \n",
              "120                MB   \n",
              "121                MB   \n",
              "122                MB   \n",
              "123                MB   \n",
              "124                MB   \n",
              "125                MB   \n",
              "126                MB   \n",
              "127                MB   \n",
              "128                MB   \n",
              "129            MB, MF   \n",
              "130                MF   \n",
              "131                MF   \n",
              "132            MB, MF   \n",
              "133                MB   \n",
              "134                MB   \n",
              "135                MB   \n",
              "136                MB   \n",
              "137                MB   \n",
              "138                MF   \n",
              "139                MB   \n",
              "140                MB   \n",
              "141                MB   \n",
              "142                MB   \n",
              "143                MB   \n",
              "144                MF   \n",
              "145                MB   \n",
              "146                MB   \n",
              "147                MB   \n",
              "148                MB   \n",
              "149                MB   \n",
              "150                MB   \n",
              "151                MF   \n",
              "152                MF   \n",
              "153                MF   \n",
              "154                MB   \n",
              "155                MB   \n",
              "156                MB   \n",
              "157                MB   \n",
              "158                MB   \n",
              "159                MB   \n",
              "160                MB   \n",
              "161                MB   \n",
              "162                MB   \n",
              "163                MB   \n",
              "164                MB   \n",
              "165                MF   \n",
              "166                MF   \n",
              "167                MF   \n",
              "168            MB, MF   \n",
              "169                MB   \n",
              "170                MB   \n",
              "171                MB   \n",
              "172                MB   \n",
              "173                MB   \n",
              "174                MF   \n",
              "175                MF   \n",
              "176                MF   \n",
              "177                MB   \n",
              "178                MB   \n",
              "179                MB   \n",
              "180            MB, MF   \n",
              "181                MB   \n",
              "182            MB, MF   \n",
              "183                MF   \n",
              "184                MF   \n",
              "185                MF   \n",
              "186                MF   \n",
              "187                MF   \n",
              "188                MF   \n",
              "189                MB   \n",
              "190                MB   \n",
              "191                MB   \n",
              "192                MF   \n",
              "193                MF   \n",
              "194                MF   \n",
              "195            MB, MF   \n",
              "196                MB   \n",
              "197                MB   \n",
              "198                MB   \n",
              "199                MB   \n",
              "200                MB   \n",
              "201            MB, MF   \n",
              "202                MB   \n",
              "203                MB   \n",
              "204                MB   \n",
              "205                MB   \n",
              "206                MB   \n",
              "207                MB   \n",
              "208                MB   \n",
              "209                MB   \n",
              "210                MB   \n",
              "211                MB   \n",
              "212                MB   \n",
              "213                MB   \n",
              "214                MB   \n",
              "215                MB   \n",
              "216                MF   \n",
              "217                MB   \n",
              "218                MB   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
              "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. The findings suggest that MMLA can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding   \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Common case of multimodal outperform unimodal models, through the addition of gaze to classify student's posttest performance and interest.   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
              "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Authors conclude that the feature combination of gaze and physiological MMD provide the most accurate predictions of correct answers. They also show the feasibility of early prediction of children's performance by using half (as oppose to full) data lengths to extract features and predict correctness.   \n",
              "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Researchers show the ability to predict student's performance in embodied game using various data sources (gaze, physiological, skeleton). However, they note that using certain modalities (e.g., skeleton and physiological) reduce the predictive performance of the model.   \n",
              "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Findings provide evidence that HMMs can encode the relationship between effort and behaviour (captured by the multimodal data) in a more efficient way than the other methods. A practical implication of the approach is that the derived HMMs also pinpoint the moments to provide preventive/prescriptive feedback to the learners in real-time, by building-upon the relationship between behavioural patterns and the effort the learners are putting in.   \n",
              "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The results show that the proposed method not only outperforms the contemporary classification algorithms but it also gives the educators several opportunities for providing (proactive) actionable feedback by pinpointing the exact moments in the learning activity\\nwhere feedback is needed.   \n",
              "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Used each Chest Compression as training sample by masking/windowing of the original time series, then trained an LSTM network with all these samples and were able to classify accurately the target classes, however discarding the rest of the time-series they were not able to detect if a CC happened. Author asks Doctorial Consortium how, given the available data, could they train a classifier able to detect whether a CC happened or not.   \n",
              "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Trained an LSTM to predict ['too slow', 'on-point', 'too fast'] for Chest compression training. Achieved 70-75% accuracy.   \n",
              "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoidance of filled pauses)   \n",
              "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Authors showcase that the training tool improved manually defined scores between an initial and second use of the tool.   \n",
              "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.   \n",
              "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Evaluating how multimodal features contribute to a model's performance to predict learning gains. Audio features introduce noise that negatively impacted the error of the model.   \n",
              "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "24                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, authors found that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.   \n",
              "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Predicting the learning gains via classification (bad, ok, good) through gaze, logs, audio, and dialog. Determined that distance measures between hands and gaze fixations was the key features to predict students' performance.   \n",
              "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Presented a novel methodology for the automatic recognition of student engagement in prosocial games aiming to capture the different dimensions of engagement, i.e., behavioral, cognitive, and affective, by exploiting real-time engagement cues from different input modalities (body motion and facial expression analysis to identify the affective state of students, features related to their cognitive and behavioral engagement based on the analysis of their interaction with the game)   \n",
              "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Same as Joyce (first paragraph in conclusion)   \n",
              "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           By aplying reinforcement learning to select appropriate learning activities without being aware of the learning model of each student, simulation results showed that the proposed learning recommendation systems can promote students’ performance with higher average scores in the tests   \n",
              "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Determined that an RL activity recommendataiton system would assit (via simulated data) teachers in select the optimal learning activity to optimize collective learning gains in a classroom.   \n",
              "32                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The evaluation revealed that predictive models of visitor dwell time can make improved predictions over time, and using additional modalities yields better performance as visitors near the end of their interactions. Random forest models outperformed competing models on three of the four modality combinations, and Lasso regression performed best on the unimodal configuration for predicting dwell time. Notably, overall predictive performance improves when removing eye gaze and interaction log modalities. However, removing facial expression results in a steep drop in performance.   \n",
              "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Same as Joyce (first paragraph in conclusion)   \n",
              "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The tutor can detect 6 emotions in mobile MOOC learning reliably with high accuracy; it can also predict learning outcomes; work rpoves it is feasible to track both PPG signals and facial expressions in real time in a scalable manner on today’s unmodified smartphones.   \n",
              "37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The study shows the feasibility of capturing rich and fine-grained physiological signals such as PPG signals and facial expressions in mobile learning contexts without introducing any addi‐\\ntional hardware. Experimental results show that PPG signals and facial expressions collected by AttentiveLearner2 in real time are complementary and can serve as fine-grained, rich signals to understand learners’ emotions. By capturing the temporal dynamics of both feature channels, AttentiveLearner2 can achieve higher performance by combining both PPG features and FEA features. Our approach is complementary to today’s existing technique such as clickstream analysis and is promising towards enabling personalized interventions for mobile MOOC learning.   \n",
              "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Results demonstrate how NLP and ML techniques allow us to use different modalities of the same data, voice and transcript, and different modalities of different data sources, voice data from interviews, answers to a goal orientation questionnaire, and answers to open-ended questions about energy, in order to better understand individual differences in students’ performances   \n",
              "40                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Used text and audio to predict student's affect. With the affect, the authors' explored its statistical relation to student's knowledge. Results point that they need more data to improve performance in affect prediction but promising direction.   \n",
              "41                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "42                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Sadness, anger and stress are negatively, and arousal is positively related to students’ relative learning gains; experienced fun is positively related to students’ RLG   \n",
              "43                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Our findings support endeavors of educators, designers, and re-\\nsearchers to make learning to code a fun experience, as we found a positive relationship between those. Further research studies could aim to improve the applicability of physiological measure devices (e.g. wristbands) for children.   \n",
              "44                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "45                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               A multimodal approach offers higher accuracy and better robustness as compared to a unimodal approach (multimodal fusion leads to higher detection accuracy over unimodal model). In addition, the inclusion of keystrokes and mouse clicks makes up for the detection gap where video based sensing modes (facial and head postures) are not available.   \n",
              "46                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The main goal of this study is to xplore automated techniques for the detection of frustration in a naturalistic learning environment. With adequate detection of frustration on a moment by moment\\nbasis, hints and tutorial supports can be provided to the students to overcome learning barriers and\\nalleviate their frustration so as to sustain their engagement in learning.   \n",
              "47                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "48                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Significant correlations found between average movement of points along the upper right side of participants’ bodies with outcome measures indicates the importance of gesturing and physical movement when communicating ideas.   \n",
              "49                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We plan to further identify productive micro-behaviors from the Kinect data to gain additional insights in the ways that dyads synchronized their actions. Future work with regards to prototypical postures would also explore both participants in a dyad at once, clustering on both joint angles simultaneously. This may reveal combinations of postures that are informative and could extend our exploration of physical synchrony within dyads.\\nThe differences between dyads in different conditions will also be a main focus of analysis moving forward.   \n",
              "50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "51                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Authors present an initial pilot using these methods in concert to identify key moments in multiple modalities. While the use of constructivist dialogue mapping showed that the users learned during their interaction with Ant Adaption, emotional logging identified alternative moments of learning outside of their analytic framework.   \n",
              "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 This paper presented a preliminary approach to augment qualitative analysis of an\\ninformal learning environment. Using techniques from multimodal learning analytics,\\nwe were able to expand our analysis of learning while participants interacted with a\\nmultitouch environment. Our methodological approach required us to extract emotions\\nfrom the low-level logs of facial action units using FACET and then revisit video\\ncorresponding to particular FACET values to identify moments of high emotional stimulation theoretically implicated in learning.   \n",
              "53                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "54                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ``There was no difference between the proportion of the session spent being bored or frustrated or in the achievement scores for the two conditions. In order to determine whether the effect on achievement was different depending on length of exposure, participants with a duration less than 60 minutes were excluded from the analysis. However, there was still no significant difference between achievement scores from intervention and control sessions for the group as a whole or for each subgroup.``   \n",
              "55                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Main findings of the case study are the relationships between prior experience in software requirements and the way the team members collaborate, and the lower productivity of low experienced groups. No evidence was found that performance of domain experts was superior from non-experts during collaborative problem-solving sessions. Although it was stated that low experience subjects produced more user stories, a greater productivity of top experience subjects was not statistically verified.   \n",
              "56                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "57                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         This is the first study to evaluate an adaptive learning system for learners with ID based on multi-modal affect recognition. Three separate states were automatically identified, with lower levels of the state labelled “boredom” having the strongest link to learning achievement. Both those labelled “frustration” and “engagement” were positively related to achievement. Our results are in line with other studies showing that engagement increases when activities are tailored to the personal needs\\nand emotional states of learners (Athanasiadis et al., 2017), but no significant difference in learn-ing achievement was found (at least for the period of our study) when adaption was based on both the affective state and achievement of the learner, compared with achievement alone.   \n",
              "58                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Results suggest that engagement does increase when activities are tailored to the personal needs and emotional state of the learner and that the system was promoting affective states that in turn promote learning. However, longer exposure is necessary to determine the effect on learning.   \n",
              "59                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "60                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Against our expectations, results revealed that physiological data could not be used to detect differences in CL based on intrinsic and extraneous manipulations. By contrast, most of the significant results are related to OSPAN and the baseline measurement. Based on our findings related to OSPAN, we might be able to conclude that HR, HRV and ST is more sensitive to high CL, namely, exceeding the learner’s cognitive capacity and the related mental states (ie, stress). In this respect, as high CL can also provoke stress, it is not always clear what exactly is measured via physiological data.   \n",
              "61                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     This study manipulated intrinsic and extraneous load to investigate how physiological features, namely, GSR, ST and HR(V) vary as a result of changes in CL. Results revealed no significant differences between the manipulated conditions in terms of physiological data. Nonetheless, HR and ST were significantly related to self-reported CL, whereas ST to task performance. Additionally, this study revealed the potential of ST and HR to assess high CL.   \n",
              "62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "63                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           While this study was not able to show a clear effect of providing a real-time visualization to support\\ncollaboration, it made many other valuable contributions. First, it showed that simple verbal interventions can\\nhelp participants pay attention to particular aspects of their collaborative behavior (i.e., how much they are\\ntalking and how much space they are providing to their partner). Second, it suggested that awareness tools such\\nas the one developed for this study have to be designed differently to impact social interactions (e.g., by being\\nmore salient or be used in a setting where users have the mental bandwidth to reflect on their collaborative\\nstyle). Third, we collected a rich multi-modal dataset that can be used to build proxies for measuring effective\\ncollaborations.   \n",
              "64                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The purpose of this paper was to explore the effect of two collaboration interventions and the relationship between collaboration quality, task performance and learning gains, however this study was not able to show a clear effect of providing a real-time visualization to support collaboration. It did show that simple verbal interventions can help participants pay attention to particular aspects of their collaborative behavior, and suggested that awareness tools such as the one developed for this study have to be designed differently to impact social interactions. Authors built a rich multi-modal dataset that can be used to build proxies for measuring effective collaborations. As a preliminary analysis, they found that various indicators captured by the Kinect sensor were correlated with participants’ quality of collaboration.   \n",
              "65                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "66                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          This article presented two qualitative studies conducted in authentic nursing simulation classrooms with the purpose of communicating insights to students through data stories. Given the limitations of current visual analytics, we anticipate that approaches such as DS will grow in importance to help students make the most\\nof the new forms of feedback that are becoming possible.   \n",
              "67                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           This article results show that the enhancements using DS principles helped students identify misconceptions, think about strategies to address errors they made, and reflect on the arousal levels they may have experienced during the simulations. Although the studies presented in this article were conducted in the context of complex, multimodal learning situations, there is no reason why a storytelling approach could not be implemented to aid in the interpretation of more conventional LA visualizations supporting noncollocated teamwork.   \n",
              "68                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "69                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In this work, we proposed a data imputation method called PRIME for blockwise missingness handling in multimodal data and measured its effectiveness in a student modeling task to predict students’ learning gain in an ITS. Through experiments, we demonstrated that: (1) the multimodal data is more effective than the single-modal data; (2) compared to competitive baseline missing data handling methods, the PRIME can not only improve the prediction performance, but also achieve more accurate reconstruction results.   \n",
              "70                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Results show that using multimodal data as a result of missing data handling yields better prediction performance than using logfiles only, and PRIME outperforms other baseline methods for both learning gain prediction and data reconstruction tasks   \n",
              "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "72                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Which data fusion approach and classification algorithms produce the best results from our data? The use of ensembles and selecting the best attributes approach from discretized summary data produced our highest/best results in Accuracy and AUC values. The REPTree classification algorithm obtained the highest/best results in this approach from discretized summary data. \\n\\n• How useful are the prediction models we produce to help teachers detect students who are at risk of drop out or fail the course? The white-box models we produced give teachers very understandable explanations (IF-THEN rules) of how they classified the students’ final performance or classification. They showed that the attributes that appear most in these rules were attention in theory classes,\\nscores in Moodle quizzes, and the level of activity in the Moodle forum.   \n",
              "73                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The results show that the best predictions are produced using ensembles and selecting the best attributes approach with discretized data. The best prediction models show that the level of attention in theory classes, scores in Moodle quizzes, and the level of activity in Moodle forums are the best set of attributes for predicting students’ final performance in their courses.   \n",
              "74                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "75                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Overall, feedback was very positive and responses can be found in Table IX. Most students would recommend this system to students attending the same course next year or would like to see this system included in other courses as shown in questions 5 and 6 respectively. In terms of the last question to improve the system, students who were doing well or very well, were getting an increasingly similar response each week and were demanding a more personalised notification and some other additional learning resources.   \n",
              "76                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Predictive models built using student characteristics, prior academic history, logged interactions between students and online resources, and students’ progress in programming laboratory work were used to give weekly predictions to students. Predictions worked relatively well with one year of training data for the three courses. Authors noted that CS2 and SH1’s models were based on 2015/16’s previous student data and PF3’s was based on PF2’s student data from the first semester of the academic year, thus they did not expect it to work as well as the other models as the courseware was not the same.   \n",
              "77                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "78                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      This paper documents how we have wrestled with the challenge of designing activity-based feedback visualisations which draw the attention of non-technical users to key insights in the data. We argue, supported by user studies, that this work advances the state of the art in making multimodal data streams intelligible to non-data experts. The approach should enable similar collocated activities to benefit from these novel collaboration analytics.   \n",
              "79                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            This paper documents how authors tackled the challenge of designing activity-based feedback visualisations which draw the attention of non-technical users to key insights in the data. We argue, supported by user studies, that this work advances the state of the art in making multimodal data streams intelligible to non-data experts. The approach should enable similar collocated activities to benefit from these novel collaboration analytics.   \n",
              "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "81                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Finally, it was clear from the two intervention studies that the simulation provided by mobile devices, although not as powerful as the desktop-computer-powered Oculus Rift mixed reality or physical simulators, has no negative impacts on the learners. In fact, the simulation improves learners’ competence and skills with the skills intervention showing statistically significant improvements in the learners that received the mobile mixed reality simulation tools prior to residential school.   \n",
              "82                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 This study validates the use of mobile devices in university undergraduate health sciences curricula, and shows that not only are these modes (game engines, free AR/VR SDKs and mobile-based devices with GPU-enabled processors and high-quality screens) useful for enhancing the development of physical skills in students, but they are also received favorably.   \n",
              "83                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "84                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            In this paper, we applied clustering, natural language processing, and general linear modeling to a small yet rich dataset detailing student behaviors and speech during measurement tasks to identify successful measurement strategies. Our findings revealed profiles of student behavior and speech that may indicate different levels of conceptual knowledge as well as evidence that spatial and kinetographic gestures predict performance on mea-\\nsurement tasks.   \n",
              "85                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Authors explored students’ conceptual understanding of measurement to indentify measurement estimation strategies that should be emphasized in classroom instruction. By applying machine-learning methods to a small, multimodal dataset from a study on student behavior in mathematics, we identified behavioral profiles, patterns in speech, and specific actions and gestures that are predictive of performance.   \n",
              "86                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "87                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    RQ1: Better communication, better collaboration\\nRQ2: Collaborative teams showed lower variability in the estimates of story points (same page)\\nRQ3: Democratic leadership in collaborative groups   \n",
              "88                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The results obtained allowed the detection of important relationships and characteristics of the collaborative and Non-Collaborative groups, with productivity, effort, and predominant personality styles in the groups. Authors conclude that the Multimodal Learning Analysis techniques offer considerable feasibilities to support the process of skills development in students.   \n",
              "89                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "90                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The implications of the current study point to Web ITS and Web-based Adaptive Educational Systems. If data is captured from diferent data sources, the classifer ensemble methodology proposed in this study could make better, earlier performance predictions than the single data source models that are commonly used at present.   \n",
              "91                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Authors tested whether prediction of learning performance could be improved by using attribute selection and classification ensembles. By carrying out three experiments and applying six classifcation algorithms to numerical and discretized preprocessed multimodal data, results show that the best predictions were produced using ensembles and selecting the best attributes approach with numerical data.   \n",
              "92                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "93                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                This paper described Learning Pulse, an exploratory study whose aim was to use predictive modelling to generate timely predictions about learners’ performance during self-regulated learning by collecting multimodal data about their body, activity and context. Although the prediction accuracy with the data sources and experimental setup chosen in Learning Pulse led to modest results, all the research questions have been answered positively and have lead towards new insights on the storing, modelling and processing multimodal data.   \n",
              "94                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This paper described an exploratory study whose aim was to use predictive modelling to generate timely predictions about learners’ performance during self-regulated learning by collecting multimodal data about their body, activity and context. The limited significance of the prediction results did not allow authors to assert that accurate and learner-specific predictions can be generated, however all the research questions have been answered positively and have lead towards new insights on the storing, modelling and processing multimodal data.   \n",
              "95                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "96                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Our major claim is that multichannel data can be potential for understanding regulatory processes in collaboration. With our five empirical case examples, we illustrate how triangulating multiple sources of data has potential to advance the theoretical and conceptual progress in social aspects of SRL theory.   \n",
              "97                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Authors show with five empirical cases that multichannel data can be potential for understanding regulatory processes in collaboration, illustrating how triangulating multiple sources of data has potential to advance the theoretical and conceptual progress in social aspects of SRL theory: (1) understanding how interactions between different facets of regulation, such as cognition, motivation and emotion interact with cognitive strategic action by using video and EDA data; (2) visualizing how physiological synchrony measured from the heart rate can reveal or backup the interpretation of socially shared regulation of learning or co-regulation of learning located from the video; (3) visualizing temporality and cyclical processes (i.e., planning, enacting strategies, reflecting, adapting) of regulation by using video, EDA and facial expression recognition data; (5)) illustrating how combining not only physiological measures, but also facial expression data can lead even more accurate interpretations of the situations where regulation of learning is needed.   \n",
              "98                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN   \n",
              "99                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         We identified a positive correlation between overall narrative skills and other non-verbal behaviors. This result indicates that the overall narrative skills were subjectively decided based on other non-verbal behavior skills [22]. Although the automated social skills trainer did not provide feedback regarding narrative structure or eye gaze, these elements were also improved after the training.   \n",
              "100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The focus of this study assessed the effectiveness of an automated social skills trainer with multimodal information that adheres to the basic human-based SST as closely as possible. Authors extended a previous method for automatic social skills training by adding audiovisual information regarding smiling ratio and head pose that improved the training effect. \\nMultimodal feedback is also useful for both members of the general population with social difficulties and people with ASD because it helps such people understand and improve their narrative skills, as was previously reported in human-based SST [2, 3].   \n",
              "101                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "102                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        In this study, we used non-transparent prediction models of openSMILE to predict the emotional traits of tutor candidates based on their audio data, and we used transparent logistic regression models to identify exactly what personality, emotion, and experience traits lead to effective debate tutoring skills. Predictive models were very powerful to make sense of complex and nonlinear audio data, whereas the transparent regression models were valuable to identify key aspects for tutors to reflect upon their own decisions and provide tutor candidates with feedback on their performance.   \n",
              "103                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Authors combined predictive and transparent models to support the human decision-making processes involved in tutor trainee evaluations and results showed that models with multimodal data can accurately classify tutors and have the potential to support the intuitive decision-making of expert tutors in the context of evaluating trainee applicants.   \n",
              "104                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "105                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                We presented two empirical studies, collected in classroom studies with two distinct learning technology systems in different contexts (individual and collaborative). Our analyses and findings showcase a few different ways, in which multimodal data sources can enrich our understanding of student learning and provide a more holistic picture.   \n",
              "106                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Authors collected student‐focused screen and webcam video which were useful for understanding students' learning processes and approaches based on detailed analyses of their interactions with the tutor interface, mouse movements, and out‐of‐tutor (in person) help‐seeking. High‐fidelity audio of students' collaborative dialogue was collected to generate high‐quality transcriptions of students' dialogue and apply an NLP approach to make use of the large quantity of audio dialogue. The verbal data allowed authors to identify linguistic features in students' collaborative dialogue that were highly predictive of math performance on pretest and posttest assessments, above and beyond any nonlinguistic variables.   \n",
              "107                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "108                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          We found that idling with limited speech (i.e., silence or backchannel feedback only) and without movement was negatively correlated with task performance and with participants’ subjective perceptions of the collaboration. However, being silent and focused during solution execution was positively correlated with task performance. Results illustrate that in some cases, multimodal patterns improved the predictions and improved explanatory power over the unimodal primitives.   \n",
              "109                                                                                                                                                                                                                                                                                                                                                                                                                    \"Here, we examine unimodal primitives (activity on the screen, speech, and body movements), and their multimodal combinations during remote CPS.\"\\n\\nMixed findings for uni- versus multi-modal:\\n\\n\"These results lead us to question: are the multimodal patterns better than the unimodal primitives? As illustrated above, we found evidence for both sides of the argument. In the case of code execution, the answer is no, but it is a yes in the case of idling. However, it is important to go beyond the significant correlations as there is an informative signal in the non-significant ones as well. For example, consider idling once again. By itself, this pattern is negatively correlated with the task score (r = -.21) and the correlation is even more negative when idling is accompanied by silence/back channeling and little movement (r = -.35). However, there are many other configurations where idling is weak or negligible predictor of task score. For example, idling occurring in the context of the contributors speaking with some movement is more weakly correlated with task score (r = -.11) and the correlation is essentially null when idling is accompanied with the controller speaking and some movement (r = -.06). Thus, even when they do not improve predictive power, multimodal patterns help contextualize and reveal nuances in the unimodal primitives. This supports the overall idea of multimodal learning analytics in which the additional modalities (speech and body movement in our case) help to understand unclear patterns such as idling. This finding is interesting from two perspectives.\"   \n",
              "110                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "111                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The experiments on authentically collected student dataset showed that it is beneficial to have separate classification pipelines for different learning sections of Instructional and Assessment. For the Instructional section, using only Appearance modality yields an F1-measure of 0.74, compared to fused performance of 0.70. For Assessment, fusing all three modalities (F1-measure of 0.89) provide a prominent improvement over the best performing unimodality (0.81 for Appearance).   \n",
              "112                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Mixed findings for uni- versus multi-modal:\\n\\n\"The experiments on authentically collected student dataset showed that it is beneficial to have separate classification pipelines for different learning sections of Instructional and Assessment. For the Instructional section, using only Appearance modality yields an F1-measure of 0.74, compared to fused performance of 0.70. For Assessment, fusing all three modalities (F1-measure of 0.89) provide a prominent improvement over the best performing unimodality (0.81 for Appearance). Interestingly, although Context-Performance modality performs poorly for the Off-Task class when considered alone, it helps to eliminate false positives (especially for the Off-Task class) when incorporated into the other modalities. In summary, we can say that for Instructional section types, Appearance modality provides acceptable results; whereas for Assessment sections, all available information should be fused to achieve best performance.\"   \n",
              "113                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "114                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Overall, our study suggests that coupling style can be operationalised as a multimodal indicator suitable to investigate complex collaboration dynamics in games, thus confirming what has been found in other domains   \n",
              "115                                                                                                                                                                                                                                                                                                                           \"Firstly, we found that coupling styles can be used to characterise collaboration in a co-located SG. Consistently with previous research (Isenberg et al, 2010; Niu et al, 2018), our results showed that teams displayed both close and loose coupling styles while performing individual actions to accomplish shared goals. Interestingly, we found a positive association between the time spent working closely coupled and the individual interactions with the technology. This suggests that the pursuit of collective goals requires players to continuously alternate and integrate individual planning and action with closely-coupled, likely to verify and synchronise their own actions with others. Secondly, we found that the perceived quality of collaboration does not appear to be an effective indicator of collaboration quality by itself. However, its small association with close-coupling style suggests a conscious, continuous, and proactive approach to collaboration, since players who appreciate the value of collaboration also seem to actively engage in closely-coupled interactions with others. Thirdly, our findings suggest that better-performing teams do work more closely-coupled and alternate their interactions with individual work. This result indicates that freely alternating individual work with closely-coupled interaction is an effective collaboration strategy, and that collaborative SGs should afford this opportunity. Overall, our study suggests that coupling style can be operationalised as a multimodal indicator suitable to investigate complex collaboration dynamics in games, thus confirming what has been found in other domains (Isenberg et al, 2010; Niu et al, 2018).\"   \n",
              "116                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "117                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \"We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain corpora and classification procedures resulted in the best performance on an argument structure mining task.\"   \n",
              "118                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \"We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain corpora and classification procedures resulted in the best performance on an argument structure mining task. Results of the argument quality experiments showed that argument com- prehensibility is affected by the number of referring expressions, information complexity, and presentation fluency. Presence of intensification and segmentation markers, position and movements of hands/ams and certain postures may affect the perception of the clarity, persuasiveness, and credibility of debaters.\"   \n",
              "119                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "120                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    System architecture is functional in predicting novice and expert compressions with low error rate   \n",
              "121                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \"...we collected observations that, while cannot be generalised, provide some indication that the feedback of the CPR tutor had a positive influence on the CPR performance on the target classes. To sum up, the architecture used for the CPR Tutor allowed for provision of real-time multimodal feedback (H1) and the generated feedback seem to have a short-term positive influence on the CPR performance on the target classes considered.\"   \n",
              "122                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "123                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Significant impact on the teacher's scaffolding behavior and student engagement (less bordem)   \n",
              "124                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      SEAT had positive impact on student engagement and was also helpful to teachers.   \n",
              "125                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "126                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Feedback generated by RAP is similar to human feedback across several dimensions and similar to other more complex systems   \n",
              "127                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \"It revealed an overwhelmingly positive perception of the system especially in the dimensions of perceived usefulness and feedback which were rated as excellent by 65% and 58% of the students respectively. The qualitative analysis helped discover specific issues, on the positive side, students commented on the potential of the system to quickly learn some basic presentation skills: \"I would like to see this system used in our Communications class\". On the negative side, students commented that they sometimes were aware that they were being recorded and that the environment was too small. Also, some students felt uncomfortable with a pre-recorded audience because it didn’t seem to react to their presentation: \"the audience had always the same expressions\". Overall, the students agreed that the system was useful and that they learned about their own presentation skills while using it.\"   \n",
              "128                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "129                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The use of MMD can help to triangulate with traditional research methods and explore hidden cognitive states   \n",
              "130  \"During informed problem solving episodes, we observed that children’s physiological stress, cognitive load, and emotional regu- lation were the highest. When children are presented a problem to solve, they may feel under pressure (external or self-imposed [60]) to answer the question correctly.\"\\n\\n\"During our study, children interacted with a MBEG; however, despite the intended “fun factor” that typically accompanies games, the pressure to academically perform (i.e., correctly match a card-box pair) may have elevated children’s stress levels [38, 87]. This may explain why children’s stress levels peaked during episodes of informed problem solving. In a similar vein, increased levels of cognitive load during informed problem solving may be directly linked to the mental effort that children expended as they reasoned through problems [88].\"\\n\\n\"Lastly, emotional regulation relates to children’s HRV [9, 97]. A plausible reason for observing the highest levels of emotional regulation during informed problem solving might be due to the immediate feedback that children received directly after they attempted to make a card-box match. The anticipation of the MBEG assessment/evaluation may have influenced children’s heart rate, causing high levels of variability as children invested themselves in informed problem solving. Thus, in accordance with prior research [32], we hypothesise that the feedback in general, may have triggered cognitive and affective responses which affected learning, particularly during this ongoing tasks (i.e., a collection of questions asked in series).\"\\n\\n\"Contrary to previous research [69, 84], our results did not indicate a connection between guessing behaviour and children’s lack of engagement during their interactions with the MBEG (Figure 6, top right). As such, we propose that during episodes of guessing, children experienced some degree of external and/or self-imposed pressures to determine answers correctly (as during informed problem solving).\"   \n",
              "131                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "132                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Corse-grained learning trajectories were able to identify key moments during learning that warrented futher analysis with other MMD   \n",
              "133                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \"Our results from this analysis showed how students’ early experiences struggling with a novel concept could significantly affect both their entire learning trajectories within an activity and pre-test–post-test measurements of learning gains related to that concept. It provided evidence of an important “moment” for early instructional intervention.\\n\\nThe analysis we conducted on the Concentration knowledge component was an example of how a knowledge component-centred analysis can benefit from this multi-step approach as well. Our results led to a modification in the knowledge component assignment to problem steps within a ChemVLab+ activity as well as instructional implications for promoting better learning of a previously hidden conceptual difficulty.\"   \n",
              "134                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "135                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \"each approach provides different affordances depending on the similarity metric and the dependent variable.\"\\n \"The take-away is that there are several strategies for doing multimodal learning analytics, and that many of these approaches can provide a meaningful glimpse into a complex data set, glimpses that may be difficult to identify using traditional approaches.\"   \n",
              "136                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \"Looking across analyses, there are clear instances where each provided some novel insights. In this sense, the overall algorithm appears to have relevance for studying learning, success and experimental condition; but honing in on these correlations requires different modes of analysis.\\n\\nAs a whole this article has shown that success, learning and process are not equivalent, though they may occasionally overlap. Thus, when thinking about measuring the effectiveness of a given learning environment it is important to be clear about which metrics one hopes to optimize. At the same time, this article has provided additional evidence that experimental condition can have an impact on learning, success and process. Because of this, one has to be cognizant about how to develop learning and reasoning approaches that allow the environment to realize the desired outcomes.\"   \n",
              "137                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "138                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \"Experiments of different types supported fairly reliable identification of markers from multimodal data, and linking these to assessments of debater confidence level and intensification behaviour.\"   \n",
              "139                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \"In line with previous empirical findings, we acknowledge that persuasive speech is rather difficult to characterize. Neverthe- less, based on theoretical and empirical frameworks set up by Grice (1975), Gussenhoven (2002) and Hirschberg (2002), we were able to define a set of criteria which help us to explain observed regularities and define rules, strategies and constraints for the generation, assessment and correction of trainees’ debate performance. Experiments of different types supported fairly reliable identification of markers from multimodal data, and linking these to assessments of debater confidence level and intensification behaviour.\"   \n",
              "140                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "141                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \"This study contributes an initial understanding into how different modalities mediate students’ interactions and offers implications for scaffolding peer interactions during multimodal composing processes.\"   \n",
              "142                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \"Overall, students were more likely to share ideas and ask questions, and they tended to use quick- response strategies while multimodal composing.\"\\n\\n\"Students’ interview responses also suggested that providing short responses was a typical strategy during multimodal composing.\"\\n\\n\"When examining interactions across sessions, the group was more engaged in discussions at the beginning and the end of the project while fewer interactions occurred during the middle of their composing process.\"\\n\\n\"Giving commands occurred much less frequently than other interaction types (Figure 3).\"\\n\\n\"Students discussed more often about comics that combined visuals and text than other modal elements.\"\\n\\n\"Making learning visible in different modes was critical to foster peer interaction (Jahnke, Norqvist, &\\nOlsson, 2013).\"\\n\\n\"Results showed that there were interactional differences based on different modes.\"\\n\\n\"While comparing discussions on static visual modes, namely images and multimodal comics, we found that images involved more self-oriented and less group-oriented contributions.\"\\n\\n\"Discussions on animations included more elaborated feedback.\"\\n\\n\"Written narrative provided the least opportunity for group-oriented contributions.\"   \n",
              "143                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "144                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \"Using MMLA techniques, we were able to spot differences in students’ motion sequences while students interacted with our embodied simulation.\"   \n",
              "145                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \"Using MMLA techniques, we were able to spot differences in students’ motion sequences while students interacted with our embodied simulation.\"   \n",
              "146                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "147                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \"The findings emphasize the need for learning analytics that are consequential for learning, rather than easy and convenient to collect.\"   \n",
              "148                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \"The models from M2 to M8 significantly outperformed M1 (see Figure 3), with M1 exhibiting a significantly lower adjusted\\nR2 value of 0.42 (std.dev. = 0.116).\"\\n\\ni.e., multimodal much better than logs alone for predicting debugging performance. Best model had every single modality.\\n\\n\"Using machine learning, we looked at the overall patterns in the data and performed feature importance among the 72 measures extracted from the multimodal data. Findings like ours combined with pedagogical intent from educators and theories from the LS, can advance the synergy between LA and LD by translating results in applicable design guidelines that can lead to improvements in the design of learning activities, instructional methods for teaching particular skills, and even the overall course (re)design. The complexity of the MMLA approach is congruous with learning theories because it can be used to understand how effectively students use the opportunities for learning as given in the LD. Such understanding promises to support versatile improvements in the LD in digital environments, from setting the right feedback loop (e.g., explaining misconceptions vs. challenging the student), to the design of personalized interventions, and modelling effective learning strategies considering skills and knowledge proficiency.\"\\n\\n\"The findings emphasize the need for learning analytics that are consequential for learning, rather than easy and convenient to collect.\"   \n",
              "149                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "150                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The analysis revealed six configurations that explain learners’ high performance and three that explain learners’ medium/low performance, driven by engagement measures coming from the multimodal data.   \n",
              "151                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \"This study demonstrated a consolidated analysis of multimodal data collected during an adaptive self-assessment activity, utilizing fsQCA for deeper understanding engagement in this setting. What this study adds to engagement literature is that when the learning tasks facilitate one’s own learning needs (motivation), it is likely that one will be deeper and more substantially involved with those tasks, yet the thorough analysis showcased that multimodal data can provide more than one engagement patterns to facilitate this objective.\"   \n",
              "152                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "153                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \"physical aspect of collaborative is an important part of this type of learning and that learning analytics systems can identify features that are relevant for helping researchers, teachers, and learners unpack what is happening.\"   \n",
              "154                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \"In this research study, we presented that where the students are looking, the distance between them, the motion of their hands our key features for a learning analytics system to be effectively used to identify collaboration in small groups of Engineering students.\"   \n",
              "155                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "156                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \"The best performing engagement classifiers achieved AUCs of .620 and .720 in Grades 8 and 12, respectively. We further investigated fusion strategies and found score-level fusion either improves the engagement classifiers or is on par with the best performing modality. We also investigated the effect of personalization and found that using only 60-seconds of person-specific data selected by margin uncertainty of the base classifier yielded an average AUC improvement of .084.\"   \n",
              "157                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \"In contrast to the previous works that used mainly handcrafted local (i.e., local binary patterns, Gabor filters) and precomputed features such as head pose or estimated facial action units, we showed that engagement as a 3-class classification problem can be predicted in the classroom. We gathered a large-scale classroom observation dataset and collected the observer ratings of student engagement for Grades 8 and 12 (N=15). In contrast to the limited training and testing protocols in the literature, our study is the first to validate the use of automated engagement analysis in the classroom.\\n\\nOur work proves that even a small amount of person-specific data could considerably enhance the performance of engagement classifiers. In comparison to the person-independent settings of many machine learning and computer vision tasks, personalization in engagement analysis significantly impacts performance. We find this to be the case because of personal differences in visible behaviors during levels of low and high engagement. Furthermore, engagement can even reveal variation in time (for instance, the indicators of engagement are not the same in different classes, i.e., math and history).\"   \n",
              "158                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "159                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \"In a 26-participant user study, we found that it is feasible to detect 6 types of emotion during learning via collected PPG signals and facial expressions and these modalities are complement with each other.\"   \n",
              "160                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \"In a 26-participant user study, we found that it is feasible to detect 6 types of emotion during learning via collected PPG signals and facial expressions and these modalities are complement with each other.\"\\n\\n\"AttentiveLearner2 achieved high performance as all our models outperformed the baseline. Moreover, we found PPG signals and facial expressions are complement each other. If FEA features can win in 3 emotions (Confusion, Happiness, and Self-efficacy), PPG features are the best solution for Curiosity, and feature fusion can improve detection performance for Boredom and Frustration.\"\\n\\n\"In a 26-participant user study, we found that by taking advantages from two modalities, AttentiveLearner2 achieved higher detection accuracy than models using only one modality across 6 different emotions. More importantly, these results were achieved on unmodified smartphones which supports the scalable deployment of AttentiveLearner2\"   \n",
              "161                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "162                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \"this research contributes to the relatively sparse literature in multimodal learning analytics by providing a balanced view of the teacher and student interactions with a data set collected in naturalistic home environments.\"   \n",
              "163                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \"Firstly, we note a clear causal pathway between the group of variables describing parents’ support and another group representing the child’s cognitive-affective experience.\"\\n\\n\"Secondly, we note the causal pathway from Profile to Affect and, indirectly, to Support.\"   \n",
              "164                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "165                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \"The statistical analysis has shown significant differences between the levels of independent variables related to table shape and how the effect differs between two different levels of education, and this was further supported by a qualitative analysis of the observations obtained from the video recording of the activities.\"   \n",
              "166                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \"Results show that the use of round tables (vs rectangular tables) leads to higher levels of on-task participation in the case of elementary school students. For university students, different table shapes seem to have a limited impact on their levels of participation in collaborative problem solving.\"   \n",
              "167                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "168                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \"Our combined multi-modal learning analytics and interaction analysis methodology enabled us to identify two multi-modal profles of learners who have learning gains and one multi-modal profle of learners who do not have learning gains.\"\\n\\n\"Using this approach, we are able to build the multimodal behavioral profles for each group of learners.\"   \n",
              "169                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \"Our combined multi-modal learning analytics and interaction analysis methodology enabled us to identify two multi-modal profles of learners who have learning gains and one multi-modal profle of learners who do not have learning gains.\"   \n",
              "170                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "171                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \"the architecture used for the CPR Tutor allowed for the provision of real-time multimodal feedback, and the generated feedback seemed to have a short-term positive influence on the considered CPR performance indicators.\"\\n\\n\"The CPR Tutor automatically recognises and assesses the quality of the chest compressions according to five CPR performance\\nindicators. It detects training mistakes in real-time by analysing a multimodal data stream consisting of kinematic and electromyographic data. Based on this assessment, the CPR Tutor provides audio feedback to correct the most critical mistakes and improve the CPR performance.\"   \n",
              "172                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \"we collected findings that, while they cannot be generalised, indicate that the feedback of the CPR tutor had a short-term positive influence on the CPR performance in the target classes.\"   \n",
              "173                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "174                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Exploration, increasing confluence during Discovery, and high confluence during Fluency.\"\\n\\n\"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Exploration, increasing confluence during Discovery, and high confluence during Fluency. Towards the end of the Discovery stage, a coordination of coordinations (Piaget, 1970) developed wherein the coordination between the left- and right hands became coordinated with newly developed gaze structures spanning different screen locations.\"   \n",
              "175                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \"Our findings point to the importance of MMLA work that attunes to intermodal dynamics of learning, both as a pragmatic resource for identifying key moments in learning and as a resource for refining theoretical understandings of learning processes.\"   \n",
              "176                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "177                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \"In summary, the results from our evaluation of personalized and general models to automatically extract orchestration graphs highlight the fact that machine learning models can be successfully trained with such multimodal sensor data, using relatively low-level features.\"   \n",
              "178                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \"In summary, the results from our evaluation of personalized and general models to automatically extract orchestration graphs highlight the fact that machine learning models can be successfully trained with such multimodal sensor data, using relatively low-level features.\"   \n",
              "179                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "180                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \"Our work exemplifies how the confluence of MMD and video coding can go further than data triangulation, and contribute to a holistic understanding of children’s play and problem-solving behaviours during their interactions with MBLT, by enabling researchers and designers the capacity to cater to children’s cognitive, affective and physiological processes to support learning through use of MBLT. To the best of our knowledge, there are no previous studies that use MMD from wearable and ubiquitous sensors (e.g., eye tracking glasses, wristbands and skeletal tracking) to investigate children’s behaviours in this context.\"   \n",
              "181                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \"Our work exemplifies how the confluence of MMD and video coding can go further than data triangulation, and contribute to a holistic understanding of children’s play and problem-solving behaviours during their interactions with MBLT, by enabling researchers and designers the capacity to cater to children’s cognitive, affective and physiological processes to support learning through use of MBLT.\"   \n",
              "182                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "183                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \"We conducted a case study to compare the visualizations provided by the system in two different situations: collaborative and competitive activities. The results suggest that the provided visualizations help to identify issues on cognitive contribution, assimilation, self-regulation, and integration of the team members. They could also support teachers to decide whether they must assist a team in fostering collaboration.\\n\\nWhile the results are naturally constrained to the characteristics of the activities in which we tested the platform, they provide initial evidence about the technical fea-\\nsibility of extracting behavioral indicators and traces using MMLA to give insights onteam collaboration.\"   \n",
              "184                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \"The results suggest that the provided visualizations help to identify issues on cognitive contribution, assimilation, self-regulation, and integration of the team members. They could also support teachers to decide whether they must assist a team in fostering collaboration.\"   \n",
              "185                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "186                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \"Through our user studies, we found that the platform helps fulfill some of those goals by providing capabilities that can spur on amazement and excitement among traditional Minecraft users and novices. We also find that many of the multimodal components, while not immediately intuitive for users, proved to be preferred modes of game play. In this sense, we feel that this tool is moving in the right direction in terms of the system capabilities that it provides. Our analyses also point to the meaningful ways that multimodal data can be used to study student learning in these game-based environments, and free students from standardized testing and learning experiences.\"   \n",
              "187                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \"Through our user studies, we found that the platform helps fulfill some of those goals by providing capabilities that can spur on amazement and excitement among traditional Minecraft users and novices. We also find that many of the multimodal components, while not immediately intuitive for users, proved to be preferred modes of game play.\"   \n",
              "188                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "189                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \"In this paper, we show that MMLA and the state-of-the-art computational techniques can be used to generate insights into the \"black box\" of learning in students’ project-based activities. These insights generated from multimodal data can be used to inform teachers about the key features of project-based learning and help them support students appropriately in similar pedagogical approaches.\"   \n",
              "190                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \"In this paper, we show that MMLA and the state-of-the-art computational techniques can be used to generate insights into the \"black box\" of learning in students’ project-based activities. These insights generated from multimodal data can be used to inform teachers about the key features of project-based learning and help them support students appropriately in similar pedagogical approaches. Towards achieving this ultimate aim, this paper has three main contributions to the field. First, we show that the distances between students’ hands and faces while they are working on projects is a strong predictor of students’ artefact quality which indicates the value of student collaboration in these pedagogical approaches. Second, we show that both, new and promising approaches such as neural networks and more traditional regression approaches, can be used to classify MMLA data and both have advantages and disadvantages depending on the research questions and contexts being investigated. At last but not least, although, it is traditionally notoriously challenging to provide evidence about the robust and objective evaluations of project-based learning activities, techniques and types of data we presented here can be the first step towards effective implementation and evaluation of project-based learning at a scale.\"   \n",
              "191                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "192                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \"In this study, we have given special emphasis to the interpersonal metafunction (see Fig. 1), which is crucial to enhance EMI students’ active participation and engagement. The outcomes of this study point to the need to address multimodal interactional competence in professional development programs, as described in Morell et al. (2022) to promote effective lecturing in the diverse EMI scenarios.\"   \n",
              "193                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \"The exploration of how EMI lecturers use semiotic resources to construct meaning and to create engagement paves the way to a unified multimodal interactional competence. In general, the mastery of this competence enables lecturers to convert students from passive listeners/observers to active participants, giving them opportunities to engage in active learning, language usage and critical thinking.\"   \n",
              "194                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "195                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \"I was able to use multimodal data and machine learning to develop\\nplausible arguments for students’ differential learning gains.\"   \n",
              "196                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \"I was able to draw inferences related to productive engagement and disengagement in the context of collaborative problem solving. These inferences would have been difficult to articulate using only machine learning, and hard to identify using only human coding. Hence, I argue that an intermediate model that leverages the affordances of multimodal data and computation, but leaves inference development to trained scholars could offer a viable alternative to purely qualitative or machine learning approaches.\"   \n",
              "197                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "198                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \"The precision (73%) and recall (61%) for the combined devices (Smartphone + Kinect) achieved the best results compared to the other two classes.\"   \n",
              "199                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \"We observed, within our context, that smartphone sensors by themselves are unable to perform better than the Kinect. In addition, it is likely that the smartphone sensors are able to classify the strokes by complete chance due to 51% of accuracy. However,the performance improves when both of the devices are combined.\"   \n",
              "200                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "201                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \"Overall, our work shows that capturing multimodal data can help us increase the prediction accuracy of users’ learning performance in learner–computer interaction (LCI). In addition, the study shows that the most commonly used data-stream (i.e., keystrokes) is the poorest proxy of our learning performance. Thus, leveraging advances in contemporary learning environments and physiological sensing (wearable, EEG etc.), we provide evidence that multimodal data can be a viable method to accurately track users’ states during learning, thereby providing unique possibilities of closing the loop between the learning technology and the learner. Therefore, the incorporation of multimodal data enables HCI and learning technology researchers to examine unscripted, complex tasks in more holistic and accurate ways.\"\\n\\nIdentified specific multimodal features that were best predictors of performance.   \n",
              "202                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \"Overall, our work shows that capturing multimodal data can help us increase the prediction accuracy of users’ learning performance in learner–computer interaction (LCI). In addition, the study shows that the most commonly used data-stream (i.e., keystrokes) is the poorest proxy of our learning performance. Thus, leveraging advances in contemporary learning environments and physiological sensing (wearable, EEG etc.), we provide evidence that multimodal data can be a viable method to accurately track users’ states during learning, thereby providing unique possibilities of closing the loop between the learning technology and the learner. Therefore, the incorporation of multimodal data enables HCI and learning technology researchers to examine unscripted, complex tasks in more holistic and accurate ways.\"   \n",
              "203                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "204                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \"Through comparison we can find that our proposed model is superior to the most commonly used model of feature fusion in the past.\"   \n",
              "205                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \"From the perspective of multiple features, we get the fusion of multiple short video features by using two fully connected layers. It can be found from Table I that our method can more accurately predict the student's learning participation than the traditional method of weighted summation and averaging.\"   \n",
              "206                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "207                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \"Results indicate that multimodal approaches outperform unimodal baseline classifiers, and feature-level concatenation offers the highest performance among the data fusion techniques.\"   \n",
              "208                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \"We show the improvement that multimodal classifiers achieve compared with unimodal classifiers for both modalities. We also demonstrate that SVMs outperform ANNs as a unimodal classifier in this particular domain. Finally, we demonstrate that data fusion is an effective way to combine multiple modalities, either prior to or following classification.\"   \n",
              "209                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "210                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \"The proposed method with multi-person detection, multi-modality, group engagement score, inquiry intervention and with an accuracy of 0.77 for a test data of more than 350 students, outperforms the existing methods.   \n",
              "211                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \"The overall experimental results demonstrate that there is a positive correlation with r = 0.74 between students’ affective states and their performance. Proposed inquiry intervention improved the students’ performance as there is a decrease of 65%, 43%, 43%, and 53% in overall in-attentive affective state instances using the inquiry interventions in e-learning, flipped classroom, classroom and webinar environments, respectively.\"   \n",
              "212                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "213                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \"We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements in the video modality are the most significant unimodal indicators of impasse.\"   \n",
              "214                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \"We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements in the video modality are the most significant unimodal indicators of impasse. We also trained several multimodal models and found that combining indicators from these three modalities provided the best impasse detection performance.\"   \n",
              "215                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "216                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \"We illustrate the approach from the automated analysis of healthcare simulations to train nurses, generating four visual proxies that fuse multimodal data into higher order patterns.\"   \n",
              "217                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \"we introduce the concept of collaboration translucence, an approach to make visible selected features of group activity. This is grounded both theoretically (in the physical, epistemic, social and affective dimensions of group activity), and contextually (using domain-specific concepts). We illustrate the approach from the automated analysis of healthcare simulations to train nurses, generating four visual proxies that fuse multimodal data into higher order patterns.\"   \n",
              "218                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "\n",
              "    Full-Read 3 by Researcher Reviewer  \\\n",
              "0                       Joyce        1   \n",
              "1                     Eduardo        2   \n",
              "2               Joyce/Eduardo      1&2   \n",
              "3                       Joyce        1   \n",
              "4                     Eduardo        2   \n",
              "5               Joyce/Eduardo      1&2   \n",
              "6                       Joyce        1   \n",
              "7                     Eduardo        2   \n",
              "8               Joyce/Eduardo      1&2   \n",
              "9                       Joyce        1   \n",
              "10                    Eduardo        2   \n",
              "11              Joyce/Eduardo      1&2   \n",
              "12                      Joyce        1   \n",
              "13                    Eduardo        2   \n",
              "14              Joyce/Eduardo      1&2   \n",
              "15                      Joyce        1   \n",
              "16                    Eduardo        2   \n",
              "17              Joyce/Eduardo      1&2   \n",
              "18                      Joyce        1   \n",
              "19                    Eduardo        2   \n",
              "20              Joyce/Eduardo      1&2   \n",
              "21                      Joyce        1   \n",
              "22                    Eduardo        2   \n",
              "23              Joyce/Eduardo      1&2   \n",
              "24                      Joyce        1   \n",
              "25                    Eduardo        2   \n",
              "26              Joyce/Eduardo      1&2   \n",
              "27                      Joyce        1   \n",
              "28                    Eduardo        2   \n",
              "29              Joyce/Eduardo      1&2   \n",
              "30                      Joyce        1   \n",
              "31                    Eduardo        2   \n",
              "32              Joyce/Eduardo      1&2   \n",
              "33                      Joyce        1   \n",
              "34                    Eduardo        2   \n",
              "35              Joyce/Eduardo      1&2   \n",
              "36                      Joyce        1   \n",
              "37                    Eduardo        2   \n",
              "38              Joyce/Eduardo      1&2   \n",
              "39                      Joyce        1   \n",
              "40                    Eduardo        2   \n",
              "41              Joyce/Eduardo      1&2   \n",
              "42                      Joyce        1   \n",
              "43                    Eduardo        2   \n",
              "44              Joyce/Eduardo      1&2   \n",
              "45                      Joyce        1   \n",
              "46                    Eduardo        2   \n",
              "47              Joyce/Eduardo      1&2   \n",
              "48                      Joyce        1   \n",
              "49                    Eduardo        2   \n",
              "50              Joyce/Eduardo      1&2   \n",
              "51                      Joyce        1   \n",
              "52                    Eduardo        2   \n",
              "53              Joyce/Eduardo      1&2   \n",
              "54                    Eduardo        1   \n",
              "55                      Joyce        2   \n",
              "56              Eduardo/Joyce      1&2   \n",
              "57                    Eduardo        1   \n",
              "58                      Joyce        2   \n",
              "59              Eduardo/Joyce      1&2   \n",
              "60                    Eduardo        1   \n",
              "61                      Joyce        2   \n",
              "62              Eduardo/Joyce      1&2   \n",
              "63                    Eduardo        1   \n",
              "64                      Joyce        2   \n",
              "65              Eduardo/Joyce      1&2   \n",
              "66                    Eduardo        1   \n",
              "67                      Joyce        2   \n",
              "68              Eduardo/Joyce      1&2   \n",
              "69                    Eduardo        1   \n",
              "70                      Joyce        2   \n",
              "71              Eduardo/Joyce      1&2   \n",
              "72                    Eduardo        1   \n",
              "73                      Joyce        2   \n",
              "74              Eduardo/Joyce      1&2   \n",
              "75                    Eduardo        1   \n",
              "76                      Joyce        2   \n",
              "77              Eduardo/Joyce      1&2   \n",
              "78                    Eduardo        1   \n",
              "79                      Joyce        2   \n",
              "80              Eduardo/Joyce      1&2   \n",
              "81                    Eduardo        1   \n",
              "82                      Joyce        2   \n",
              "83              Eduardo/Joyce      1&2   \n",
              "84                    Eduardo        1   \n",
              "85                      Joyce        2   \n",
              "86              Eduardo/Joyce      1&2   \n",
              "87                    Eduardo        1   \n",
              "88                      Joyce        2   \n",
              "89              Eduardo/Joyce      1&2   \n",
              "90                    Eduardo        1   \n",
              "91                      Joyce        2   \n",
              "92              Eduardo/Joyce      1&2   \n",
              "93                    Eduardo        1   \n",
              "94                      Joyce        2   \n",
              "95              Eduardo/Joyce      1&2   \n",
              "96                    Eduardo        1   \n",
              "97                      Joyce        2   \n",
              "98              Eduardo/Joyce      1&2   \n",
              "99                    Eduardo        1   \n",
              "100                     Joyce        2   \n",
              "101             Eduardo/Joyce      1&2   \n",
              "102                   Eduardo        1   \n",
              "103                     Joyce        2   \n",
              "104             Eduardo/Joyce      1&2   \n",
              "105                   Eduardo        1   \n",
              "106                     Joyce        2   \n",
              "107             Eduardo/Joyce      1&2   \n",
              "108                     Caleb        1   \n",
              "109                   Clayton        2   \n",
              "110             Caleb/Clayton      1&2   \n",
              "111                     Caleb        1   \n",
              "112                   Clayton        2   \n",
              "113             Caleb/Clayton      1&2   \n",
              "114                     Caleb        1   \n",
              "115                   Clayton        2   \n",
              "116             Caleb/Clayton      1&2   \n",
              "117                     Caleb        1   \n",
              "118                   Clayton        2   \n",
              "119             Caleb/Clayton      1&2   \n",
              "120                     Caleb        1   \n",
              "121                   Clayton        2   \n",
              "122             Caleb/Clayton      1&2   \n",
              "123                     Caleb        1   \n",
              "124                   Clayton        2   \n",
              "125             Caleb/Clayton      1&2   \n",
              "126                     Caleb        1   \n",
              "127                   Clayton        2   \n",
              "128             Caleb/Clayton      1&2   \n",
              "129                     Caleb        1   \n",
              "130                   Clayton        2   \n",
              "131             Caleb/Clayton      1&2   \n",
              "132                     Caleb        1   \n",
              "133                   Clayton        2   \n",
              "134             Caleb/Clayton      1&2   \n",
              "135                     Caleb        1   \n",
              "136                   Clayton        2   \n",
              "137             Caleb/Clayton      1&2   \n",
              "138                     Caleb        1   \n",
              "139                   Clayton        2   \n",
              "140             Caleb/Clayton      1&2   \n",
              "141                     Caleb        1   \n",
              "142                   Clayton        2   \n",
              "143             Caleb/Clayton      1&2   \n",
              "144                     Caleb        1   \n",
              "145                   Clayton        2   \n",
              "146             Caleb/Clayton      1&2   \n",
              "147                     Caleb        1   \n",
              "148                   Clayton        2   \n",
              "149             Caleb/Clayton      1&2   \n",
              "150                     Caleb        1   \n",
              "151                   Clayton        2   \n",
              "152             Caleb/Clayton      1&2   \n",
              "153                     Caleb        1   \n",
              "154                   Clayton        2   \n",
              "155             Caleb/Clayton      1&2   \n",
              "156                     Caleb        1   \n",
              "157                   Clayton        2   \n",
              "158             Caleb/Clayton      1&2   \n",
              "159                     Caleb        1   \n",
              "160                   Clayton        2   \n",
              "161             Caleb/Clayton      1&2   \n",
              "162                     Caleb        1   \n",
              "163                   Clayton        2   \n",
              "164             Caleb/Clayton      1&2   \n",
              "165                   Clayton        1   \n",
              "166                     Caleb        2   \n",
              "167             Clayton/Caleb      1&2   \n",
              "168                   Clayton        1   \n",
              "169                     Caleb        2   \n",
              "170             Clayton/Caleb      1&2   \n",
              "171                   Clayton        1   \n",
              "172                     Caleb        2   \n",
              "173             Clayton/Caleb      1&2   \n",
              "174                   Clayton        1   \n",
              "175                     Caleb        2   \n",
              "176             Clayton/Caleb      1&2   \n",
              "177                   Clayton        1   \n",
              "178                     Caleb        2   \n",
              "179             Clayton/Caleb      1&2   \n",
              "180                   Clayton        1   \n",
              "181                     Caleb        2   \n",
              "182             Clayton/Caleb      1&2   \n",
              "183                   Clayton        1   \n",
              "184                     Caleb        2   \n",
              "185             Clayton/Caleb      1&2   \n",
              "186                   Clayton        1   \n",
              "187                     Caleb        2   \n",
              "188             Clayton/Caleb      1&2   \n",
              "189                   Clayton        1   \n",
              "190                     Caleb        2   \n",
              "191             Clayton/Caleb      1&2   \n",
              "192                   Clayton        1   \n",
              "193                     Caleb        2   \n",
              "194             Clayton/Caleb      1&2   \n",
              "195                   Clayton        1   \n",
              "196                     Caleb        2   \n",
              "197             Clayton/Caleb      1&2   \n",
              "198                   Clayton        1   \n",
              "199                     Caleb        2   \n",
              "200             Clayton/Caleb      1&2   \n",
              "201                   Clayton        1   \n",
              "202                     Caleb        2   \n",
              "203             Clayton/Caleb      1&2   \n",
              "204                   Clayton        1   \n",
              "205                     Caleb        2   \n",
              "206             Clayton/Caleb      1&2   \n",
              "207                   Clayton        1   \n",
              "208                     Caleb        2   \n",
              "209             Clayton/Caleb      1&2   \n",
              "210                   Clayton        1   \n",
              "211                     Caleb        2   \n",
              "212             Clayton/Caleb      1&2   \n",
              "213                   Clayton        1   \n",
              "214                     Caleb        2   \n",
              "215             Clayton/Caleb      1&2   \n",
              "216                   Clayton        1   \n",
              "217                     Caleb        2   \n",
              "218             Clayton/Caleb      1&2   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Reviewer Notes  \n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                NaN  \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                NaN  \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                NaN  \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                NaN  \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                NaN  \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                NaN  \n",
              "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                NaN  \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                NaN  \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                NaN  \n",
              "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                NaN  \n",
              "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "24                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "32                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "40                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "41                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "42                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "43                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "44                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "45                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "46                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "47                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "48                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "49                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "51                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "53                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "54                                                                                                                                                                                                                                                                                                                                                                                               model-free for correlation study, mode-based for collaborative and non-collaborative classification  \n",
              "55                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "56                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "57                                                                                                                                                                                                                                                                                                                                                                                                                                                                   statistical model -> model-free  \n",
              "58                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "59                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "60                                                                                                                                                                                                                                                                                                                                                                                                                                                  Relating to cognitive load theory as their model  \n",
              "61                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "63                                                                                                                                                                                                                                                                                                                                                                                           Relating their research to Roschelle's 1992 framework of convergent conceptual change for collaboration  \n",
              "64                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "65                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "66                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Data stories  \n",
              "67                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "68                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "69                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Data imputation  \n",
              "70                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "72                                                                                                                                                                                                                                                                                                                                                                                                                                                         Predicting student drop-out from LMS data  \n",
              "73                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "74                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "75                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "76                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "77                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "78                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "79                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "81                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "82                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "83                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "84                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "85                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "86                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "87                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "88                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "89                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "90                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "91                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "92                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "93                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "94                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "95                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "96                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "97                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "98                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "99                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NaN  \n",
              "100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "101                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "102                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "103                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "104                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "105                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "106                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "107                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NLP  \n",
              "108                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "109                                                                                                                                                                                                                                                               CPS study, Minecraft playing via Blockly. Model-free because only stats methods/pattern extraction and no AI/ML model or formal theoretical model. Compared unimodal to multimodal, and made arguments for both given the context.  \n",
              "110                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "111                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "112                                                                                                                                                                                                                                                                                                                                                                                                         Math virtual environment with 9th graders for engagement detection. Random Forest model.  \n",
              "113                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "114                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "115                                                                                                                                                                                                                                                                                                                                            Blended game environment for sustainable development (collaborative). Only stats methods applied, and no formal model presented or addressed via RQs.  \n",
              "116                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "117                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "118                                                                                                                                                                                                                                               Real-time and post hoc vitual debate coach. SVM model for classification. Could make argument for blended environment with virtual coach, but I would consider the environment to be in person, as that is where the students interact and debate.  \n",
              "119                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "120                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "121                                                                                                                                                                                                                                     CPR tutor. Model-based via RNN (LSTM) classification. Environment is \"European University Hospital.\" No mention of undergraduats, and I got the impression this was professional development given the students needed to have recurring CPR qualifications.  \n",
              "122                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "123                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "124                                                                                                                                                                                                                                                                                                                                                                      Real-time, multimodal Student Engagement Analytics Technology. Model-based via ML models (RF) referenced in previous works.  \n",
              "125                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "126                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "127                                                                                                                                                                                                                                                                                                                                             RAP system evaluation. Blended because interactions with screen crowd and presentation but IRL presenting. Model-based due to AI/ML methods like RF.  \n",
              "128                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "129                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "130                                                                                                                                                                                                                                                       Marvy learns. Embodied...child must move physicall to get on-screen \"monster\" Marvy to place shapes in the right buckets. No AI/ML model (stats via ANOVA). Lots of references to \"groundwork\" that directs research, but no formal model.  \n",
              "131                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "132                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "133                                                                                                                                                                                                                                                                                                                                     Chemistry Virtual Lab. ChemVLab+ tutor. Model-based via regressing post-test scores and calculating probability individual student will get problem correct.  \n",
              "134                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "135                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "136                                                                                                                                                                                                                                                                                                Dyads working with materials like paper on engineering task. High school and undergraduate students. No AI/ML supervised model, no formal theorhetical model. Does clustering count as model? YES  \n",
              "137                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "138                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "139                                                                                                                                                                                                                                                                                                                                                                           Designing a virtual debate coach. Model-based via SVM classification and also three different theorhetical frameworks.  \n",
              "140                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "141                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "142                                                                                                                                                                                                                                                                                                                                            Focus on how different modes influence student interactions over\\ntime during science multimodal composing. Provides model via theoretical framework.  \n",
              "143                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "144                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "145                                                                       Embodied predator-prey ecosystem environment. No AI/ML or formal theoretical model. Does HMM count as model? YES\\n\\n\"The aim of this paper is to show how multimodal learning analytics (MMLA) can help understand how elementary students explore the concept of feedback loops while controlling an embodied simulation of a predator-prey ecosystem using hand movements as an interface with the computer simulation.\"  \n",
              "146                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "147                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "148                                                                                                                                                                                                                                                                                                                                                                                               Java debugging env looking at just logs as a baseline then compared to multimodal. RF model-based.  \n",
              "149                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "150                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "151                                                                                                                                                                                                                                                                                                                       Fuzzy set qualitative comparative analysis (fsQCA) approach to shed light to learners’ engagement patterns. Model free because no formal theoretical model or AI/ML model.  \n",
              "152                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "153                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "154                                                                                                                                                                                                                                                                                                                                                                                                              Determine best MMLA features for CPS. Blened Arduino+IDE. Model-based (regression).  \n",
              "155                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "156                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "157                                                                                                                                                                                                                                                                                                                              AV facial detection. Model-based: RF, SVM, MLP, LSTM. Classroon setting (instructional, multi-person, physical). Many subjects stemming across STEM and humanities.  \n",
              "158                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "159                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "160                                                                                                                                                                             AttentiveLearner2 implicitly infers learners’ affective and cognitive states during learning by analyzing learners’ PPG signals and facial expressions to improve mobile MOOC learning. \\n\\nModel-based via multimodal emotion detection, but it's a short paper and the exact classification model is not provided.  \n",
              "161                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "162                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "163  Model-based via presented framework.\\n\\n\"We explore an analytical framework that is explainable and amenable to incorporating domain knowledge. The proposed framework combines statistical approaches in Sparse Multiple Canonical Correlation, causal discovery, and inference methods for observations. We demonstrate this framework using a multimodal one-on-one math problem-solving coaching dataset collected in naturalistic home environments involving parents and young children.\"  \n",
              "164                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "165                                                                                                                                                                                                                                                                                                                                                                                                     2 environments: blended arduino and blended pen/paper. \\nSTEM for CS, humanities for design.  \n",
              "166                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "167                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "168                                                                                                                                                                                                                               blended: environment is the screen, but outside env is physical robot agent IRL.\\nmodel-based (SVM, RF) and model-free because classification and clustering used, along with stats.\\nsubject is STEM because goal of env is to teach about minimum-spanning-tree.  \n",
              "169                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "170                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "171                                                                                                                                                                                                                                                                                                                                                                                                                                                                           CPR tutor. LSTM model.  \n",
              "172                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "173                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "174                                                                                                                                                                                                                                                                                                                                                                                                                          Physcal interaction with tablet to change ratios of two bars on screen.  \n",
              "175                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "176                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "177                                                                                                                                                                                                                                                                                                                                                                                                      Orchestration graphs. Multi-person because classroom-based and therefore multiple students.  \n",
              "178                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "179                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "180                                                                                                                                                                                                                                                                                                                                                            Marvy learns (geometry). RF for classification (model-based). Exploratory Factor Analysis (EFA) for statistical analysis (model-free)  \n",
              "181                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "182                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "183                                                                                                                                                                                                                                                                                                           2 env tasks: collaboratively write a sentence about what might be the first article of Chile's new constitution, and decide who should be saved in a bunker in an apocalyptic scenario  \n",
              "184                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "185                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "186                                                                                                                                                                                                                                                                                                                                                                                                                                                                Multicraft: multimodal Minecraft.  \n",
              "187                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "188                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "189                                                                                                                                                                                                                                                                                                                                                                                                        Arduino + browser IDE, so blended engineering. DNN + regression. Also tested SVM, NB, LR.  \n",
              "190                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "191                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "192                                                                                                                                                                                                                                                                                                                                                                                       Workshop for professional development for professors teaching in English (in non-English speaking country)  \n",
              "193                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "194                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "195                                                                                                                                                                                                                                                                                                                              2 tasks: use sheet of paper to hold text book, and use other materials to support a 0.5lb mass\\n\\nModel-based w/ DT classifier, model-free with k-means clustering.  \n",
              "196                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "197                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "198                                                                                                                                                                                                                                                                                                                                                                                                                                                         Table Tennis Tutor, LSTM classification.  \n",
              "199                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "200                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "201                                                                                                                                                                                                                                                                                                        Pac-Man. Model-based for regression, model-free for statistical methods.\\n\\nDoes training always map to training or is Pacman both a training environment that is informal, for instance?  \n",
              "202                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "203                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "204                                                                                                                                                                                                                                                                                                                                                                                                                           Engagement detection, DAiSEE dataset. Neural Turing Machine (NTM) RNN.  \n",
              "205                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "206                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "207                                                                                                                                                                                                                                                                                                                                                                    Affect detection in simulated medical environment. \"other\" for environment type because medical? SVM and DNN for model-based.  \n",
              "208                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "209                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "210                                                                                                                                                                                                                                                                                                  Ashwin paper! Automated inquiry-based instruction based on affective state. Two environments: classroom (multi-student) and e-learning (single-student). Classification via Inception and YOLO.  \n",
              "211                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "212                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "213                                                                                                                                                                                                                                                                                                                                                                                                                            Learning CS w/ Snap! collaboratively. SVM, BERT, MLP for model-based.  \n",
              "214                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "215                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "216                                                                                                                                                                                                                                                                                                                                                                                                                                                      Monash nursing group. Other for healthcare?  \n",
              "217                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  \n",
              "218                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c8b7ef6-74ce-45ac-a789-9df4df748038\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. The findings suggest that MMLA can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>Common case of multimodal outperform unimodal models, through the addition of gaze to classify student's posttest performance and interest.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1877483551</td>\n",
              "      <td>motion-based educational games: using multi-modal data to predict player’s performance</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR</td>\n",
              "      <td>PULSE,TEMP,EDA,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>COG</td>\n",
              "      <td>IEEE Conference on Games</td>\n",
              "      <td>6</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Authors conclude that the feature combination of gaze and physiological MMD provide the most accurate predictions of correct answers. They also show the feasibility of early prediction of children's performance by using half (as oppose to full) data lengths to extract features and predict correctness.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1877483551</td>\n",
              "      <td>motion-based educational games: using multi-modal data to predict player’s performance</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR</td>\n",
              "      <td>PULSE,TEMP,EDA,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>COG</td>\n",
              "      <td>IEEE Conference on Games</td>\n",
              "      <td>6</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Researchers show the ability to predict student's performance in embodied game using various data sources (gaze, physiological, skeleton). However, they note that using certain modalities (e.g., skeleton and physiological) reduce the predictive performance of the model.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1877483551</td>\n",
              "      <td>motion-based educational games: using multi-modal data to predict player’s performance</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR</td>\n",
              "      <td>PULSE,TEMP,EDA,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>COG</td>\n",
              "      <td>IEEE Conference on Games</td>\n",
              "      <td>6</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2000036002</td>\n",
              "      <td>predicting learners’ effortful behaviour in adaptive assessment using multimodal data</td>\n",
              "      <td>Kshitij Sharma</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR</td>\n",
              "      <td>EDA,TEMP,PULSE,EEG,GAZE,AFFECT</td>\n",
              "      <td>CLUST,CLS,PATT</td>\n",
              "      <td>MID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>7</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>Findings provide evidence that HMMs can encode the relationship between effort and behaviour (captured by the multimodal data) in a more efficient way than the other methods. A practical implication of the approach is that the derived HMMs also pinpoint the moments to provide preventive/prescriptive feedback to the learners in real-time, by building-upon the relationship between behavioural patterns and the effort the learners are putting in.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2000036002</td>\n",
              "      <td>predicting learners’ effortful behaviour in adaptive assessment using multimodal data</td>\n",
              "      <td>Kshitij Sharma</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR</td>\n",
              "      <td>EDA,TEMP,PULSE,EEG,GAZE,AFFECT</td>\n",
              "      <td>CLUST,CLS,PATT</td>\n",
              "      <td>MID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>7</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>The results show that the proposed method not only outperforms the contemporary classification algorithms but it also gives the educators several opportunities for providing (proactive) actionable feedback by pinpointing the exact moments in the learning activity\\nwhere feedback is needed.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2000036002</td>\n",
              "      <td>predicting learners’ effortful behaviour in adaptive assessment using multimodal data</td>\n",
              "      <td>Kshitij Sharma</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR</td>\n",
              "      <td>EDA,TEMP,PULSE,EEG,GAZE,AFFECT</td>\n",
              "      <td>CLUST,CLS,PATT</td>\n",
              "      <td>MID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>7</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2070224207</td>\n",
              "      <td>detecting medical simulation errors with machine learning and multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,LOGS</td>\n",
              "      <td>POSE,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>CAIM</td>\n",
              "      <td>Conference on Artificial Intelligence in Medicine</td>\n",
              "      <td>11</td>\n",
              "      <td>BLND</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>Used each Chest Compression as training sample by masking/windowing of the original time series, then trained an LSTM network with all these samples and were able to classify accurately the target classes, however discarding the rest of the time-series they were not able to detect if a CC happened. Author asks Doctorial Consortium how, given the available data, could they train a classifier able to detect whether a CC happened or not.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2070224207</td>\n",
              "      <td>detecting medical simulation errors with machine learning and multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,LOGS</td>\n",
              "      <td>POSE,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>CAIM</td>\n",
              "      <td>Conference on Artificial Intelligence in Medicine</td>\n",
              "      <td>11</td>\n",
              "      <td>BLND</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>Trained an LSTM to predict ['too slow', 'on-point', 'too fast'] for Chest compression training. Achieved 70-75% accuracy.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2070224207</td>\n",
              "      <td>detecting medical simulation errors with machine learning and multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,LOGS</td>\n",
              "      <td>POSE,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>CAIM</td>\n",
              "      <td>Conference on Artificial Intelligence in Medicine</td>\n",
              "      <td>11</td>\n",
              "      <td>BLND</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>12</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MF</td>\n",
              "      <td>Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoidance of filled pauses)</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>12</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MF</td>\n",
              "      <td>Authors showcase that the training tool improved manually defined scores between an initial and second use of the tool.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>12</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3051560548</td>\n",
              "      <td>temporal analysis of multimodal data to predict collaborative learning outcomes</td>\n",
              "      <td>Jennifer K. Olsen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,EYE</td>\n",
              "      <td>GAZE,LOGS,PROS,TRANS,QUAL</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>13</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3051560548</td>\n",
              "      <td>temporal analysis of multimodal data to predict collaborative learning outcomes</td>\n",
              "      <td>Jennifer K. Olsen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,EYE</td>\n",
              "      <td>GAZE,LOGS,PROS,TRANS,QUAL</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>13</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Evaluating how multimodal features contribute to a model's performance to predict learning gains. Audio features introduce noise that negatively impacted the error of the model.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3051560548</td>\n",
              "      <td>temporal analysis of multimodal data to predict collaborative learning outcomes</td>\n",
              "      <td>Jennifer K. Olsen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,EYE</td>\n",
              "      <td>GAZE,LOGS,PROS,TRANS,QUAL</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>13</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>3339002981</td>\n",
              "      <td>estimation of success in collaborative learning based on multimodal learning analytics features</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,LOGS,VIDEO,AUDIO</td>\n",
              "      <td>GAZE,LOGS,PROS,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICALT</td>\n",
              "      <td>International Conference on Advanced Learning Technologies</td>\n",
              "      <td>14</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, authors found that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>3339002981</td>\n",
              "      <td>estimation of success in collaborative learning based on multimodal learning analytics features</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,LOGS,VIDEO,AUDIO</td>\n",
              "      <td>GAZE,LOGS,PROS,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICALT</td>\n",
              "      <td>International Conference on Advanced Learning Technologies</td>\n",
              "      <td>14</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>Predicting the learning gains via classification (bad, ok, good) through gaze, logs, audio, and dialog. Determined that distance measures between hands and gaze fixations was the key features to predict students' performance.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>3339002981</td>\n",
              "      <td>estimation of success in collaborative learning based on multimodal learning analytics features</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,LOGS,VIDEO,AUDIO</td>\n",
              "      <td>GAZE,LOGS,PROS,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICALT</td>\n",
              "      <td>International Conference on Advanced Learning Technologies</td>\n",
              "      <td>14</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>3408664396</td>\n",
              "      <td>multimodal student engagement recognition in prosocial games</td>\n",
              "      <td>Athanasios Psaltis</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS</td>\n",
              "      <td>POSE,AFFECT,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>LATE</td>\n",
              "      <td>T-CIAIG</td>\n",
              "      <td>Transactions on Computational Intelligence and AI in Games</td>\n",
              "      <td>15</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Presented a novel methodology for the automatic recognition of student engagement in prosocial games aiming to capture the different dimensions of engagement, i.e., behavioral, cognitive, and affective, by exploiting real-time engagement cues from different input modalities (body motion and facial expression analysis to identify the affective state of students, features related to their cognitive and behavioral engagement based on the analysis of their interaction with the game)</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>3408664396</td>\n",
              "      <td>multimodal student engagement recognition in prosocial games</td>\n",
              "      <td>Athanasios Psaltis</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS</td>\n",
              "      <td>POSE,AFFECT,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>LATE</td>\n",
              "      <td>T-CIAIG</td>\n",
              "      <td>Transactions on Computational Intelligence and AI in Games</td>\n",
              "      <td>15</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Same as Joyce (first paragraph in conclusion)</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>3408664396</td>\n",
              "      <td>multimodal student engagement recognition in prosocial games</td>\n",
              "      <td>Athanasios Psaltis</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS</td>\n",
              "      <td>POSE,AFFECT,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>LATE</td>\n",
              "      <td>T-CIAIG</td>\n",
              "      <td>Transactions on Computational Intelligence and AI in Games</td>\n",
              "      <td>15</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>804659204</td>\n",
              "      <td>towards smart educational recommendations with reinforcement learning in classroom</td>\n",
              "      <td>Su Liu</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SENSOR</td>\n",
              "      <td>PULSE,AFFECT,GAZE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>TALE</td>\n",
              "      <td>International Conference on Teaching, Assessment and Learning for Engineering</td>\n",
              "      <td>48</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>By aplying reinforcement learning to select appropriate learning activities without being aware of the learning model of each student, simulation results showed that the proposed learning recommendation systems can promote students’ performance with higher average scores in the tests</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>804659204</td>\n",
              "      <td>towards smart educational recommendations with reinforcement learning in classroom</td>\n",
              "      <td>Su Liu</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SENSOR</td>\n",
              "      <td>PULSE,AFFECT,GAZE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>TALE</td>\n",
              "      <td>International Conference on Teaching, Assessment and Learning for Engineering</td>\n",
              "      <td>48</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>Determined that an RL activity recommendataiton system would assit (via simulated data) teachers in select the optimal learning activity to optimize collective learning gains in a classroom.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>804659204</td>\n",
              "      <td>towards smart educational recommendations with reinforcement learning in classroom</td>\n",
              "      <td>Su Liu</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SENSOR</td>\n",
              "      <td>PULSE,AFFECT,GAZE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>TALE</td>\n",
              "      <td>International Conference on Teaching, Assessment and Learning for Engineering</td>\n",
              "      <td>48</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1581261659</td>\n",
              "      <td>early prediction of visitor engagement in science museums with multimodal learning analytics</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,LOGS</td>\n",
              "      <td>POSE,GEST,AFFECT,GAZE,LOGS</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICMI</td>\n",
              "      <td>International Conference on Multimodal Interaction</td>\n",
              "      <td>49</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>The evaluation revealed that predictive models of visitor dwell time can make improved predictions over time, and using additional modalities yields better performance as visitors near the end of their interactions. Random forest models outperformed competing models on three of the four modality combinations, and Lasso regression performed best on the unimodal configuration for predicting dwell time. Notably, overall predictive performance improves when removing eye gaze and interaction log modalities. However, removing facial expression results in a steep drop in performance.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1581261659</td>\n",
              "      <td>early prediction of visitor engagement in science museums with multimodal learning analytics</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,LOGS</td>\n",
              "      <td>POSE,GEST,AFFECT,GAZE,LOGS</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICMI</td>\n",
              "      <td>International Conference on Multimodal Interaction</td>\n",
              "      <td>49</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Same as Joyce (first paragraph in conclusion)</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1581261659</td>\n",
              "      <td>early prediction of visitor engagement in science museums with multimodal learning analytics</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,LOGS</td>\n",
              "      <td>POSE,GEST,AFFECT,GAZE,LOGS</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICMI</td>\n",
              "      <td>International Conference on Multimodal Interaction</td>\n",
              "      <td>49</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>2836996318</td>\n",
              "      <td>predicting learners' emotions in mobile mooc learning via a multimodal intelligent tutor</td>\n",
              "      <td>Phuong Pham</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO</td>\n",
              "      <td>AFFECT,PULSE</td>\n",
              "      <td>REG</td>\n",
              "      <td>LATE</td>\n",
              "      <td>ITS</td>\n",
              "      <td>International Conference on Intelligent Tutoring Systems</td>\n",
              "      <td>50</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>The tutor can detect 6 emotions in mobile MOOC learning reliably with high accuracy; it can also predict learning outcomes; work rpoves it is feasible to track both PPG signals and facial expressions in real time in a scalable manner on today’s unmodified smartphones.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>2836996318</td>\n",
              "      <td>predicting learners' emotions in mobile mooc learning via a multimodal intelligent tutor</td>\n",
              "      <td>Phuong Pham</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO</td>\n",
              "      <td>AFFECT,PULSE</td>\n",
              "      <td>REG</td>\n",
              "      <td>LATE</td>\n",
              "      <td>ITS</td>\n",
              "      <td>International Conference on Intelligent Tutoring Systems</td>\n",
              "      <td>50</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>The study shows the feasibility of capturing rich and fine-grained physiological signals such as PPG signals and facial expressions in mobile learning contexts without introducing any addi‐\\ntional hardware. Experimental results show that PPG signals and facial expressions collected by AttentiveLearner2 in real time are complementary and can serve as fine-grained, rich signals to understand learners’ emotions. By capturing the temporal dynamics of both feature channels, AttentiveLearner2 can achieve higher performance by combining both PPG features and FEA features. Our approach is complementary to today’s existing technique such as clickstream analysis and is promising towards enabling personalized interventions for mobile MOOC learning.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>2836996318</td>\n",
              "      <td>predicting learners' emotions in mobile mooc learning via a multimodal intelligent tutor</td>\n",
              "      <td>Phuong Pham</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO</td>\n",
              "      <td>AFFECT,PULSE</td>\n",
              "      <td>REG</td>\n",
              "      <td>LATE</td>\n",
              "      <td>ITS</td>\n",
              "      <td>International Conference on Intelligent Tutoring Systems</td>\n",
              "      <td>50</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>32184286</td>\n",
              "      <td>once more with feeling: emotions in multimodal learning analytics</td>\n",
              "      <td>Marcus Kubsch</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,PPA,AUDIO</td>\n",
              "      <td>INTER,SURVEY,TRANS,PROS,AFFECT</td>\n",
              "      <td>CLS,REG,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MMLA Handbook</td>\n",
              "      <td>The Multimodal Learning Analytics Handbook</td>\n",
              "      <td>51</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Results demonstrate how NLP and ML techniques allow us to use different modalities of the same data, voice and transcript, and different modalities of different data sources, voice data from interviews, answers to a goal orientation questionnaire, and answers to open-ended questions about energy, in order to better understand individual differences in students’ performances</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>32184286</td>\n",
              "      <td>once more with feeling: emotions in multimodal learning analytics</td>\n",
              "      <td>Marcus Kubsch</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,PPA,AUDIO</td>\n",
              "      <td>INTER,SURVEY,TRANS,PROS,AFFECT</td>\n",
              "      <td>CLS,REG,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MMLA Handbook</td>\n",
              "      <td>The Multimodal Learning Analytics Handbook</td>\n",
              "      <td>51</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>Used text and audio to predict student's affect. With the affect, the authors' explored its statistical relation to student's knowledge. Results point that they need more data to improve performance in affect prediction but promising direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>32184286</td>\n",
              "      <td>once more with feeling: emotions in multimodal learning analytics</td>\n",
              "      <td>Marcus Kubsch</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,PPA,AUDIO</td>\n",
              "      <td>INTER,SURVEY,TRANS,PROS,AFFECT</td>\n",
              "      <td>CLS,REG,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MMLA Handbook</td>\n",
              "      <td>The Multimodal Learning Analytics Handbook</td>\n",
              "      <td>51</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>433919853</td>\n",
              "      <td>understanding fun in learning to code: a multi-modal data approach</td>\n",
              "      <td>Gabriella Tisza</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SENSOR,VIDEO,PPA</td>\n",
              "      <td>TEMP,PULSE,EDA,BP,AFFECT</td>\n",
              "      <td>REG,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>IDC</td>\n",
              "      <td>Interaction Design and Children Conference</td>\n",
              "      <td>52</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Sadness, anger and stress are negatively, and arousal is positively related to students’ relative learning gains; experienced fun is positively related to students’ RLG</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>433919853</td>\n",
              "      <td>understanding fun in learning to code: a multi-modal data approach</td>\n",
              "      <td>Gabriella Tisza</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SENSOR,VIDEO,PPA</td>\n",
              "      <td>TEMP,PULSE,EDA,BP,AFFECT</td>\n",
              "      <td>REG,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>IDC</td>\n",
              "      <td>Interaction Design and Children Conference</td>\n",
              "      <td>52</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>Our findings support endeavors of educators, designers, and re-\\nsearchers to make learning to code a fun experience, as we found a positive relationship between those. Further research studies could aim to improve the applicability of physiological measure devices (e.g. wristbands) for children.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>433919853</td>\n",
              "      <td>understanding fun in learning to code: a multi-modal data approach</td>\n",
              "      <td>Gabriella Tisza</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SENSOR,VIDEO,PPA</td>\n",
              "      <td>TEMP,PULSE,EDA,BP,AFFECT</td>\n",
              "      <td>REG,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>IDC</td>\n",
              "      <td>Interaction Design and Children Conference</td>\n",
              "      <td>52</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>483140962</td>\n",
              "      <td>investigating multimodal affect sensing in an affective tutoring system using unobtrusive sensors</td>\n",
              "      <td>Hua Leong Fwa</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS</td>\n",
              "      <td>POSE,ACT,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>PPIG</td>\n",
              "      <td>Workshop Psychology of Programming Interest Group</td>\n",
              "      <td>53</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>A multimodal approach offers higher accuracy and better robustness as compared to a unimodal approach (multimodal fusion leads to higher detection accuracy over unimodal model). In addition, the inclusion of keystrokes and mouse clicks makes up for the detection gap where video based sensing modes (facial and head postures) are not available.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>483140962</td>\n",
              "      <td>investigating multimodal affect sensing in an affective tutoring system using unobtrusive sensors</td>\n",
              "      <td>Hua Leong Fwa</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS</td>\n",
              "      <td>POSE,ACT,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>PPIG</td>\n",
              "      <td>Workshop Psychology of Programming Interest Group</td>\n",
              "      <td>53</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>The main goal of this study is to xplore automated techniques for the detection of frustration in a naturalistic learning environment. With adequate detection of frustration on a moment by moment\\nbasis, hints and tutorial supports can be provided to the students to overcome learning barriers and\\nalleviate their frustration so as to sustain their engagement in learning.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>483140962</td>\n",
              "      <td>investigating multimodal affect sensing in an affective tutoring system using unobtrusive sensors</td>\n",
              "      <td>Hua Leong Fwa</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS</td>\n",
              "      <td>POSE,ACT,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>PPIG</td>\n",
              "      <td>Workshop Psychology of Programming Interest Group</td>\n",
              "      <td>53</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>3308658121</td>\n",
              "      <td>exploring collaboration using motion sensors and multi-modal learning analytics</td>\n",
              "      <td>Joseph M. Reilly</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>PPA,RPA,POSE,GEST</td>\n",
              "      <td>CLUST,PATT,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>EDM</td>\n",
              "      <td>International Conference on Educational Data Mining</td>\n",
              "      <td>54</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>Significant correlations found between average movement of points along the upper right side of participants’ bodies with outcome measures indicates the importance of gesturing and physical movement when communicating ideas.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>3308658121</td>\n",
              "      <td>exploring collaboration using motion sensors and multi-modal learning analytics</td>\n",
              "      <td>Joseph M. Reilly</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>PPA,RPA,POSE,GEST</td>\n",
              "      <td>CLUST,PATT,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>EDM</td>\n",
              "      <td>International Conference on Educational Data Mining</td>\n",
              "      <td>54</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>We plan to further identify productive micro-behaviors from the Kinect data to gain additional insights in the ways that dyads synchronized their actions. Future work with regards to prototypical postures would also explore both participants in a dyad at once, clustering on both joint angles simultaneously. This may reveal combinations of postures that are informative and could extend our exploration of physical synchrony within dyads.\\nThe differences between dyads in different conditions will also be a main focus of analysis moving forward.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>3308658121</td>\n",
              "      <td>exploring collaboration using motion sensors and multi-modal learning analytics</td>\n",
              "      <td>Joseph M. Reilly</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>PPA,RPA,POSE,GEST</td>\n",
              "      <td>CLUST,PATT,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>EDM</td>\n",
              "      <td>International Conference on Educational Data Mining</td>\n",
              "      <td>54</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>1847468084</td>\n",
              "      <td>computationally augmented ethnography: emotion tracking and learning in museum games</td>\n",
              "      <td>Kit Martin</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA,RPA</td>\n",
              "      <td>TRANS,AFFECT,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ICQE</td>\n",
              "      <td>International Conference on Quantitative Ethnography</td>\n",
              "      <td>55</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>Authors present an initial pilot using these methods in concert to identify key moments in multiple modalities. While the use of constructivist dialogue mapping showed that the users learned during their interaction with Ant Adaption, emotional logging identified alternative moments of learning outside of their analytic framework.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>1847468084</td>\n",
              "      <td>computationally augmented ethnography: emotion tracking and learning in museum games</td>\n",
              "      <td>Kit Martin</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA,RPA</td>\n",
              "      <td>TRANS,AFFECT,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ICQE</td>\n",
              "      <td>International Conference on Quantitative Ethnography</td>\n",
              "      <td>55</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MF</td>\n",
              "      <td>This paper presented a preliminary approach to augment qualitative analysis of an\\ninformal learning environment. Using techniques from multimodal learning analytics,\\nwe were able to expand our analysis of learning while participants interacted with a\\nmultitouch environment. Our methodological approach required us to extract emotions\\nfrom the low-level logs of facial action units using FACET and then revisit video\\ncorresponding to particular FACET values to identify moments of high emotional stimulation theoretically implicated in learning.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>1847468084</td>\n",
              "      <td>computationally augmented ethnography: emotion tracking and learning in museum games</td>\n",
              "      <td>Kit Martin</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA,RPA</td>\n",
              "      <td>TRANS,AFFECT,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ICQE</td>\n",
              "      <td>International Conference on Quantitative Ethnography</td>\n",
              "      <td>55</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>2345021698</td>\n",
              "      <td>exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course</td>\n",
              "      <td>René Noël</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,PPA,RPA</td>\n",
              "      <td>RPA,PROS</td>\n",
              "      <td>QUAL,NET,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>Access</td>\n",
              "      <td>IEEE Access</td>\n",
              "      <td>56</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>``There was no difference between the proportion of the session spent being bored or frustrated or in the achievement scores for the two conditions. In order to determine whether the effect on achievement was different depending on length of exposure, participants with a duration less than 60 minutes were excluded from the analysis. However, there was still no significant difference between achievement scores from intervention and control sessions for the group as a whole or for each subgroup.``</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>1</td>\n",
              "      <td>model-free for correlation study, mode-based for collaborative and non-collaborative classification</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>2345021698</td>\n",
              "      <td>exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course</td>\n",
              "      <td>René Noël</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,PPA,RPA</td>\n",
              "      <td>RPA,PROS</td>\n",
              "      <td>QUAL,NET,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>Access</td>\n",
              "      <td>IEEE Access</td>\n",
              "      <td>56</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>Main findings of the case study are the relationships between prior experience in software requirements and the way the team members collaborate, and the lower productivity of low experienced groups. No evidence was found that performance of domain experts was superior from non-experts during collaborative problem-solving sessions. Although it was stated that low experience subjects produced more user stories, a greater productivity of top experience subjects was not statistically verified.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>2345021698</td>\n",
              "      <td>exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course</td>\n",
              "      <td>René Noël</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,PPA,RPA</td>\n",
              "      <td>RPA,PROS</td>\n",
              "      <td>QUAL,NET,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>Access</td>\n",
              "      <td>IEEE Access</td>\n",
              "      <td>56</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eduardo/Joyce</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>3796643912</td>\n",
              "      <td>an evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities</td>\n",
              "      <td>Penelope J. Standen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,RPA</td>\n",
              "      <td>AFFECT,POSE,LOGS,RPA,GAZE,PROS,GEST</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>57</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>This is the first study to evaluate an adaptive learning system for learners with ID based on multi-modal affect recognition. Three separate states were automatically identified, with lower levels of the state labelled “boredom” having the strongest link to learning achievement. Both those labelled “frustration” and “engagement” were positively related to achievement. Our results are in line with other studies showing that engagement increases when activities are tailored to the personal needs\\nand emotional states of learners (Athanasiadis et al., 2017), but no significant difference in learn-ing achievement was found (at least for the period of our study) when adaption was based on both the affective state and achievement of the learner, compared with achievement alone.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>1</td>\n",
              "      <td>statistical model -&gt; model-free</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>3796643912</td>\n",
              "      <td>an evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities</td>\n",
              "      <td>Penelope J. Standen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,RPA</td>\n",
              "      <td>AFFECT,POSE,LOGS,RPA,GAZE,PROS,GEST</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>57</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>HUM, OTH, STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Results suggest that engagement does increase when activities are tailored to the personal needs and emotional state of the learner and that the system was promoting affective states that in turn promote learning. However, longer exposure is necessary to determine the effect on learning.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>3796643912</td>\n",
              "      <td>an evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities</td>\n",
              "      <td>Penelope J. Standen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,RPA</td>\n",
              "      <td>AFFECT,POSE,LOGS,RPA,GAZE,PROS,GEST</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>57</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>HUM, OTH, STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eduardo/Joyce</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>205660768</td>\n",
              "      <td>multimodal learning analytics to investigate cognitive load during online problem solving</td>\n",
              "      <td>Charlotte Larmuseau</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>PPA,SENSOR</td>\n",
              "      <td>PPA,PULSE,EDA</td>\n",
              "      <td>STATS,CLS</td>\n",
              "      <td>EARLY</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>58</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>OTH</td>\n",
              "      <td>IND</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>Against our expectations, results revealed that physiological data could not be used to detect differences in CL based on intrinsic and extraneous manipulations. By contrast, most of the significant results are related to OSPAN and the baseline measurement. Based on our findings related to OSPAN, we might be able to conclude that HR, HRV and ST is more sensitive to high CL, namely, exceeding the learner’s cognitive capacity and the related mental states (ie, stress). In this respect, as high CL can also provoke stress, it is not always clear what exactly is measured via physiological data.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>1</td>\n",
              "      <td>Relating to cognitive load theory as their model</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>205660768</td>\n",
              "      <td>multimodal learning analytics to investigate cognitive load during online problem solving</td>\n",
              "      <td>Charlotte Larmuseau</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>PPA,SENSOR</td>\n",
              "      <td>PPA,PULSE,EDA</td>\n",
              "      <td>STATS,CLS</td>\n",
              "      <td>EARLY</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>58</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>This study manipulated intrinsic and extraneous load to investigate how physiological features, namely, GSR, ST and HR(V) vary as a result of changes in CL. Results revealed no significant differences between the manipulated conditions in terms of physiological data. Nonetheless, HR and ST were significantly related to self-reported CL, whereas ST to task performance. Additionally, this study revealed the potential of ST and HR to assess high CL.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>205660768</td>\n",
              "      <td>multimodal learning analytics to investigate cognitive load during online problem solving</td>\n",
              "      <td>Charlotte Larmuseau</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>PPA,SENSOR</td>\n",
              "      <td>PPA,PULSE,EDA</td>\n",
              "      <td>STATS,CLS</td>\n",
              "      <td>EARLY</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>58</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eduardo/Joyce</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>2181637610</td>\n",
              "      <td>toward using multi-modal learning analytics to support and measure collaboration in co-located dyads</td>\n",
              "      <td>Emma L. Starr</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA,RPA,LOGS</td>\n",
              "      <td>POSE,PROS,PPA,RPA,LOGS</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ICLS</td>\n",
              "      <td>International Conference of the Learning Sciences</td>\n",
              "      <td>59</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>While this study was not able to show a clear effect of providing a real-time visualization to support\\ncollaboration, it made many other valuable contributions. First, it showed that simple verbal interventions can\\nhelp participants pay attention to particular aspects of their collaborative behavior (i.e., how much they are\\ntalking and how much space they are providing to their partner). Second, it suggested that awareness tools such\\nas the one developed for this study have to be designed differently to impact social interactions (e.g., by being\\nmore salient or be used in a setting where users have the mental bandwidth to reflect on their collaborative\\nstyle). Third, we collected a rich multi-modal dataset that can be used to build proxies for measuring effective\\ncollaborations.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>1</td>\n",
              "      <td>Relating their research to Roschelle's 1992 framework of convergent conceptual change for collaboration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>2181637610</td>\n",
              "      <td>toward using multi-modal learning analytics to support and measure collaboration in co-located dyads</td>\n",
              "      <td>Emma L. Starr</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA,RPA,LOGS</td>\n",
              "      <td>POSE,PROS,PPA,RPA,LOGS</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ICLS</td>\n",
              "      <td>International Conference of the Learning Sciences</td>\n",
              "      <td>59</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>The purpose of this paper was to explore the effect of two collaboration interventions and the relationship between collaboration quality, task performance and learning gains, however this study was not able to show a clear effect of providing a real-time visualization to support collaboration. It did show that simple verbal interventions can help participants pay attention to particular aspects of their collaborative behavior, and suggested that awareness tools such as the one developed for this study have to be designed differently to impact social interactions. Authors built a rich multi-modal dataset that can be used to build proxies for measuring effective collaborations. As a preliminary analysis, they found that various indicators captured by the Kinect sensor were correlated with participants’ quality of collaboration.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>2181637610</td>\n",
              "      <td>toward using multi-modal learning analytics to support and measure collaboration in co-located dyads</td>\n",
              "      <td>Emma L. Starr</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA,RPA,LOGS</td>\n",
              "      <td>POSE,PROS,PPA,RPA,LOGS</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ICLS</td>\n",
              "      <td>International Conference of the Learning Sciences</td>\n",
              "      <td>59</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eduardo/Joyce</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>4035649049</td>\n",
              "      <td>storytelling with learner data: guiding student reflection on multimodal team data</td>\n",
              "      <td>Gloria Fernández-Nieto</td>\n",
              "      <td>2021</td>\n",
              "      <td>Training</td>\n",
              "      <td>SENSOR,LOGS,RPA</td>\n",
              "      <td>EDA,LOGS,RPA,AFFECT</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>TLT</td>\n",
              "      <td>Transactions on Learning Technologies</td>\n",
              "      <td>60</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>This article presented two qualitative studies conducted in authentic nursing simulation classrooms with the purpose of communicating insights to students through data stories. Given the limitations of current visual analytics, we anticipate that approaches such as DS will grow in importance to help students make the most\\nof the new forms of feedback that are becoming possible.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>1</td>\n",
              "      <td>Data stories</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>4035649049</td>\n",
              "      <td>storytelling with learner data: guiding student reflection on multimodal team data</td>\n",
              "      <td>Gloria Fernández-Nieto</td>\n",
              "      <td>2021</td>\n",
              "      <td>Training</td>\n",
              "      <td>SENSOR,LOGS,RPA</td>\n",
              "      <td>EDA,LOGS,RPA,AFFECT</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>TLT</td>\n",
              "      <td>Transactions on Learning Technologies</td>\n",
              "      <td>60</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR, TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>This article results show that the enhancements using DS principles helped students identify misconceptions, think about strategies to address errors they made, and reflect on the arousal levels they may have experienced during the simulations. Although the studies presented in this article were conducted in the context of complex, multimodal learning situations, there is no reason why a storytelling approach could not be implemented to aid in the interpretation of more conventional LA visualizations supporting noncollocated teamwork.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>4035649049</td>\n",
              "      <td>storytelling with learner data: guiding student reflection on multimodal team data</td>\n",
              "      <td>Gloria Fernández-Nieto</td>\n",
              "      <td>2021</td>\n",
              "      <td>Training</td>\n",
              "      <td>SENSOR,LOGS,RPA</td>\n",
              "      <td>EDA,LOGS,RPA,AFFECT</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>TLT</td>\n",
              "      <td>Transactions on Learning Technologies</td>\n",
              "      <td>60</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eduardo/Joyce</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>1019093033</td>\n",
              "      <td>prime: block-wise missingness handling for multi-modalities in intelligent tutoring systems</td>\n",
              "      <td>Xi Yang</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>PPA,VIDEO,EYE</td>\n",
              "      <td>LOGS,AFFECT,GAZE</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>MMM</td>\n",
              "      <td>International Conference on Multimedia Modeling</td>\n",
              "      <td>61</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>In this work, we proposed a data imputation method called PRIME for blockwise missingness handling in multimodal data and measured its effectiveness in a student modeling task to predict students’ learning gain in an ITS. Through experiments, we demonstrated that: (1) the multimodal data is more effective than the single-modal data; (2) compared to competitive baseline missing data handling methods, the PRIME can not only improve the prediction performance, but also achieve more accurate reconstruction results.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>1</td>\n",
              "      <td>Data imputation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>1019093033</td>\n",
              "      <td>prime: block-wise missingness handling for multi-modalities in intelligent tutoring systems</td>\n",
              "      <td>Xi Yang</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>PPA,VIDEO,EYE</td>\n",
              "      <td>LOGS,AFFECT,GAZE</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>MMM</td>\n",
              "      <td>International Conference on Multimedia Modeling</td>\n",
              "      <td>61</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>Results show that using multimodal data as a result of missing data handling yields better prediction performance than using logfiles only, and PRIME outperforms other baseline methods for both learning gain prediction and data reconstruction tasks</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>1019093033</td>\n",
              "      <td>prime: block-wise missingness handling for multi-modalities in intelligent tutoring systems</td>\n",
              "      <td>Xi Yang</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>PPA,VIDEO,EYE</td>\n",
              "      <td>LOGS,AFFECT,GAZE</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>MMM</td>\n",
              "      <td>International Conference on Multimedia Modeling</td>\n",
              "      <td>61</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eduardo/Joyce</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>2936220551</td>\n",
              "      <td>multi-source and multimodal data fusion for predicting academic performance in blended learning university courses</td>\n",
              "      <td>Wilson Chango</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,PPA</td>\n",
              "      <td>LOGS,POSE,RPA,ACT</td>\n",
              "      <td>CLS,QUAL,STATS</td>\n",
              "      <td>MID,LATE</td>\n",
              "      <td>CEE</td>\n",
              "      <td>Elsevier Computers and Electrical Engineering</td>\n",
              "      <td>62</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>Which data fusion approach and classification algorithms produce the best results from our data? The use of ensembles and selecting the best attributes approach from discretized summary data produced our highest/best results in Accuracy and AUC values. The REPTree classification algorithm obtained the highest/best results in this approach from discretized summary data. \\n\\n• How useful are the prediction models we produce to help teachers detect students who are at risk of drop out or fail the course? The white-box models we produced give teachers very understandable explanations (IF-THEN rules) of how they classified the students’ final performance or classification. They showed that the attributes that appear most in these rules were attention in theory classes,\\nscores in Moodle quizzes, and the level of activity in the Moodle forum.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>1</td>\n",
              "      <td>Predicting student drop-out from LMS data</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>2936220551</td>\n",
              "      <td>multi-source and multimodal data fusion for predicting academic performance in blended learning university courses</td>\n",
              "      <td>Wilson Chango</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,PPA</td>\n",
              "      <td>LOGS,POSE,RPA,ACT</td>\n",
              "      <td>CLS,QUAL,STATS</td>\n",
              "      <td>MID,LATE</td>\n",
              "      <td>CEE</td>\n",
              "      <td>Elsevier Computers and Electrical Engineering</td>\n",
              "      <td>62</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>The results show that the best predictions are produced using ensembles and selecting the best attributes approach with discretized data. The best prediction models show that the level of attention in theory classes, scores in Moodle quizzes, and the level of activity in Moodle forums are the best set of attributes for predicting students’ final performance in their courses.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>2936220551</td>\n",
              "      <td>multi-source and multimodal data fusion for predicting academic performance in blended learning university courses</td>\n",
              "      <td>Wilson Chango</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,PPA</td>\n",
              "      <td>LOGS,POSE,RPA,ACT</td>\n",
              "      <td>CLS,QUAL,STATS</td>\n",
              "      <td>MID,LATE</td>\n",
              "      <td>CEE</td>\n",
              "      <td>Elsevier Computers and Electrical Engineering</td>\n",
              "      <td>62</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eduardo/Joyce</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>1886134458</td>\n",
              "      <td>personalizing computer science education by leveraging multimodal learning analytics</td>\n",
              "      <td>David Azcona</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,PPA</td>\n",
              "      <td>LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>FIE</td>\n",
              "      <td>Frontiers in Education Conference</td>\n",
              "      <td>63</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>Overall, feedback was very positive and responses can be found in Table IX. Most students would recommend this system to students attending the same course next year or would like to see this system included in other courses as shown in questions 5 and 6 respectively. In terms of the last question to improve the system, students who were doing well or very well, were getting an increasingly similar response each week and were demanding a more personalised notification and some other additional learning resources.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>1886134458</td>\n",
              "      <td>personalizing computer science education by leveraging multimodal learning analytics</td>\n",
              "      <td>David Azcona</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,PPA</td>\n",
              "      <td>LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>FIE</td>\n",
              "      <td>Frontiers in Education Conference</td>\n",
              "      <td>63</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>Predictive models built using student characteristics, prior academic history, logged interactions between students and online resources, and students’ progress in programming laboratory work were used to give weekly predictions to students. Predictions worked relatively well with one year of training data for the three courses. Authors noted that CS2 and SH1’s models were based on 2015/16’s previous student data and PF3’s was based on PF2’s student data from the first semester of the academic year, thus they did not expect it to work as well as the other models as the courseware was not the same.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>1886134458</td>\n",
              "      <td>personalizing computer science education by leveraging multimodal learning analytics</td>\n",
              "      <td>David Azcona</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,PPA</td>\n",
              "      <td>LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>FIE</td>\n",
              "      <td>Frontiers in Education Conference</td>\n",
              "      <td>63</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eduardo/Joyce</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>2879332689</td>\n",
              "      <td>from data to insights: a layered storytelling approach for multimodal learning analytics</td>\n",
              "      <td>Roberto Martinez-Maldonado</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>LOGS,MOTION,SENSOR,RPA,VIDEO</td>\n",
              "      <td>POSE,EDA,LOGS,RPA</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>CHI</td>\n",
              "      <td>Conference on Human Factors in Computing Systems</td>\n",
              "      <td>64</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>This paper documents how we have wrestled with the challenge of designing activity-based feedback visualisations which draw the attention of non-technical users to key insights in the data. We argue, supported by user studies, that this work advances the state of the art in making multimodal data streams intelligible to non-data experts. The approach should enable similar collocated activities to benefit from these novel collaboration analytics.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>2879332689</td>\n",
              "      <td>from data to insights: a layered storytelling approach for multimodal learning analytics</td>\n",
              "      <td>Roberto Martinez-Maldonado</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>LOGS,MOTION,SENSOR,RPA,VIDEO</td>\n",
              "      <td>POSE,EDA,LOGS,RPA</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>CHI</td>\n",
              "      <td>Conference on Human Factors in Computing Systems</td>\n",
              "      <td>64</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR, TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>This paper documents how authors tackled the challenge of designing activity-based feedback visualisations which draw the attention of non-technical users to key insights in the data. We argue, supported by user studies, that this work advances the state of the art in making multimodal data streams intelligible to non-data experts. The approach should enable similar collocated activities to benefit from these novel collaboration analytics.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>2879332689</td>\n",
              "      <td>from data to insights: a layered storytelling approach for multimodal learning analytics</td>\n",
              "      <td>Roberto Martinez-Maldonado</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>LOGS,MOTION,SENSOR,RPA,VIDEO</td>\n",
              "      <td>POSE,EDA,LOGS,RPA</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>CHI</td>\n",
              "      <td>Conference on Human Factors in Computing Systems</td>\n",
              "      <td>64</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eduardo/Joyce</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>3146393211</td>\n",
              "      <td>mobile mixed reality for experiential learning and simulation in medical and health sciences education</td>\n",
              "      <td>James Birt</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>PPA,INTER</td>\n",
              "      <td>PPA,TRANS</td>\n",
              "      <td>QUAL,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>Information</td>\n",
              "      <td>MDPI Information</td>\n",
              "      <td>65</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>Finally, it was clear from the two intervention studies that the simulation provided by mobile devices, although not as powerful as the desktop-computer-powered Oculus Rift mixed reality or physical simulators, has no negative impacts on the learners. In fact, the simulation improves learners’ competence and skills with the skills intervention showing statistically significant improvements in the learners that received the mobile mixed reality simulation tools prior to residential school.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>3146393211</td>\n",
              "      <td>mobile mixed reality for experiential learning and simulation in medical and health sciences education</td>\n",
              "      <td>James Birt</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>PPA,INTER</td>\n",
              "      <td>PPA,TRANS</td>\n",
              "      <td>QUAL,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>Information</td>\n",
              "      <td>MDPI Information</td>\n",
              "      <td>65</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>This study validates the use of mobile devices in university undergraduate health sciences curricula, and shows that not only are these modes (game engines, free AR/VR SDKs and mobile-based devices with GPU-enabled processors and high-quality screens) useful for enhancing the development of physical skills in students, but they are also received favorably.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>3146393211</td>\n",
              "      <td>mobile mixed reality for experiential learning and simulation in medical and health sciences education</td>\n",
              "      <td>James Birt</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>PPA,INTER</td>\n",
              "      <td>PPA,TRANS</td>\n",
              "      <td>QUAL,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>Information</td>\n",
              "      <td>MDPI Information</td>\n",
              "      <td>65</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eduardo/Joyce</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>3809293172</td>\n",
              "      <td>blending learning analytics and embodied design to model students' comprehension of measurement using their actions, speech, and gestures</td>\n",
              "      <td>Avery H. Closser</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>RPA</td>\n",
              "      <td>CLUST,REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>IJCCI</td>\n",
              "      <td>International Journal of Child-Computer Interaction</td>\n",
              "      <td>66</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>In this paper, we applied clustering, natural language processing, and general linear modeling to a small yet rich dataset detailing student behaviors and speech during measurement tasks to identify successful measurement strategies. Our findings revealed profiles of student behavior and speech that may indicate different levels of conceptual knowledge as well as evidence that spatial and kinetographic gestures predict performance on mea-\\nsurement tasks.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>3809293172</td>\n",
              "      <td>blending learning analytics and embodied design to model students' comprehension of measurement using their actions, speech, and gestures</td>\n",
              "      <td>Avery H. Closser</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>RPA</td>\n",
              "      <td>CLUST,REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>IJCCI</td>\n",
              "      <td>International Journal of Child-Computer Interaction</td>\n",
              "      <td>66</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>Authors explored students’ conceptual understanding of measurement to indentify measurement estimation strategies that should be emphasized in classroom instruction. By applying machine-learning methods to a small, multimodal dataset from a study on student behavior in mathematics, we identified behavioral profiles, patterns in speech, and specific actions and gestures that are predictive of performance.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>3809293172</td>\n",
              "      <td>blending learning analytics and embodied design to model students' comprehension of measurement using their actions, speech, and gestures</td>\n",
              "      <td>Avery H. Closser</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>RPA</td>\n",
              "      <td>CLUST,REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>IJCCI</td>\n",
              "      <td>International Journal of Child-Computer Interaction</td>\n",
              "      <td>66</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eduardo/Joyce</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>4019205162</td>\n",
              "      <td>introducing low-cost sensors into the classroom settings: improving the assessment in agile practices with multimodal learning analytics</td>\n",
              "      <td>Hector Cornide-Reyes</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,SURVEY,PPA,RPA</td>\n",
              "      <td>SURVEY,TRANS,PPA,RPA</td>\n",
              "      <td>NET,STATS</td>\n",
              "      <td>MID,OTH</td>\n",
              "      <td>Sensors</td>\n",
              "      <td>MDPI Sensors</td>\n",
              "      <td>67</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>RQ1: Better communication, better collaboration\\nRQ2: Collaborative teams showed lower variability in the estimates of story points (same page)\\nRQ3: Democratic leadership in collaborative groups</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>4019205162</td>\n",
              "      <td>introducing low-cost sensors into the classroom settings: improving the assessment in agile practices with multimodal learning analytics</td>\n",
              "      <td>Hector Cornide-Reyes</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,SURVEY,PPA,RPA</td>\n",
              "      <td>SURVEY,TRANS,PPA,RPA</td>\n",
              "      <td>NET,STATS</td>\n",
              "      <td>MID,OTH</td>\n",
              "      <td>Sensors</td>\n",
              "      <td>MDPI Sensors</td>\n",
              "      <td>67</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>The results obtained allowed the detection of important relationships and characteristics of the collaborative and Non-Collaborative groups, with productivity, effort, and predominant personality styles in the groups. Authors conclude that the Multimodal Learning Analysis techniques offer considerable feasibilities to support the process of skills development in students.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>4019205162</td>\n",
              "      <td>introducing low-cost sensors into the classroom settings: improving the assessment in agile practices with multimodal learning analytics</td>\n",
              "      <td>Hector Cornide-Reyes</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,SURVEY,PPA,RPA</td>\n",
              "      <td>SURVEY,TRANS,PPA,RPA</td>\n",
              "      <td>NET,STATS</td>\n",
              "      <td>MID,OTH</td>\n",
              "      <td>Sensors</td>\n",
              "      <td>MDPI Sensors</td>\n",
              "      <td>67</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eduardo/Joyce</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>4277812050</td>\n",
              "      <td>improving prediction of students' performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources</td>\n",
              "      <td>Wilson Chango</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,VIDEO,EYE,PPA</td>\n",
              "      <td>AFFECT,LOGS,GAZE,PPA</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID,LATE</td>\n",
              "      <td>JCHE</td>\n",
              "      <td>Journal of Computing in Higher Education</td>\n",
              "      <td>68</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>The implications of the current study point to Web ITS and Web-based Adaptive Educational Systems. If data is captured from diferent data sources, the classifer ensemble methodology proposed in this study could make better, earlier performance predictions than the single data source models that are commonly used at present.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>4277812050</td>\n",
              "      <td>improving prediction of students' performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources</td>\n",
              "      <td>Wilson Chango</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,VIDEO,EYE,PPA</td>\n",
              "      <td>AFFECT,LOGS,GAZE,PPA</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID,LATE</td>\n",
              "      <td>JCHE</td>\n",
              "      <td>Journal of Computing in Higher Education</td>\n",
              "      <td>68</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>Authors tested whether prediction of learning performance could be improved by using attribute selection and classification ensembles. By carrying out three experiments and applying six classifcation algorithms to numerical and discretized preprocessed multimodal data, results show that the best predictions were produced using ensembles and selecting the best attributes approach with numerical data.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>4277812050</td>\n",
              "      <td>improving prediction of students' performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources</td>\n",
              "      <td>Wilson Chango</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,VIDEO,EYE,PPA</td>\n",
              "      <td>AFFECT,LOGS,GAZE,PPA</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID,LATE</td>\n",
              "      <td>JCHE</td>\n",
              "      <td>Journal of Computing in Higher Education</td>\n",
              "      <td>68</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eduardo/Joyce</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>1609706685</td>\n",
              "      <td>learning pulse: a machine learning approach for predicting performance in self-regulated learning using multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>SENSOR,LOGS,MOTION,PPA</td>\n",
              "      <td>PULSE,ACT,AFFECT</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>69</td>\n",
              "      <td>BLND</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>This paper described Learning Pulse, an exploratory study whose aim was to use predictive modelling to generate timely predictions about learners’ performance during self-regulated learning by collecting multimodal data about their body, activity and context. Although the prediction accuracy with the data sources and experimental setup chosen in Learning Pulse led to modest results, all the research questions have been answered positively and have lead towards new insights on the storing, modelling and processing multimodal data.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>1609706685</td>\n",
              "      <td>learning pulse: a machine learning approach for predicting performance in self-regulated learning using multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>SENSOR,LOGS,MOTION,PPA</td>\n",
              "      <td>PULSE,ACT,AFFECT</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>69</td>\n",
              "      <td>BLND</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>This paper described an exploratory study whose aim was to use predictive modelling to generate timely predictions about learners’ performance during self-regulated learning by collecting multimodal data about their body, activity and context. The limited significance of the prediction results did not allow authors to assert that accurate and learner-specific predictions can be generated, however all the research questions have been answered positively and have lead towards new insights on the storing, modelling and processing multimodal data.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>1609706685</td>\n",
              "      <td>learning pulse: a machine learning approach for predicting performance in self-regulated learning using multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>SENSOR,LOGS,MOTION,PPA</td>\n",
              "      <td>PULSE,ACT,AFFECT</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>69</td>\n",
              "      <td>BLND</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eduardo/Joyce</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>3398902089</td>\n",
              "      <td>what multimodal data can tell us about the students’ regulation of their learning process?</td>\n",
              "      <td>Sanna Järvelä</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SENSOR,VIDEO,AUDIO</td>\n",
              "      <td>EDA,AFFECT,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>LAI</td>\n",
              "      <td>Elsevier Learning and Instruction</td>\n",
              "      <td>70</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>Our major claim is that multichannel data can be potential for understanding regulatory processes in collaboration. With our five empirical case examples, we illustrate how triangulating multiple sources of data has potential to advance the theoretical and conceptual progress in social aspects of SRL theory.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>3398902089</td>\n",
              "      <td>what multimodal data can tell us about the students’ regulation of their learning process?</td>\n",
              "      <td>Sanna Järvelä</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SENSOR,VIDEO,AUDIO</td>\n",
              "      <td>EDA,AFFECT,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>LAI</td>\n",
              "      <td>Elsevier Learning and Instruction</td>\n",
              "      <td>70</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Authors show with five empirical cases that multichannel data can be potential for understanding regulatory processes in collaboration, illustrating how triangulating multiple sources of data has potential to advance the theoretical and conceptual progress in social aspects of SRL theory: (1) understanding how interactions between different facets of regulation, such as cognition, motivation and emotion interact with cognitive strategic action by using video and EDA data; (2) visualizing how physiological synchrony measured from the heart rate can reveal or backup the interpretation of socially shared regulation of learning or co-regulation of learning located from the video; (3) visualizing temporality and cyclical processes (i.e., planning, enacting strategies, reflecting, adapting) of regulation by using video, EDA and facial expression recognition data; (5)) illustrating how combining not only physiological measures, but also facial expression data can lead even more accurate interpretations of the situations where regulation of learning is needed.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>3398902089</td>\n",
              "      <td>what multimodal data can tell us about the students’ regulation of their learning process?</td>\n",
              "      <td>Sanna Järvelä</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SENSOR,VIDEO,AUDIO</td>\n",
              "      <td>EDA,AFFECT,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>LAI</td>\n",
              "      <td>Elsevier Learning and Instruction</td>\n",
              "      <td>70</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eduardo/Joyce</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>3093310941</td>\n",
              "      <td>embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders</td>\n",
              "      <td>Hiroki Tanaka</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>AUDIO,VIDEO,PPA</td>\n",
              "      <td>POSE,PROS,AFFECT</td>\n",
              "      <td>REG,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>PLOS</td>\n",
              "      <td>PLOS ONE</td>\n",
              "      <td>71</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>We identified a positive correlation between overall narrative skills and other non-verbal behaviors. This result indicates that the overall narrative skills were subjectively decided based on other non-verbal behavior skills [22]. Although the automated social skills trainer did not provide feedback regarding narrative structure or eye gaze, these elements were also improved after the training.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>3093310941</td>\n",
              "      <td>embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders</td>\n",
              "      <td>Hiroki Tanaka</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>AUDIO,VIDEO,PPA</td>\n",
              "      <td>POSE,PROS,AFFECT</td>\n",
              "      <td>REG,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>PLOS</td>\n",
              "      <td>PLOS ONE</td>\n",
              "      <td>71</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>The focus of this study assessed the effectiveness of an automated social skills trainer with multimodal information that adheres to the basic human-based SST as closely as possible. Authors extended a previous method for automatic social skills training by adding audiovisual information regarding smiling ratio and head pose that improved the training effect. \\nMultimodal feedback is also useful for both members of the general population with social difficulties and people with ASD because it helps such people understand and improve their narrative skills, as was previously reported in human-based SST [2, 3].</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>3093310941</td>\n",
              "      <td>embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders</td>\n",
              "      <td>Hiroki Tanaka</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>AUDIO,VIDEO,PPA</td>\n",
              "      <td>POSE,PROS,AFFECT</td>\n",
              "      <td>REG,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>PLOS</td>\n",
              "      <td>PLOS ONE</td>\n",
              "      <td>71</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eduardo/Joyce</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>1576545447</td>\n",
              "      <td>artificial intelligence and multimodal data in the service of human decision-making: a case study in debate tutoring</td>\n",
              "      <td>Mutlu Cukurova</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,SURVEY</td>\n",
              "      <td>AFFECT,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>72</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MF</td>\n",
              "      <td>In this study, we used non-transparent prediction models of openSMILE to predict the emotional traits of tutor candidates based on their audio data, and we used transparent logistic regression models to identify exactly what personality, emotion, and experience traits lead to effective debate tutoring skills. Predictive models were very powerful to make sense of complex and nonlinear audio data, whereas the transparent regression models were valuable to identify key aspects for tutors to reflect upon their own decisions and provide tutor candidates with feedback on their performance.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>1576545447</td>\n",
              "      <td>artificial intelligence and multimodal data in the service of human decision-making: a case study in debate tutoring</td>\n",
              "      <td>Mutlu Cukurova</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,SURVEY</td>\n",
              "      <td>AFFECT,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>72</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>Authors combined predictive and transparent models to support the human decision-making processes involved in tutor trainee evaluations and results showed that models with multimodal data can accurately classify tutors and have the potential to support the intuitive decision-making of expert tutors in the context of evaluating trainee applicants.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>1576545447</td>\n",
              "      <td>artificial intelligence and multimodal data in the service of human decision-making: a case study in debate tutoring</td>\n",
              "      <td>Mutlu Cukurova</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,SURVEY</td>\n",
              "      <td>AFFECT,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>72</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eduardo/Joyce</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>3796180663</td>\n",
              "      <td>learning linkages: integrating data streams of multiple modalities and timescales</td>\n",
              "      <td>Ran Liu</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,SCREEN,PPA</td>\n",
              "      <td>TRANS,QUAL,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>Journal of Computer Assisted Learning</td>\n",
              "      <td>73</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>We presented two empirical studies, collected in classroom studies with two distinct learning technology systems in different contexts (individual and collaborative). Our analyses and findings showcase a few different ways, in which multimodal data sources can enrich our understanding of student learning and provide a more holistic picture.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>3796180663</td>\n",
              "      <td>learning linkages: integrating data streams of multiple modalities and timescales</td>\n",
              "      <td>Ran Liu</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,SCREEN,PPA</td>\n",
              "      <td>TRANS,QUAL,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>Journal of Computer Assisted Learning</td>\n",
              "      <td>73</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>Authors collected student‐focused screen and webcam video which were useful for understanding students' learning processes and approaches based on detailed analyses of their interactions with the tutor interface, mouse movements, and out‐of‐tutor (in person) help‐seeking. High‐fidelity audio of students' collaborative dialogue was collected to generate high‐quality transcriptions of students' dialogue and apply an NLP approach to make use of the large quantity of audio dialogue. The verbal data allowed authors to identify linguistic features in students' collaborative dialogue that were highly predictive of math performance on pretest and posttest assessments, above and beyond any nonlinguistic variables.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>3796180663</td>\n",
              "      <td>learning linkages: integrating data streams of multiple modalities and timescales</td>\n",
              "      <td>Ran Liu</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,SCREEN,PPA</td>\n",
              "      <td>TRANS,QUAL,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>Journal of Computer Assisted Learning</td>\n",
              "      <td>73</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eduardo/Joyce</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NLP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>1770989706</td>\n",
              "      <td>focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving</td>\n",
              "      <td>Hana Vrzakova</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,SCREEN,SURVEY,PPA</td>\n",
              "      <td>PROS,ACT,GEST,PPA</td>\n",
              "      <td>STATS,PATT</td>\n",
              "      <td>MID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>1</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>We found that idling with limited speech (i.e., silence or backchannel feedback only) and without movement was negatively correlated with task performance and with participants’ subjective perceptions of the collaboration. However, being silent and focused during solution execution was positively correlated with task performance. Results illustrate that in some cases, multimodal patterns improved the predictions and improved explanatory power over the unimodal primitives.</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>1770989706</td>\n",
              "      <td>focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving</td>\n",
              "      <td>Hana Vrzakova</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,SCREEN,SURVEY,PPA</td>\n",
              "      <td>PROS,ACT,GEST,PPA</td>\n",
              "      <td>STATS,PATT</td>\n",
              "      <td>MID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>1</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"Here, we examine unimodal primitives (activity on the screen, speech, and body movements), and their multimodal combinations during remote CPS.\"\\n\\nMixed findings for uni- versus multi-modal:\\n\\n\"These results lead us to question: are the multimodal patterns better than the unimodal primitives? As illustrated above, we found evidence for both sides of the argument. In the case of code execution, the answer is no, but it is a yes in the case of idling. However, it is important to go beyond the significant correlations as there is an informative signal in the non-significant ones as well. For example, consider idling once again. By itself, this pattern is negatively correlated with the task score (r = -.21) and the correlation is even more negative when idling is accompanied by silence/back channeling and little movement (r = -.35). However, there are many other configurations where idling is weak or negligible predictor of task score. For example, idling occurring in the context of the contributors speaking with some movement is more weakly correlated with task score (r = -.11) and the correlation is essentially null when idling is accompanied with the controller speaking and some movement (r = -.06). Thus, even when they do not improve predictive power, multimodal patterns help contextualize and reveal nuances in the unimodal primitives. This supports the overall idea of multimodal learning analytics in which the additional modalities (speech and body movement in our case) help to understand unclear patterns such as idling. This finding is interesting from two perspectives.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>2</td>\n",
              "      <td>CPS study, Minecraft playing via Blockly. Model-free because only stats methods/pattern extraction and no AI/ML model or formal theoretical model. Compared unimodal to multimodal, and made arguments for both given the context.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>1770989706</td>\n",
              "      <td>focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving</td>\n",
              "      <td>Hana Vrzakova</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,SCREEN,SURVEY,PPA</td>\n",
              "      <td>PROS,ACT,GEST,PPA</td>\n",
              "      <td>STATS,PATT</td>\n",
              "      <td>MID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>1</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caleb/Clayton</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>2456887548</td>\n",
              "      <td>an unobtrusive and multimodal approach for behavioral engagement detection of students</td>\n",
              "      <td>Nese Alyuz</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,VIDEO,SCREEN,PPA</td>\n",
              "      <td>AFFECT,POSE,LOGS,PPA</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>MIE</td>\n",
              "      <td>International Workshop on Multimodal Interaction for Education</td>\n",
              "      <td>2</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>The experiments on authentically collected student dataset showed that it is beneficial to have separate classification pipelines for different learning sections of Instructional and Assessment. For the Instructional section, using only Appearance modality yields an F1-measure of 0.74, compared to fused performance of 0.70. For Assessment, fusing all three modalities (F1-measure of 0.89) provide a prominent improvement over the best performing unimodality (0.81 for Appearance).</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>2456887548</td>\n",
              "      <td>an unobtrusive and multimodal approach for behavioral engagement detection of students</td>\n",
              "      <td>Nese Alyuz</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,VIDEO,SCREEN,PPA</td>\n",
              "      <td>AFFECT,POSE,LOGS,PPA</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>MIE</td>\n",
              "      <td>International Workshop on Multimodal Interaction for Education</td>\n",
              "      <td>2</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Mixed findings for uni- versus multi-modal:\\n\\n\"The experiments on authentically collected student dataset showed that it is beneficial to have separate classification pipelines for different learning sections of Instructional and Assessment. For the Instructional section, using only Appearance modality yields an F1-measure of 0.74, compared to fused performance of 0.70. For Assessment, fusing all three modalities (F1-measure of 0.89) provide a prominent improvement over the best performing unimodality (0.81 for Appearance). Interestingly, although Context-Performance modality performs poorly for the Off-Task class when considered alone, it helps to eliminate false positives (especially for the Off-Task class) when incorporated into the other modalities. In summary, we can say that for Instructional section types, Appearance modality provides acceptable results; whereas for Assessment sections, all available information should be fused to achieve best performance.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>2</td>\n",
              "      <td>Math virtual environment with 9th graders for engagement detection. Random Forest model.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>2456887548</td>\n",
              "      <td>an unobtrusive and multimodal approach for behavioral engagement detection of students</td>\n",
              "      <td>Nese Alyuz</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,VIDEO,SCREEN,PPA</td>\n",
              "      <td>AFFECT,POSE,LOGS,PPA</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>MIE</td>\n",
              "      <td>International Workshop on Multimodal Interaction for Education</td>\n",
              "      <td>2</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caleb/Clayton</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>518268671</td>\n",
              "      <td>using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game</td>\n",
              "      <td>María Ximena López</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,LOGS,AUDIO,VIDEO</td>\n",
              "      <td>LOGS,SURVEY,GAZE,PROS</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ECGBL</td>\n",
              "      <td>European Conference on Games Based Learning</td>\n",
              "      <td>8</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>Overall, our study suggests that coupling style can be operationalised as a multimodal indicator suitable to investigate complex collaboration dynamics in games, thus confirming what has been found in other domains</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>518268671</td>\n",
              "      <td>using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game</td>\n",
              "      <td>María Ximena López</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,LOGS,AUDIO,VIDEO</td>\n",
              "      <td>LOGS,SURVEY,GAZE,PROS</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ECGBL</td>\n",
              "      <td>European Conference on Games Based Learning</td>\n",
              "      <td>8</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"Firstly, we found that coupling styles can be used to characterise collaboration in a co-located SG. Consistently with previous research (Isenberg et al, 2010; Niu et al, 2018), our results showed that teams displayed both close and loose coupling styles while performing individual actions to accomplish shared goals. Interestingly, we found a positive association between the time spent working closely coupled and the individual interactions with the technology. This suggests that the pursuit of collective goals requires players to continuously alternate and integrate individual planning and action with closely-coupled, likely to verify and synchronise their own actions with others. Secondly, we found that the perceived quality of collaboration does not appear to be an effective indicator of collaboration quality by itself. However, its small association with close-coupling style suggests a conscious, continuous, and proactive approach to collaboration, since players who appreciate the value of collaboration also seem to actively engage in closely-coupled interactions with others. Thirdly, our findings suggest that better-performing teams do work more closely-coupled and alternate their interactions with individual work. This result indicates that freely alternating individual work with closely-coupled interaction is an effective collaboration strategy, and that collaborative SGs should afford this opportunity. Overall, our study suggests that coupling style can be operationalised as a multimodal indicator suitable to investigate complex collaboration dynamics in games, thus confirming what has been found in other domains (Isenberg et al, 2010; Niu et al, 2018).\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>2</td>\n",
              "      <td>Blended game environment for sustainable development (collaborative). Only stats methods applied, and no formal model presented or addressed via RQs.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>518268671</td>\n",
              "      <td>using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game</td>\n",
              "      <td>María Ximena López</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,LOGS,AUDIO,VIDEO</td>\n",
              "      <td>LOGS,SURVEY,GAZE,PROS</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ECGBL</td>\n",
              "      <td>European Conference on Games Based Learning</td>\n",
              "      <td>8</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caleb/Clayton</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>957160695</td>\n",
              "      <td>virtual debate coach design: assessing multimodal argumentation performance</td>\n",
              "      <td>Volha Petukhova</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>GEST,TRANS,PROS,SURVEY,GAZE</td>\n",
              "      <td>STATS,CLS,QUAL</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICMI</td>\n",
              "      <td>International Conference on Multimodal Interaction</td>\n",
              "      <td>9</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain corpora and classification procedures resulted in the best performance on an argument structure mining task.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>957160695</td>\n",
              "      <td>virtual debate coach design: assessing multimodal argumentation performance</td>\n",
              "      <td>Volha Petukhova</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>GEST,TRANS,PROS,SURVEY,GAZE</td>\n",
              "      <td>STATS,CLS,QUAL</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICMI</td>\n",
              "      <td>International Conference on Multimodal Interaction</td>\n",
              "      <td>9</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain corpora and classification procedures resulted in the best performance on an argument structure mining task. Results of the argument quality experiments showed that argument com- prehensibility is affected by the number of referring expressions, information complexity, and presentation fluency. Presence of intensification and segmentation markers, position and movements of hands/ams and certain postures may affect the perception of the clarity, persuasiveness, and credibility of debaters.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>2</td>\n",
              "      <td>Real-time and post hoc vitual debate coach. SVM model for classification. Could make argument for blended environment with virtual coach, but I would consider the environment to be in person, as that is where the students interact and debate.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>957160695</td>\n",
              "      <td>virtual debate coach design: assessing multimodal argumentation performance</td>\n",
              "      <td>Volha Petukhova</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>GEST,TRANS,PROS,SURVEY,GAZE</td>\n",
              "      <td>STATS,CLS,QUAL</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICMI</td>\n",
              "      <td>International Conference on Multimodal Interaction</td>\n",
              "      <td>9</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caleb/Clayton</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>3009548670</td>\n",
              "      <td>real-time multimodal feedback with the cpr tutor</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>LOGS,VIDEO,SENSOR,MOTION</td>\n",
              "      <td>POSE,EMG,GEST</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>AIED</td>\n",
              "      <td>International Conference on Artificial Intelligence in Education</td>\n",
              "      <td>10</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MB</td>\n",
              "      <td>System architecture is functional in predicting novice and expert compressions with low error rate</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>3009548670</td>\n",
              "      <td>real-time multimodal feedback with the cpr tutor</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>LOGS,VIDEO,SENSOR,MOTION</td>\n",
              "      <td>POSE,EMG,GEST</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>AIED</td>\n",
              "      <td>International Conference on Artificial Intelligence in Education</td>\n",
              "      <td>10</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"...we collected observations that, while cannot be generalised, provide some indication that the feedback of the CPR tutor had a positive influence on the CPR performance on the target classes. To sum up, the architecture used for the CPR Tutor allowed for provision of real-time multimodal feedback (H1) and the generated feedback seem to have a short-term positive influence on the CPR performance on the target classes considered.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>2</td>\n",
              "      <td>CPR tutor. Model-based via RNN (LSTM) classification. Environment is \"European University Hospital.\" No mention of undergraduats, and I got the impression this was professional development given the students needed to have recurring CPR qualifications.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>3009548670</td>\n",
              "      <td>real-time multimodal feedback with the cpr tutor</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>LOGS,VIDEO,SENSOR,MOTION</td>\n",
              "      <td>POSE,EMG,GEST</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>AIED</td>\n",
              "      <td>International Conference on Artificial Intelligence in Education</td>\n",
              "      <td>10</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caleb/Clayton</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>3448122334</td>\n",
              "      <td>investigating the impact of a real-time, multimodal student engagement analytics technology in authentic classrooms</td>\n",
              "      <td>Sinem Aslan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,SCREEN,AUDIO,SURVEY,RPA,PPA,INTER</td>\n",
              "      <td>AFFECT,LOGS,POSE,QUAL,INTER,SURVEY,RPA</td>\n",
              "      <td>QUAL,STATS,CLS</td>\n",
              "      <td>LATE</td>\n",
              "      <td>CHI</td>\n",
              "      <td>Conference on Human Factors in Computing Systems</td>\n",
              "      <td>16</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Significant impact on the teacher's scaffolding behavior and student engagement (less bordem)</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>3448122334</td>\n",
              "      <td>investigating the impact of a real-time, multimodal student engagement analytics technology in authentic classrooms</td>\n",
              "      <td>Sinem Aslan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,SCREEN,AUDIO,SURVEY,RPA,PPA,INTER</td>\n",
              "      <td>AFFECT,LOGS,POSE,QUAL,INTER,SURVEY,RPA</td>\n",
              "      <td>QUAL,STATS,CLS</td>\n",
              "      <td>LATE</td>\n",
              "      <td>CHI</td>\n",
              "      <td>Conference on Human Factors in Computing Systems</td>\n",
              "      <td>16</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>SEAT had positive impact on student engagement and was also helpful to teachers.</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>2</td>\n",
              "      <td>Real-time, multimodal Student Engagement Analytics Technology. Model-based via ML models (RF) referenced in previous works.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>3448122334</td>\n",
              "      <td>investigating the impact of a real-time, multimodal student engagement analytics technology in authentic classrooms</td>\n",
              "      <td>Sinem Aslan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,SCREEN,AUDIO,SURVEY,RPA,PPA,INTER</td>\n",
              "      <td>AFFECT,LOGS,POSE,QUAL,INTER,SURVEY,RPA</td>\n",
              "      <td>QUAL,STATS,CLS</td>\n",
              "      <td>LATE</td>\n",
              "      <td>CHI</td>\n",
              "      <td>Conference on Human Factors in Computing Systems</td>\n",
              "      <td>16</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caleb/Clayton</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>2497456347</td>\n",
              "      <td>the rap system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2018</td>\n",
              "      <td>Training</td>\n",
              "      <td>AUDIO,VIDEO,PPA,SURVEY</td>\n",
              "      <td>PPA,GAZE,POSE,PROS</td>\n",
              "      <td>CLS,STATS,QUAL</td>\n",
              "      <td>LATE</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>17</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>Feedback generated by RAP is similar to human feedback across several dimensions and similar to other more complex systems</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>2497456347</td>\n",
              "      <td>the rap system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2018</td>\n",
              "      <td>Training</td>\n",
              "      <td>AUDIO,VIDEO,PPA,SURVEY</td>\n",
              "      <td>PPA,GAZE,POSE,PROS</td>\n",
              "      <td>CLS,STATS,QUAL</td>\n",
              "      <td>LATE</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>17</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"It revealed an overwhelmingly positive perception of the system especially in the dimensions of perceived usefulness and feedback which were rated as excellent by 65% and 58% of the students respectively. The qualitative analysis helped discover specific issues, on the positive side, students commented on the potential of the system to quickly learn some basic presentation skills: \"I would like to see this system used in our Communications class\". On the negative side, students commented that they sometimes were aware that they were being recorded and that the environment was too small. Also, some students felt uncomfortable with a pre-recorded audience because it didn’t seem to react to their presentation: \"the audience had always the same expressions\". Overall, the students agreed that the system was useful and that they learned about their own presentation skills while using it.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>2</td>\n",
              "      <td>RAP system evaluation. Blended because interactions with screen crowd and presentation but IRL presenting. Model-based due to AI/ML methods like RF.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>2497456347</td>\n",
              "      <td>the rap system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2018</td>\n",
              "      <td>Training</td>\n",
              "      <td>AUDIO,VIDEO,PPA,SURVEY</td>\n",
              "      <td>PPA,GAZE,POSE,PROS</td>\n",
              "      <td>CLS,STATS,QUAL</td>\n",
              "      <td>LATE</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>17</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caleb/Clayton</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>3660066725</td>\n",
              "      <td>children's play and problem solving in motion-based educational games: synergies between human annotations and multi-modal data</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SENSOR,EYE,LOGS</td>\n",
              "      <td>ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>IDC</td>\n",
              "      <td>Interaction Design and Children Conference</td>\n",
              "      <td>18</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>The use of MMD can help to triangulate with traditional research methods and explore hidden cognitive states</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>3660066725</td>\n",
              "      <td>children's play and problem solving in motion-based educational games: synergies between human annotations and multi-modal data</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SENSOR,EYE,LOGS</td>\n",
              "      <td>ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>IDC</td>\n",
              "      <td>Interaction Design and Children Conference</td>\n",
              "      <td>18</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"During informed problem solving episodes, we observed that children’s physiological stress, cognitive load, and emotional regu- lation were the highest. When children are presented a problem to solve, they may feel under pressure (external or self-imposed [60]) to answer the question correctly.\"\\n\\n\"During our study, children interacted with a MBEG; however, despite the intended “fun factor” that typically accompanies games, the pressure to academically perform (i.e., correctly match a card-box pair) may have elevated children’s stress levels [38, 87]. This may explain why children’s stress levels peaked during episodes of informed problem solving. In a similar vein, increased levels of cognitive load during informed problem solving may be directly linked to the mental effort that children expended as they reasoned through problems [88].\"\\n\\n\"Lastly, emotional regulation relates to children’s HRV [9, 97]. A plausible reason for observing the highest levels of emotional regulation during informed problem solving might be due to the immediate feedback that children received directly after they attempted to make a card-box match. The anticipation of the MBEG assessment/evaluation may have influenced children’s heart rate, causing high levels of variability as children invested themselves in informed problem solving. Thus, in accordance with prior research [32], we hypothesise that the feedback in general, may have triggered cognitive and affective responses which affected learning, particularly during this ongoing tasks (i.e., a collection of questions asked in series).\"\\n\\n\"Contrary to previous research [69, 84], our results did not indicate a connection between guessing behaviour and children’s lack of engagement during their interactions with the MBEG (Figure 6, top right). As such, we propose that during episodes of guessing, children experienced some degree of external and/or self-imposed pressures to determine answers correctly (as during informed problem solving).\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>2</td>\n",
              "      <td>Marvy learns. Embodied...child must move physicall to get on-screen \"monster\" Marvy to place shapes in the right buckets. No AI/ML model (stats via ANOVA). Lots of references to \"groundwork\" that directs research, but no formal model.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>3660066725</td>\n",
              "      <td>children's play and problem solving in motion-based educational games: synergies between human annotations and multi-modal data</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SENSOR,EYE,LOGS</td>\n",
              "      <td>ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>IDC</td>\n",
              "      <td>Interaction Design and Children Conference</td>\n",
              "      <td>18</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caleb/Clayton</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>3783339081</td>\n",
              "      <td>a novel method for the in-depth multimodal analysis of student learning trajectories in intelligent tutoring systems</td>\n",
              "      <td>Ran Liu</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,SCREEN,PPA</td>\n",
              "      <td>LOGS,TRANS,ACT,QUAL,PPA</td>\n",
              "      <td>STATS,REG,QUAL</td>\n",
              "      <td>MID</td>\n",
              "      <td>JLA</td>\n",
              "      <td>Journal of Learning Analytics</td>\n",
              "      <td>19</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>Corse-grained learning trajectories were able to identify key moments during learning that warrented futher analysis with other MMD</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>3783339081</td>\n",
              "      <td>a novel method for the in-depth multimodal analysis of student learning trajectories in intelligent tutoring systems</td>\n",
              "      <td>Ran Liu</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,SCREEN,PPA</td>\n",
              "      <td>LOGS,TRANS,ACT,QUAL,PPA</td>\n",
              "      <td>STATS,REG,QUAL</td>\n",
              "      <td>MID</td>\n",
              "      <td>JLA</td>\n",
              "      <td>Journal of Learning Analytics</td>\n",
              "      <td>19</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"Our results from this analysis showed how students’ early experiences struggling with a novel concept could significantly affect both their entire learning trajectories within an activity and pre-test–post-test measurements of learning gains related to that concept. It provided evidence of an important “moment” for early instructional intervention.\\n\\nThe analysis we conducted on the Concentration knowledge component was an example of how a knowledge component-centred analysis can benefit from this multi-step approach as well. Our results led to a modification in the knowledge component assignment to problem steps within a ChemVLab+ activity as well as instructional implications for promoting better learning of a previously hidden conceptual difficulty.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>2</td>\n",
              "      <td>Chemistry Virtual Lab. ChemVLab+ tutor. Model-based via regressing post-test scores and calculating probability individual student will get problem correct.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>3783339081</td>\n",
              "      <td>a novel method for the in-depth multimodal analysis of student learning trajectories in intelligent tutoring systems</td>\n",
              "      <td>Ran Liu</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,SCREEN,PPA</td>\n",
              "      <td>LOGS,TRANS,ACT,QUAL,PPA</td>\n",
              "      <td>STATS,REG,QUAL</td>\n",
              "      <td>MID</td>\n",
              "      <td>JLA</td>\n",
              "      <td>Journal of Learning Analytics</td>\n",
              "      <td>19</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caleb/Clayton</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>3095923626</td>\n",
              "      <td>a multimodal analysis of making</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR,PPA,INTER</td>\n",
              "      <td>GEST,PPA,EDA,ACT,PROS,QUAL,INTER</td>\n",
              "      <td>STATS,CLUST,QUAL,PATT</td>\n",
              "      <td>EARLY</td>\n",
              "      <td>IJAIED</td>\n",
              "      <td>International Journal of Artificial Intelligence in Education</td>\n",
              "      <td>20</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"each approach provides different affordances depending on the similarity metric and the dependent variable.\"\\n \"The take-away is that there are several strategies for doing multimodal learning analytics, and that many of these approaches can provide a meaningful glimpse into a complex data set, glimpses that may be difficult to identify using traditional approaches.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>3095923626</td>\n",
              "      <td>a multimodal analysis of making</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR,PPA,INTER</td>\n",
              "      <td>GEST,PPA,EDA,ACT,PROS,QUAL,INTER</td>\n",
              "      <td>STATS,CLUST,QUAL,PATT</td>\n",
              "      <td>EARLY</td>\n",
              "      <td>IJAIED</td>\n",
              "      <td>International Journal of Artificial Intelligence in Education</td>\n",
              "      <td>20</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"Looking across analyses, there are clear instances where each provided some novel insights. In this sense, the overall algorithm appears to have relevance for studying learning, success and experimental condition; but honing in on these correlations requires different modes of analysis.\\n\\nAs a whole this article has shown that success, learning and process are not equivalent, though they may occasionally overlap. Thus, when thinking about measuring the effectiveness of a given learning environment it is important to be clear about which metrics one hopes to optimize. At the same time, this article has provided additional evidence that experimental condition can have an impact on learning, success and process. Because of this, one has to be cognizant about how to develop learning and reasoning approaches that allow the environment to realize the desired outcomes.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>2</td>\n",
              "      <td>Dyads working with materials like paper on engineering task. High school and undergraduate students. No AI/ML supervised model, no formal theorhetical model. Does clustering count as model? YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>3095923626</td>\n",
              "      <td>a multimodal analysis of making</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR,PPA,INTER</td>\n",
              "      <td>GEST,PPA,EDA,ACT,PROS,QUAL,INTER</td>\n",
              "      <td>STATS,CLUST,QUAL,PATT</td>\n",
              "      <td>EARLY</td>\n",
              "      <td>IJAIED</td>\n",
              "      <td>International Journal of Artificial Intelligence in Education</td>\n",
              "      <td>20</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caleb/Clayton</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>85990093</td>\n",
              "      <td>multimodal markers of persuasive speech : designing a virtual debate coach</td>\n",
              "      <td>Volha Petukhova</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>PROS,GEST</td>\n",
              "      <td>CLS,QUAL,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>INTERSPEECH</td>\n",
              "      <td>INTERSPEECH Conference</td>\n",
              "      <td>21</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"Experiments of different types supported fairly reliable identification of markers from multimodal data, and linking these to assessments of debater confidence level and intensification behaviour.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>85990093</td>\n",
              "      <td>multimodal markers of persuasive speech : designing a virtual debate coach</td>\n",
              "      <td>Volha Petukhova</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>PROS,GEST</td>\n",
              "      <td>CLS,QUAL,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>INTERSPEECH</td>\n",
              "      <td>INTERSPEECH Conference</td>\n",
              "      <td>21</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"In line with previous empirical findings, we acknowledge that persuasive speech is rather difficult to characterize. Neverthe- less, based on theoretical and empirical frameworks set up by Grice (1975), Gussenhoven (2002) and Hirschberg (2002), we were able to define a set of criteria which help us to explain observed regularities and define rules, strategies and constraints for the generation, assessment and correction of trainees’ debate performance. Experiments of different types supported fairly reliable identification of markers from multimodal data, and linking these to assessments of debater confidence level and intensification behaviour.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>2</td>\n",
              "      <td>Designing a virtual debate coach. Model-based via SVM classification and also three different theorhetical frameworks.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>85990093</td>\n",
              "      <td>multimodal markers of persuasive speech : designing a virtual debate coach</td>\n",
              "      <td>Volha Petukhova</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>PROS,GEST</td>\n",
              "      <td>CLS,QUAL,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>INTERSPEECH</td>\n",
              "      <td>INTERSPEECH Conference</td>\n",
              "      <td>21</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caleb/Clayton</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>86191824</td>\n",
              "      <td>examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes</td>\n",
              "      <td>Shiyan Jiang</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SCREEN,INTER,PPA,AUDIO</td>\n",
              "      <td>INTER,QUAL,TRANS</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ILE</td>\n",
              "      <td>Interactive Learning Environments</td>\n",
              "      <td>22</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"This study contributes an initial understanding into how different modalities mediate students’ interactions and offers implications for scaffolding peer interactions during multimodal composing processes.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>86191824</td>\n",
              "      <td>examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes</td>\n",
              "      <td>Shiyan Jiang</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SCREEN,INTER,PPA,AUDIO</td>\n",
              "      <td>INTER,QUAL,TRANS</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ILE</td>\n",
              "      <td>Interactive Learning Environments</td>\n",
              "      <td>22</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"Overall, students were more likely to share ideas and ask questions, and they tended to use quick- response strategies while multimodal composing.\"\\n\\n\"Students’ interview responses also suggested that providing short responses was a typical strategy during multimodal composing.\"\\n\\n\"When examining interactions across sessions, the group was more engaged in discussions at the beginning and the end of the project while fewer interactions occurred during the middle of their composing process.\"\\n\\n\"Giving commands occurred much less frequently than other interaction types (Figure 3).\"\\n\\n\"Students discussed more often about comics that combined visuals and text than other modal elements.\"\\n\\n\"Making learning visible in different modes was critical to foster peer interaction (Jahnke, Norqvist, &amp;\\nOlsson, 2013).\"\\n\\n\"Results showed that there were interactional differences based on different modes.\"\\n\\n\"While comparing discussions on static visual modes, namely images and multimodal comics, we found that images involved more self-oriented and less group-oriented contributions.\"\\n\\n\"Discussions on animations included more elaborated feedback.\"\\n\\n\"Written narrative provided the least opportunity for group-oriented contributions.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>2</td>\n",
              "      <td>Focus on how different modes influence student interactions over\\ntime during science multimodal composing. Provides model via theoretical framework.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>86191824</td>\n",
              "      <td>examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes</td>\n",
              "      <td>Shiyan Jiang</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SCREEN,INTER,PPA,AUDIO</td>\n",
              "      <td>INTER,QUAL,TRANS</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ILE</td>\n",
              "      <td>Interactive Learning Environments</td>\n",
              "      <td>22</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caleb/Clayton</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>818492192</td>\n",
              "      <td>understanding student learning trajectories using multimodal learning analytics within an embodied-interaction learning environment</td>\n",
              "      <td>Alejandro Andrade</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,INTER,PPA</td>\n",
              "      <td>GAZE,LOGS,INTER,PPA,GEST</td>\n",
              "      <td>CLUST,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>23</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"Using MMLA techniques, we were able to spot differences in students’ motion sequences while students interacted with our embodied simulation.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>818492192</td>\n",
              "      <td>understanding student learning trajectories using multimodal learning analytics within an embodied-interaction learning environment</td>\n",
              "      <td>Alejandro Andrade</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,INTER,PPA</td>\n",
              "      <td>GAZE,LOGS,INTER,PPA,GEST</td>\n",
              "      <td>CLUST,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>23</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"Using MMLA techniques, we were able to spot differences in students’ motion sequences while students interacted with our embodied simulation.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>2</td>\n",
              "      <td>Embodied predator-prey ecosystem environment. No AI/ML or formal theoretical model. Does HMM count as model? YES\\n\\n\"The aim of this paper is to show how multimodal learning analytics (MMLA) can help understand how elementary students explore the concept of feedback loops while controlling an embodied simulation of a predator-prey ecosystem using hand movements as an interface with the computer simulation.\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>818492192</td>\n",
              "      <td>understanding student learning trajectories using multimodal learning analytics within an embodied-interaction learning environment</td>\n",
              "      <td>Alejandro Andrade</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,INTER,PPA</td>\n",
              "      <td>GAZE,LOGS,INTER,PPA,GEST</td>\n",
              "      <td>CLUST,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>23</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caleb/Clayton</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>147203129</td>\n",
              "      <td>multimodal learning analytics to inform learning design: lessons learned from computing education</td>\n",
              "      <td>Katerina Mangaroska</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR,LOGS</td>\n",
              "      <td>LOGS,GAZE,EDA,PULSE,AFFECT,TEMP</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JLA</td>\n",
              "      <td>Journal of Learning Analytics</td>\n",
              "      <td>24</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"The findings emphasize the need for learning analytics that are consequential for learning, rather than easy and convenient to collect.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>147203129</td>\n",
              "      <td>multimodal learning analytics to inform learning design: lessons learned from computing education</td>\n",
              "      <td>Katerina Mangaroska</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR,LOGS</td>\n",
              "      <td>LOGS,GAZE,EDA,PULSE,AFFECT,TEMP</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JLA</td>\n",
              "      <td>Journal of Learning Analytics</td>\n",
              "      <td>24</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"The models from M2 to M8 significantly outperformed M1 (see Figure 3), with M1 exhibiting a significantly lower adjusted\\nR2 value of 0.42 (std.dev. = 0.116).\"\\n\\ni.e., multimodal much better than logs alone for predicting debugging performance. Best model had every single modality.\\n\\n\"Using machine learning, we looked at the overall patterns in the data and performed feature importance among the 72 measures extracted from the multimodal data. Findings like ours combined with pedagogical intent from educators and theories from the LS, can advance the synergy between LA and LD by translating results in applicable design guidelines that can lead to improvements in the design of learning activities, instructional methods for teaching particular skills, and even the overall course (re)design. The complexity of the MMLA approach is congruous with learning theories because it can be used to understand how effectively students use the opportunities for learning as given in the LD. Such understanding promises to support versatile improvements in the LD in digital environments, from setting the right feedback loop (e.g., explaining misconceptions vs. challenging the student), to the design of personalized interventions, and modelling effective learning strategies considering skills and knowledge proficiency.\"\\n\\n\"The findings emphasize the need for learning analytics that are consequential for learning, rather than easy and convenient to collect.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>2</td>\n",
              "      <td>Java debugging env looking at just logs as a baseline then compared to multimodal. RF model-based.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>147203129</td>\n",
              "      <td>multimodal learning analytics to inform learning design: lessons learned from computing education</td>\n",
              "      <td>Katerina Mangaroska</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR,LOGS</td>\n",
              "      <td>LOGS,GAZE,EDA,PULSE,AFFECT,TEMP</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JLA</td>\n",
              "      <td>Journal of Learning Analytics</td>\n",
              "      <td>24</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caleb/Clayton</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>123412197</td>\n",
              "      <td>utilizing multimodal data through fsqca to explain engagement in adaptive learning</td>\n",
              "      <td>Zacharoula Papamitsiou</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,LOGS,EYE,SENSOR,VIDEO</td>\n",
              "      <td>PULSE,AFFECT,EEG,GAZE,LOGS,BP,TEMP,EDA,SURVEY</td>\n",
              "      <td>PATT</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>TLT</td>\n",
              "      <td>Transactions on Learning Technologies</td>\n",
              "      <td>25</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>The analysis revealed six configurations that explain learners’ high performance and three that explain learners’ medium/low performance, driven by engagement measures coming from the multimodal data.</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>123412197</td>\n",
              "      <td>utilizing multimodal data through fsqca to explain engagement in adaptive learning</td>\n",
              "      <td>Zacharoula Papamitsiou</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,LOGS,EYE,SENSOR,VIDEO</td>\n",
              "      <td>PULSE,AFFECT,EEG,GAZE,LOGS,BP,TEMP,EDA,SURVEY</td>\n",
              "      <td>PATT</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>TLT</td>\n",
              "      <td>Transactions on Learning Technologies</td>\n",
              "      <td>25</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"This study demonstrated a consolidated analysis of multimodal data collected during an adaptive self-assessment activity, utilizing fsQCA for deeper understanding engagement in this setting. What this study adds to engagement literature is that when the learning tasks facilitate one’s own learning needs (motivation), it is likely that one will be deeper and more substantially involved with those tasks, yet the thorough analysis showcased that multimodal data can provide more than one engagement patterns to facilitate this objective.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>2</td>\n",
              "      <td>Fuzzy set qualitative comparative analysis (fsQCA) approach to shed light to learners’ engagement patterns. Model free because no formal theoretical model or AI/ML model.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>123412197</td>\n",
              "      <td>utilizing multimodal data through fsqca to explain engagement in adaptive learning</td>\n",
              "      <td>Zacharoula Papamitsiou</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,LOGS,EYE,SENSOR,VIDEO</td>\n",
              "      <td>PULSE,AFFECT,EEG,GAZE,LOGS,BP,TEMP,EDA,SURVEY</td>\n",
              "      <td>PATT</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>TLT</td>\n",
              "      <td>Transactions on Learning Technologies</td>\n",
              "      <td>25</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caleb/Clayton</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>1118315889</td>\n",
              "      <td>using multimodal learning analytics to identify aspects of collaboration in project-based learning</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS</td>\n",
              "      <td>POSE,PROS</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>CSCL</td>\n",
              "      <td>Conference on Computer Supported Collaborative Learning</td>\n",
              "      <td>26</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"physical aspect of collaborative is an important part of this type of learning and that learning analytics systems can identify features that are relevant for helping researchers, teachers, and learners unpack what is happening.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>1118315889</td>\n",
              "      <td>using multimodal learning analytics to identify aspects of collaboration in project-based learning</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS</td>\n",
              "      <td>POSE,PROS</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>CSCL</td>\n",
              "      <td>Conference on Computer Supported Collaborative Learning</td>\n",
              "      <td>26</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"In this research study, we presented that where the students are looking, the distance between them, the motion of their hands our key features for a learning analytics system to be effectively used to identify collaboration in small groups of Engineering students.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>2</td>\n",
              "      <td>Determine best MMLA features for CPS. Blened Arduino+IDE. Model-based (regression).</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>1118315889</td>\n",
              "      <td>using multimodal learning analytics to identify aspects of collaboration in project-based learning</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS</td>\n",
              "      <td>POSE,PROS</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>CSCL</td>\n",
              "      <td>Conference on Computer Supported Collaborative Learning</td>\n",
              "      <td>26</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caleb/Clayton</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>1315379489</td>\n",
              "      <td>multimodal engagement analysis from facial videos in the classroom</td>\n",
              "      <td>Ömer Sümer</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO</td>\n",
              "      <td>POSE,AFFECT</td>\n",
              "      <td>CLS</td>\n",
              "      <td>EARLY,LATE</td>\n",
              "      <td>TAC</td>\n",
              "      <td>Transactions on Affective Computing</td>\n",
              "      <td>27</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM, STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"The best performing engagement classifiers achieved AUCs of .620 and .720 in Grades 8 and 12, respectively. We further investigated fusion strategies and found score-level fusion either improves the engagement classifiers or is on par with the best performing modality. We also investigated the effect of personalization and found that using only 60-seconds of person-specific data selected by margin uncertainty of the base classifier yielded an average AUC improvement of .084.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>1315379489</td>\n",
              "      <td>multimodal engagement analysis from facial videos in the classroom</td>\n",
              "      <td>Ömer Sümer</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO</td>\n",
              "      <td>POSE,AFFECT</td>\n",
              "      <td>CLS</td>\n",
              "      <td>EARLY,LATE</td>\n",
              "      <td>TAC</td>\n",
              "      <td>Transactions on Affective Computing</td>\n",
              "      <td>27</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM, STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"In contrast to the previous works that used mainly handcrafted local (i.e., local binary patterns, Gabor filters) and precomputed features such as head pose or estimated facial action units, we showed that engagement as a 3-class classification problem can be predicted in the classroom. We gathered a large-scale classroom observation dataset and collected the observer ratings of student engagement for Grades 8 and 12 (N=15). In contrast to the limited training and testing protocols in the literature, our study is the first to validate the use of automated engagement analysis in the classroom.\\n\\nOur work proves that even a small amount of person-specific data could considerably enhance the performance of engagement classifiers. In comparison to the person-independent settings of many machine learning and computer vision tasks, personalization in engagement analysis significantly impacts performance. We find this to be the case because of personal differences in visible behaviors during levels of low and high engagement. Furthermore, engagement can even reveal variation in time (for instance, the indicators of engagement are not the same in different classes, i.e., math and history).\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>2</td>\n",
              "      <td>AV facial detection. Model-based: RF, SVM, MLP, LSTM. Classroon setting (instructional, multi-person, physical). Many subjects stemming across STEM and humanities.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>1315379489</td>\n",
              "      <td>multimodal engagement analysis from facial videos in the classroom</td>\n",
              "      <td>Ömer Sümer</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO</td>\n",
              "      <td>POSE,AFFECT</td>\n",
              "      <td>CLS</td>\n",
              "      <td>EARLY,LATE</td>\n",
              "      <td>TAC</td>\n",
              "      <td>Transactions on Affective Computing</td>\n",
              "      <td>27</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM, STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caleb/Clayton</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>1374035721</td>\n",
              "      <td>attentivelearner2: a multimodal approach for improving mooc learning on mobile devices</td>\n",
              "      <td>Phuong Pham</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SURVEY</td>\n",
              "      <td>PULSE,AFFECT,SURVEY</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>AIED</td>\n",
              "      <td>International Conference on Artificial Intelligence in Education</td>\n",
              "      <td>28</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"In a 26-participant user study, we found that it is feasible to detect 6 types of emotion during learning via collected PPG signals and facial expressions and these modalities are complement with each other.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>1374035721</td>\n",
              "      <td>attentivelearner2: a multimodal approach for improving mooc learning on mobile devices</td>\n",
              "      <td>Phuong Pham</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SURVEY</td>\n",
              "      <td>PULSE,AFFECT,SURVEY</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>AIED</td>\n",
              "      <td>International Conference on Artificial Intelligence in Education</td>\n",
              "      <td>28</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"In a 26-participant user study, we found that it is feasible to detect 6 types of emotion during learning via collected PPG signals and facial expressions and these modalities are complement with each other.\"\\n\\n\"AttentiveLearner2 achieved high performance as all our models outperformed the baseline. Moreover, we found PPG signals and facial expressions are complement each other. If FEA features can win in 3 emotions (Confusion, Happiness, and Self-efficacy), PPG features are the best solution for Curiosity, and feature fusion can improve detection performance for Boredom and Frustration.\"\\n\\n\"In a 26-participant user study, we found that by taking advantages from two modalities, AttentiveLearner2 achieved higher detection accuracy than models using only one modality across 6 different emotions. More importantly, these results were achieved on unmodified smartphones which supports the scalable deployment of AttentiveLearner2\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>2</td>\n",
              "      <td>AttentiveLearner2 implicitly infers learners’ affective and cognitive states during learning by analyzing learners’ PPG signals and facial expressions to improve mobile MOOC learning. \\n\\nModel-based via multimodal emotion detection, but it's a short paper and the exact classification model is not provided.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>1374035721</td>\n",
              "      <td>attentivelearner2: a multimodal approach for improving mooc learning on mobile devices</td>\n",
              "      <td>Phuong Pham</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SURVEY</td>\n",
              "      <td>PULSE,AFFECT,SURVEY</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>AIED</td>\n",
              "      <td>International Conference on Artificial Intelligence in Education</td>\n",
              "      <td>28</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caleb/Clayton</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>1426267857</td>\n",
              "      <td>affect, support, and personal factors: multimodal causal models of one-on-one coaching</td>\n",
              "      <td>Lujie Karen Chen</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,SURVEY</td>\n",
              "      <td>PROS,GAZE,TRANS,AFFECT,SURVEY</td>\n",
              "      <td>STATS,NET</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JEDM</td>\n",
              "      <td>Journal of Educational Data Mining</td>\n",
              "      <td>29</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"this research contributes to the relatively sparse literature in multimodal learning analytics by providing a balanced view of the teacher and student interactions with a data set collected in naturalistic home environments.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>1426267857</td>\n",
              "      <td>affect, support, and personal factors: multimodal causal models of one-on-one coaching</td>\n",
              "      <td>Lujie Karen Chen</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,SURVEY</td>\n",
              "      <td>PROS,GAZE,TRANS,AFFECT,SURVEY</td>\n",
              "      <td>STATS,NET</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JEDM</td>\n",
              "      <td>Journal of Educational Data Mining</td>\n",
              "      <td>29</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"Firstly, we note a clear causal pathway between the group of variables describing parents’ support and another group representing the child’s cognitive-affective experience.\"\\n\\n\"Secondly, we note the causal pathway from Profile to Affect and, indirectly, to Support.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>2</td>\n",
              "      <td>Model-based via presented framework.\\n\\n\"We explore an analytical framework that is explainable and amenable to incorporating domain knowledge. The proposed framework combines statistical approaches in Sparse Multiple Canonical Correlation, causal discovery, and inference methods for observations. We demonstrate this framework using a multimodal one-on-one math problem-solving coaching dataset collected in naturalistic home environments involving parents and young children.\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>1426267857</td>\n",
              "      <td>affect, support, and personal factors: multimodal causal models of one-on-one coaching</td>\n",
              "      <td>Lujie Karen Chen</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,SURVEY</td>\n",
              "      <td>PROS,GAZE,TRANS,AFFECT,SURVEY</td>\n",
              "      <td>STATS,NET</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JEDM</td>\n",
              "      <td>Journal of Educational Data Mining</td>\n",
              "      <td>29</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caleb/Clayton</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>2055153191</td>\n",
              "      <td>round or rectangular tables for collaborative problem solving? a multimodal learning analytics study</td>\n",
              "      <td>Milica Vujovic</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,MOTION</td>\n",
              "      <td>POSE,GEST,ACT</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>30</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM, STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"The statistical analysis has shown significant differences between the levels of independent variables related to table shape and how the effect differs between two different levels of education, and this was further supported by a qualitative analysis of the observations obtained from the video recording of the activities.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>1</td>\n",
              "      <td>2 environments: blended arduino and blended pen/paper. \\nSTEM for CS, humanities for design.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>2055153191</td>\n",
              "      <td>round or rectangular tables for collaborative problem solving? a multimodal learning analytics study</td>\n",
              "      <td>Milica Vujovic</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,MOTION</td>\n",
              "      <td>POSE,GEST,ACT</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>30</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM, STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"Results show that the use of round tables (vs rectangular tables) leads to higher levels of on-task participation in the case of elementary school students. For university students, different table shapes seem to have a limited impact on their levels of participation in collaborative problem solving.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>2055153191</td>\n",
              "      <td>round or rectangular tables for collaborative problem solving? a multimodal learning analytics study</td>\n",
              "      <td>Milica Vujovic</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,MOTION</td>\n",
              "      <td>POSE,GEST,ACT</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>30</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM, STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Clayton/Caleb</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>2273914836</td>\n",
              "      <td>many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities</td>\n",
              "      <td>Jauwairia Nasir</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,PPA,SURVEY</td>\n",
              "      <td>PROS,AFFECT,GAZE,TRANS,LOGS</td>\n",
              "      <td>STATS,QUAL,CLUST,CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJCSCL</td>\n",
              "      <td>International Journal of Computer-Supported Collaborative Learning</td>\n",
              "      <td>31</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>\"Our combined multi-modal learning analytics and interaction analysis methodology enabled us to identify two multi-modal profles of learners who have learning gains and one multi-modal profle of learners who do not have learning gains.\"\\n\\n\"Using this approach, we are able to build the multimodal behavioral profles for each group of learners.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>1</td>\n",
              "      <td>blended: environment is the screen, but outside env is physical robot agent IRL.\\nmodel-based (SVM, RF) and model-free because classification and clustering used, along with stats.\\nsubject is STEM because goal of env is to teach about minimum-spanning-tree.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>2273914836</td>\n",
              "      <td>many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities</td>\n",
              "      <td>Jauwairia Nasir</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,PPA,SURVEY</td>\n",
              "      <td>PROS,AFFECT,GAZE,TRANS,LOGS</td>\n",
              "      <td>STATS,QUAL,CLUST,CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJCSCL</td>\n",
              "      <td>International Journal of Computer-Supported Collaborative Learning</td>\n",
              "      <td>31</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"Our combined multi-modal learning analytics and interaction analysis methodology enabled us to identify two multi-modal profles of learners who have learning gains and one multi-modal profle of learners who do not have learning gains.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>2273914836</td>\n",
              "      <td>many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities</td>\n",
              "      <td>Jauwairia Nasir</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,PPA,SURVEY</td>\n",
              "      <td>PROS,AFFECT,GAZE,TRANS,LOGS</td>\n",
              "      <td>STATS,QUAL,CLUST,CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJCSCL</td>\n",
              "      <td>International Journal of Computer-Supported Collaborative Learning</td>\n",
              "      <td>31</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Clayton/Caleb</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>1763513559</td>\n",
              "      <td>keep me in the loop: real-time feedback with multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2021</td>\n",
              "      <td>Training</td>\n",
              "      <td>SURVEY,LOGS,VIDEO,SENSOR,MOTION</td>\n",
              "      <td>POSE,EMG,GEST,SURVEY</td>\n",
              "      <td>CLS,QUAL,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJAIED</td>\n",
              "      <td>International Journal of Artificial Intelligence in Education</td>\n",
              "      <td>32</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"the architecture used for the CPR Tutor allowed for the provision of real-time multimodal feedback, and the generated feedback seemed to have a short-term positive influence on the considered CPR performance indicators.\"\\n\\n\"The CPR Tutor automatically recognises and assesses the quality of the chest compressions according to five CPR performance\\nindicators. It detects training mistakes in real-time by analysing a multimodal data stream consisting of kinematic and electromyographic data. Based on this assessment, the CPR Tutor provides audio feedback to correct the most critical mistakes and improve the CPR performance.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>1</td>\n",
              "      <td>CPR tutor. LSTM model.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>1763513559</td>\n",
              "      <td>keep me in the loop: real-time feedback with multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2021</td>\n",
              "      <td>Training</td>\n",
              "      <td>SURVEY,LOGS,VIDEO,SENSOR,MOTION</td>\n",
              "      <td>POSE,EMG,GEST,SURVEY</td>\n",
              "      <td>CLS,QUAL,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJAIED</td>\n",
              "      <td>International Journal of Artificial Intelligence in Education</td>\n",
              "      <td>32</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"we collected findings that, while they cannot be generalised, indicate that the feedback of the CPR tutor had a short-term positive influence on the CPR performance in the target classes.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>1763513559</td>\n",
              "      <td>keep me in the loop: real-time feedback with multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2021</td>\n",
              "      <td>Training</td>\n",
              "      <td>SURVEY,LOGS,VIDEO,SENSOR,MOTION</td>\n",
              "      <td>POSE,EMG,GEST,SURVEY</td>\n",
              "      <td>CLS,QUAL,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJAIED</td>\n",
              "      <td>International Journal of Artificial Intelligence in Education</td>\n",
              "      <td>32</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Clayton/Caleb</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>1345598079</td>\n",
              "      <td>intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning</td>\n",
              "      <td>Sofia Tancredi</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,VIDEO,AUDIO,INTER</td>\n",
              "      <td>GAZE,GEST,TRANS,POSE,INTER</td>\n",
              "      <td>PATT,QUAL,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MMLA Handbook</td>\n",
              "      <td>The Multimodal Learning Analytics Handbook</td>\n",
              "      <td>33</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Exploration, increasing confluence during Discovery, and high confluence during Fluency.\"\\n\\n\"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Exploration, increasing confluence during Discovery, and high confluence during Fluency. Towards the end of the Discovery stage, a coordination of coordinations (Piaget, 1970) developed wherein the coordination between the left- and right hands became coordinated with newly developed gaze structures spanning different screen locations.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>1</td>\n",
              "      <td>Physcal interaction with tablet to change ratios of two bars on screen.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>1345598079</td>\n",
              "      <td>intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning</td>\n",
              "      <td>Sofia Tancredi</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,VIDEO,AUDIO,INTER</td>\n",
              "      <td>GAZE,GEST,TRANS,POSE,INTER</td>\n",
              "      <td>PATT,QUAL,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MMLA Handbook</td>\n",
              "      <td>The Multimodal Learning Analytics Handbook</td>\n",
              "      <td>33</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"Our findings point to the importance of MMLA work that attunes to intermodal dynamics of learning, both as a pragmatic resource for identifying key moments in learning and as a resource for refining theoretical understandings of learning processes.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>1345598079</td>\n",
              "      <td>intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning</td>\n",
              "      <td>Sofia Tancredi</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,VIDEO,AUDIO,INTER</td>\n",
              "      <td>GAZE,GEST,TRANS,POSE,INTER</td>\n",
              "      <td>PATT,QUAL,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MMLA Handbook</td>\n",
              "      <td>The Multimodal Learning Analytics Handbook</td>\n",
              "      <td>33</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Clayton/Caleb</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>3135645357</td>\n",
              "      <td>multimodal teaching analytics: automated extraction of orchestration graphs from wearable sensor data</td>\n",
              "      <td>Luis P. Prieto</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,VIDEO,AUDIO,MOTION</td>\n",
              "      <td>GAZE,PROS,ACT,PIXEL</td>\n",
              "      <td>NET,CLS,STATS,PATT,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>Journal of Computer Assisted Learning</td>\n",
              "      <td>34</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"In summary, the results from our evaluation of personalized and general models to automatically extract orchestration graphs highlight the fact that machine learning models can be successfully trained with such multimodal sensor data, using relatively low-level features.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>1</td>\n",
              "      <td>Orchestration graphs. Multi-person because classroom-based and therefore multiple students.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>3135645357</td>\n",
              "      <td>multimodal teaching analytics: automated extraction of orchestration graphs from wearable sensor data</td>\n",
              "      <td>Luis P. Prieto</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,VIDEO,AUDIO,MOTION</td>\n",
              "      <td>GAZE,PROS,ACT,PIXEL</td>\n",
              "      <td>NET,CLS,STATS,PATT,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>Journal of Computer Assisted Learning</td>\n",
              "      <td>34</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"In summary, the results from our evaluation of personalized and general models to automatically extract orchestration graphs highlight the fact that machine learning models can be successfully trained with such multimodal sensor data, using relatively low-level features.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>3135645357</td>\n",
              "      <td>multimodal teaching analytics: automated extraction of orchestration graphs from wearable sensor data</td>\n",
              "      <td>Luis P. Prieto</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,VIDEO,AUDIO,MOTION</td>\n",
              "      <td>GAZE,PROS,ACT,PIXEL</td>\n",
              "      <td>NET,CLS,STATS,PATT,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>Journal of Computer Assisted Learning</td>\n",
              "      <td>34</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Clayton/Caleb</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>3856280479</td>\n",
              "      <td>children's play and problem-solving in motion-based learning technologies using a multi-modal mixed methods approach</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SENSOR,EYE,LOGS</td>\n",
              "      <td>ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP,BP</td>\n",
              "      <td>STATS,QUAL,CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJCCI</td>\n",
              "      <td>International Journal of Child-Computer Interaction</td>\n",
              "      <td>35</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>\"Our work exemplifies how the confluence of MMD and video coding can go further than data triangulation, and contribute to a holistic understanding of children’s play and problem-solving behaviours during their interactions with MBLT, by enabling researchers and designers the capacity to cater to children’s cognitive, affective and physiological processes to support learning through use of MBLT. To the best of our knowledge, there are no previous studies that use MMD from wearable and ubiquitous sensors (e.g., eye tracking glasses, wristbands and skeletal tracking) to investigate children’s behaviours in this context.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>1</td>\n",
              "      <td>Marvy learns (geometry). RF for classification (model-based). Exploratory Factor Analysis (EFA) for statistical analysis (model-free)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>3856280479</td>\n",
              "      <td>children's play and problem-solving in motion-based learning technologies using a multi-modal mixed methods approach</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SENSOR,EYE,LOGS</td>\n",
              "      <td>ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP,BP</td>\n",
              "      <td>STATS,QUAL,CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJCCI</td>\n",
              "      <td>International Journal of Child-Computer Interaction</td>\n",
              "      <td>35</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"Our work exemplifies how the confluence of MMD and video coding can go further than data triangulation, and contribute to a holistic understanding of children’s play and problem-solving behaviours during their interactions with MBLT, by enabling researchers and designers the capacity to cater to children’s cognitive, affective and physiological processes to support learning through use of MBLT.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>3856280479</td>\n",
              "      <td>children's play and problem-solving in motion-based learning technologies using a multi-modal mixed methods approach</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SENSOR,EYE,LOGS</td>\n",
              "      <td>ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP,BP</td>\n",
              "      <td>STATS,QUAL,CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJCCI</td>\n",
              "      <td>International Journal of Child-Computer Interaction</td>\n",
              "      <td>35</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Clayton/Caleb</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>2609260641</td>\n",
              "      <td>visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication</td>\n",
              "      <td>René Noël</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,RPA,INTER</td>\n",
              "      <td>PROS,POSE,RPA,INTER,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>DAMLE</td>\n",
              "      <td>Applied Sciences, Special Issue \"Data Analytics and Machine Learning in Education\"</td>\n",
              "      <td>36</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"We conducted a case study to compare the visualizations provided by the system in two different situations: collaborative and competitive activities. The results suggest that the provided visualizations help to identify issues on cognitive contribution, assimilation, self-regulation, and integration of the team members. They could also support teachers to decide whether they must assist a team in fostering collaboration.\\n\\nWhile the results are naturally constrained to the characteristics of the activities in which we tested the platform, they provide initial evidence about the technical fea-\\nsibility of extracting behavioral indicators and traces using MMLA to give insights onteam collaboration.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>1</td>\n",
              "      <td>2 env tasks: collaboratively write a sentence about what might be the first article of Chile's new constitution, and decide who should be saved in a bunker in an apocalyptic scenario</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>2609260641</td>\n",
              "      <td>visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication</td>\n",
              "      <td>René Noël</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,RPA,INTER</td>\n",
              "      <td>PROS,POSE,RPA,INTER,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>DAMLE</td>\n",
              "      <td>Applied Sciences, Special Issue \"Data Analytics and Machine Learning in Education\"</td>\n",
              "      <td>36</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"The results suggest that the provided visualizations help to identify issues on cognitive contribution, assimilation, self-regulation, and integration of the team members. They could also support teachers to decide whether they must assist a team in fostering collaboration.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>2609260641</td>\n",
              "      <td>visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication</td>\n",
              "      <td>René Noël</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,RPA,INTER</td>\n",
              "      <td>PROS,POSE,RPA,INTER,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>DAMLE</td>\n",
              "      <td>Applied Sciences, Special Issue \"Data Analytics and Machine Learning in Education\"</td>\n",
              "      <td>36</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>PROF, UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Clayton/Caleb</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>666050348</td>\n",
              "      <td>multicraft: a multimodal interface for supporting and studying learning in minecraft</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS</td>\n",
              "      <td>PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>HCII</td>\n",
              "      <td>International Conference on Human-Computer Interaction</td>\n",
              "      <td>37</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"Through our user studies, we found that the platform helps fulfill some of those goals by providing capabilities that can spur on amazement and excitement among traditional Minecraft users and novices. We also find that many of the multimodal components, while not immediately intuitive for users, proved to be preferred modes of game play. In this sense, we feel that this tool is moving in the right direction in terms of the system capabilities that it provides. Our analyses also point to the meaningful ways that multimodal data can be used to study student learning in these game-based environments, and free students from standardized testing and learning experiences.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>1</td>\n",
              "      <td>Multicraft: multimodal Minecraft.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>666050348</td>\n",
              "      <td>multicraft: a multimodal interface for supporting and studying learning in minecraft</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS</td>\n",
              "      <td>PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>HCII</td>\n",
              "      <td>International Conference on Human-Computer Interaction</td>\n",
              "      <td>37</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"Through our user studies, we found that the platform helps fulfill some of those goals by providing capabilities that can spur on amazement and excitement among traditional Minecraft users and novices. We also find that many of the multimodal components, while not immediately intuitive for users, proved to be preferred modes of game play.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>666050348</td>\n",
              "      <td>multicraft: a multimodal interface for supporting and studying learning in minecraft</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS</td>\n",
              "      <td>PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>HCII</td>\n",
              "      <td>International Conference on Human-Computer Interaction</td>\n",
              "      <td>37</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Clayton/Caleb</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>1637690235</td>\n",
              "      <td>supervised machine learning in multimodal learning analytics for estimating success in project-based learning</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,PPA,RPA</td>\n",
              "      <td>POSE,GEST,PROS,LOGS,PPA,RPA</td>\n",
              "      <td>REG,CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>Journal of Computer Assisted Learning</td>\n",
              "      <td>38</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"In this paper, we show that MMLA and the state-of-the-art computational techniques can be used to generate insights into the \"black box\" of learning in students’ project-based activities. These insights generated from multimodal data can be used to inform teachers about the key features of project-based learning and help them support students appropriately in similar pedagogical approaches.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>1</td>\n",
              "      <td>Arduino + browser IDE, so blended engineering. DNN + regression. Also tested SVM, NB, LR.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>1637690235</td>\n",
              "      <td>supervised machine learning in multimodal learning analytics for estimating success in project-based learning</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,PPA,RPA</td>\n",
              "      <td>POSE,GEST,PROS,LOGS,PPA,RPA</td>\n",
              "      <td>REG,CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>Journal of Computer Assisted Learning</td>\n",
              "      <td>38</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"In this paper, we show that MMLA and the state-of-the-art computational techniques can be used to generate insights into the \"black box\" of learning in students’ project-based activities. These insights generated from multimodal data can be used to inform teachers about the key features of project-based learning and help them support students appropriately in similar pedagogical approaches. Towards achieving this ultimate aim, this paper has three main contributions to the field. First, we show that the distances between students’ hands and faces while they are working on projects is a strong predictor of students’ artefact quality which indicates the value of student collaboration in these pedagogical approaches. Second, we show that both, new and promising approaches such as neural networks and more traditional regression approaches, can be used to classify MMLA data and both have advantages and disadvantages depending on the research questions and contexts being investigated. At last but not least, although, it is traditionally notoriously challenging to provide evidence about the robust and objective evaluations of project-based learning activities, techniques and types of data we presented here can be the first step towards effective implementation and evaluation of project-based learning at a scale.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>1637690235</td>\n",
              "      <td>supervised machine learning in multimodal learning analytics for estimating success in project-based learning</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,PPA,RPA</td>\n",
              "      <td>POSE,GEST,PROS,LOGS,PPA,RPA</td>\n",
              "      <td>REG,CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>Journal of Computer Assisted Learning</td>\n",
              "      <td>38</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Clayton/Caleb</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>2155422499</td>\n",
              "      <td>a multimodal analysis of pair work engagement episodes: implications for emi lecturer training</td>\n",
              "      <td>Teresa Morell</td>\n",
              "      <td>2022</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>TRANS,PPA,QUAL,POSE,ACT</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>JEAP</td>\n",
              "      <td>Journal of English for Academic Purposes</td>\n",
              "      <td>39</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"In this study, we have given special emphasis to the interpersonal metafunction (see Fig. 1), which is crucial to enhance EMI students’ active participation and engagement. The outcomes of this study point to the need to address multimodal interactional competence in professional development programs, as described in Morell et al. (2022) to promote effective lecturing in the diverse EMI scenarios.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>1</td>\n",
              "      <td>Workshop for professional development for professors teaching in English (in non-English speaking country)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>2155422499</td>\n",
              "      <td>a multimodal analysis of pair work engagement episodes: implications for emi lecturer training</td>\n",
              "      <td>Teresa Morell</td>\n",
              "      <td>2022</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>TRANS,PPA,QUAL,POSE,ACT</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>JEAP</td>\n",
              "      <td>Journal of English for Academic Purposes</td>\n",
              "      <td>39</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"The exploration of how EMI lecturers use semiotic resources to construct meaning and to create engagement paves the way to a unified multimodal interactional competence. In general, the mastery of this competence enables lecturers to convert students from passive listeners/observers to active participants, giving them opportunities to engage in active learning, language usage and critical thinking.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>2155422499</td>\n",
              "      <td>a multimodal analysis of pair work engagement episodes: implications for emi lecturer training</td>\n",
              "      <td>Teresa Morell</td>\n",
              "      <td>2022</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>TRANS,PPA,QUAL,POSE,ACT</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>JEAP</td>\n",
              "      <td>Journal of English for Academic Purposes</td>\n",
              "      <td>39</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Clayton/Caleb</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>3309250332</td>\n",
              "      <td>(dis)engagement matters: identifying efficacious learning practices with multimodal learning analytics</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,PPA</td>\n",
              "      <td>GEST,QUAL,PPA</td>\n",
              "      <td>CLUST,CLS,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>40</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>\"I was able to use multimodal data and machine learning to develop\\nplausible arguments for students’ differential learning gains.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>1</td>\n",
              "      <td>2 tasks: use sheet of paper to hold text book, and use other materials to support a 0.5lb mass\\n\\nModel-based w/ DT classifier, model-free with k-means clustering.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>3309250332</td>\n",
              "      <td>(dis)engagement matters: identifying efficacious learning practices with multimodal learning analytics</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,PPA</td>\n",
              "      <td>GEST,QUAL,PPA</td>\n",
              "      <td>CLUST,CLS,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>40</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"I was able to draw inferences related to productive engagement and disengagement in the context of collaborative problem solving. These inferences would have been difficult to articulate using only machine learning, and hard to identify using only human coding. Hence, I argue that an intermediate model that leverages the affordances of multimodal data and computation, but leaves inference development to trained scholars could offer a viable alternative to purely qualitative or machine learning approaches.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>3309250332</td>\n",
              "      <td>(dis)engagement matters: identifying efficacious learning practices with multimodal learning analytics</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,PPA</td>\n",
              "      <td>GEST,QUAL,PPA</td>\n",
              "      <td>CLUST,CLS,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>40</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Clayton/Caleb</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>3625722965</td>\n",
              "      <td>table tennis tutor: forehand strokes classification based on multimodal data and neural networks</td>\n",
              "      <td>Khaleel Asyraaf Mat Sanusi</td>\n",
              "      <td>2021</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,INTER</td>\n",
              "      <td>POSE,GEST,ACT,INTER</td>\n",
              "      <td>CLS,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>Sensors</td>\n",
              "      <td>MDPI Sensors</td>\n",
              "      <td>41</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>PSY</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"The precision (73%) and recall (61%) for the combined devices (Smartphone + Kinect) achieved the best results compared to the other two classes.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>1</td>\n",
              "      <td>Table Tennis Tutor, LSTM classification.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>3625722965</td>\n",
              "      <td>table tennis tutor: forehand strokes classification based on multimodal data and neural networks</td>\n",
              "      <td>Khaleel Asyraaf Mat Sanusi</td>\n",
              "      <td>2021</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,INTER</td>\n",
              "      <td>POSE,GEST,ACT,INTER</td>\n",
              "      <td>CLS,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>Sensors</td>\n",
              "      <td>MDPI Sensors</td>\n",
              "      <td>41</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI, UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"We observed, within our context, that smartphone sensors by themselves are unable to perform better than the Kinect. In addition, it is likely that the smartphone sensors are able to classify the strokes by complete chance due to 51% of accuracy. However,the performance improves when both of the devices are combined.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>3625722965</td>\n",
              "      <td>table tennis tutor: forehand strokes classification based on multimodal data and neural networks</td>\n",
              "      <td>Khaleel Asyraaf Mat Sanusi</td>\n",
              "      <td>2021</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,INTER</td>\n",
              "      <td>POSE,GEST,ACT,INTER</td>\n",
              "      <td>CLS,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>Sensors</td>\n",
              "      <td>MDPI Sensors</td>\n",
              "      <td>41</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Clayton/Caleb</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>4278392816</td>\n",
              "      <td>multimodal data as a means to understand the learning experience</td>\n",
              "      <td>Michail Giannakos</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>LOGS,EYE,SENSOR,VIDEO</td>\n",
              "      <td>EEG,GAZE,LOGS,PULSE,EDA,TEMP,BP,POSE</td>\n",
              "      <td>REG,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJIM</td>\n",
              "      <td>International Journal of Information Management</td>\n",
              "      <td>42</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>\"Overall, our work shows that capturing multimodal data can help us increase the prediction accuracy of users’ learning performance in learner–computer interaction (LCI). In addition, the study shows that the most commonly used data-stream (i.e., keystrokes) is the poorest proxy of our learning performance. Thus, leveraging advances in contemporary learning environments and physiological sensing (wearable, EEG etc.), we provide evidence that multimodal data can be a viable method to accurately track users’ states during learning, thereby providing unique possibilities of closing the loop between the learning technology and the learner. Therefore, the incorporation of multimodal data enables HCI and learning technology researchers to examine unscripted, complex tasks in more holistic and accurate ways.\"\\n\\nIdentified specific multimodal features that were best predictors of performance.</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>1</td>\n",
              "      <td>Pac-Man. Model-based for regression, model-free for statistical methods.\\n\\nDoes training always map to training or is Pacman both a training environment that is informal, for instance?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>4278392816</td>\n",
              "      <td>multimodal data as a means to understand the learning experience</td>\n",
              "      <td>Michail Giannakos</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>LOGS,EYE,SENSOR,VIDEO</td>\n",
              "      <td>EEG,GAZE,LOGS,PULSE,EDA,TEMP,BP,POSE</td>\n",
              "      <td>REG,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJIM</td>\n",
              "      <td>International Journal of Information Management</td>\n",
              "      <td>42</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"Overall, our work shows that capturing multimodal data can help us increase the prediction accuracy of users’ learning performance in learner–computer interaction (LCI). In addition, the study shows that the most commonly used data-stream (i.e., keystrokes) is the poorest proxy of our learning performance. Thus, leveraging advances in contemporary learning environments and physiological sensing (wearable, EEG etc.), we provide evidence that multimodal data can be a viable method to accurately track users’ states during learning, thereby providing unique possibilities of closing the loop between the learning technology and the learner. Therefore, the incorporation of multimodal data enables HCI and learning technology researchers to examine unscripted, complex tasks in more holistic and accurate ways.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>4278392816</td>\n",
              "      <td>multimodal data as a means to understand the learning experience</td>\n",
              "      <td>Michail Giannakos</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>LOGS,EYE,SENSOR,VIDEO</td>\n",
              "      <td>EEG,GAZE,LOGS,PULSE,EDA,TEMP,BP,POSE</td>\n",
              "      <td>REG,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJIM</td>\n",
              "      <td>International Journal of Information Management</td>\n",
              "      <td>42</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Clayton/Caleb</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>566043228</td>\n",
              "      <td>automatic student engagement in online learning environment based on neural turing machine</td>\n",
              "      <td>Xiaoyang Ma</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO</td>\n",
              "      <td>POSE,GAZE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>IJIET</td>\n",
              "      <td>International Journal of Information and Education Technology</td>\n",
              "      <td>43</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"Through comparison we can find that our proposed model is superior to the most commonly used model of feature fusion in the past.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>1</td>\n",
              "      <td>Engagement detection, DAiSEE dataset. Neural Turing Machine (NTM) RNN.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>566043228</td>\n",
              "      <td>automatic student engagement in online learning environment based on neural turing machine</td>\n",
              "      <td>Xiaoyang Ma</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO</td>\n",
              "      <td>POSE,GAZE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>IJIET</td>\n",
              "      <td>International Journal of Information and Education Technology</td>\n",
              "      <td>43</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"From the perspective of multiple features, we get the fusion of multiple short video features by using two fully connected layers. It can be found from Table I that our method can more accurately predict the student's learning participation than the traditional method of weighted summation and averaging.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>566043228</td>\n",
              "      <td>automatic student engagement in online learning environment based on neural turing machine</td>\n",
              "      <td>Xiaoyang Ma</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO</td>\n",
              "      <td>POSE,GAZE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>IJIET</td>\n",
              "      <td>International Journal of Information and Education Technology</td>\n",
              "      <td>43</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Clayton/Caleb</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>853680639</td>\n",
              "      <td>sensor-based data fusion for multimodal affect detection in game-based learning environments</td>\n",
              "      <td>Nathan Henderson</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,SENSOR,RPA</td>\n",
              "      <td>POSE,EDA,AFFECT</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>EDM</td>\n",
              "      <td>International Conference on Educational Data Mining</td>\n",
              "      <td>44</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>OTH</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"Results indicate that multimodal approaches outperform unimodal baseline classifiers, and feature-level concatenation offers the highest performance among the data fusion techniques.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>1</td>\n",
              "      <td>Affect detection in simulated medical environment. \"other\" for environment type because medical? SVM and DNN for model-based.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>853680639</td>\n",
              "      <td>sensor-based data fusion for multimodal affect detection in game-based learning environments</td>\n",
              "      <td>Nathan Henderson</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,SENSOR,RPA</td>\n",
              "      <td>POSE,EDA,AFFECT</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>EDM</td>\n",
              "      <td>International Conference on Educational Data Mining</td>\n",
              "      <td>44</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"We show the improvement that multimodal classifiers achieve compared with unimodal classifiers for both modalities. We also demonstrate that SVMs outperform ANNs as a unimodal classifier in this particular domain. Finally, we demonstrate that data fusion is an effective way to combine multiple modalities, either prior to or following classification.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>853680639</td>\n",
              "      <td>sensor-based data fusion for multimodal affect detection in game-based learning environments</td>\n",
              "      <td>Nathan Henderson</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,SENSOR,RPA</td>\n",
              "      <td>POSE,EDA,AFFECT</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>EDM</td>\n",
              "      <td>International Conference on Educational Data Mining</td>\n",
              "      <td>44</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Clayton/Caleb</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>3637456466</td>\n",
              "      <td>impact of inquiry interventions on students in e-learning and classroom environments using affective computing framework</td>\n",
              "      <td>T. S. Ashwin</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,PPA</td>\n",
              "      <td>AFFECT,POSE,GEST</td>\n",
              "      <td>CLS,STATS,PATT</td>\n",
              "      <td>MID</td>\n",
              "      <td>UMUAI</td>\n",
              "      <td>User Modeling and User-Adapted Interaction</td>\n",
              "      <td>45</td>\n",
              "      <td>PHYS, VIRT</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"The proposed method with multi-person detection, multi-modality, group engagement score, inquiry intervention and with an accuracy of 0.77 for a test data of more than 350 students, outperforms the existing methods.</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>1</td>\n",
              "      <td>Ashwin paper! Automated inquiry-based instruction based on affective state. Two environments: classroom (multi-student) and e-learning (single-student). Classification via Inception and YOLO.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>3637456466</td>\n",
              "      <td>impact of inquiry interventions on students in e-learning and classroom environments using affective computing framework</td>\n",
              "      <td>T. S. Ashwin</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,PPA</td>\n",
              "      <td>AFFECT,POSE,GEST</td>\n",
              "      <td>CLS,STATS,PATT</td>\n",
              "      <td>MID</td>\n",
              "      <td>UMUAI</td>\n",
              "      <td>User Modeling and User-Adapted Interaction</td>\n",
              "      <td>45</td>\n",
              "      <td>PHYS, VIRT</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"The overall experimental results demonstrate that there is a positive correlation with r = 0.74 between students’ affective states and their performance. Proposed inquiry intervention improved the students’ performance as there is a decrease of 65%, 43%, 43%, and 53% in overall in-attentive affective state instances using the inquiry interventions in e-learning, flipped classroom, classroom and webinar environments, respectively.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>3637456466</td>\n",
              "      <td>impact of inquiry interventions on students in e-learning and classroom environments using affective computing framework</td>\n",
              "      <td>T. S. Ashwin</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,PPA</td>\n",
              "      <td>AFFECT,POSE,GEST</td>\n",
              "      <td>CLS,STATS,PATT</td>\n",
              "      <td>MID</td>\n",
              "      <td>UMUAI</td>\n",
              "      <td>User Modeling and User-Adapted Interaction</td>\n",
              "      <td>45</td>\n",
              "      <td>PHYS, VIRT</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Clayton/Caleb</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>3754172825</td>\n",
              "      <td>detecting impasse during collaborative problem solving with multimodal learning analytics</td>\n",
              "      <td>Yingbo Ma</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>TRANS,PROS,SPECT,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>46</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements in the video modality are the most significant unimodal indicators of impasse.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>1</td>\n",
              "      <td>Learning CS w/ Snap! collaboratively. SVM, BERT, MLP for model-based.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>3754172825</td>\n",
              "      <td>detecting impasse during collaborative problem solving with multimodal learning analytics</td>\n",
              "      <td>Yingbo Ma</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>TRANS,PROS,SPECT,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>46</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements in the video modality are the most significant unimodal indicators of impasse. We also trained several multimodal models and found that combining indicators from these three modalities provided the best impasse detection performance.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>3754172825</td>\n",
              "      <td>detecting impasse during collaborative problem solving with multimodal learning analytics</td>\n",
              "      <td>Yingbo Ma</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>TRANS,PROS,SPECT,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>International Conference on Learning Analytics &amp; Knowledge</td>\n",
              "      <td>46</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Clayton/Caleb</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>1296637108</td>\n",
              "      <td>towards collaboration translucence: giving meaning to multimodal group data</td>\n",
              "      <td>Vanessa Echeverria</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER</td>\n",
              "      <td>POSE,LOGS,TRANS,EDA,ACT,PROS,INTER</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>CHI</td>\n",
              "      <td>Conference on Human Factors in Computing Systems</td>\n",
              "      <td>47</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"We illustrate the approach from the automated analysis of healthcare simulations to train nurses, generating four visual proxies that fuse multimodal data into higher order patterns.\"</td>\n",
              "      <td>Clayton</td>\n",
              "      <td>1</td>\n",
              "      <td>Monash nursing group. Other for healthcare?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>1296637108</td>\n",
              "      <td>towards collaboration translucence: giving meaning to multimodal group data</td>\n",
              "      <td>Vanessa Echeverria</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER</td>\n",
              "      <td>POSE,LOGS,TRANS,EDA,ACT,PROS,INTER</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>CHI</td>\n",
              "      <td>Conference on Human Factors in Computing Systems</td>\n",
              "      <td>47</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"we introduce the concept of collaboration translucence, an approach to make visible selected features of group activity. This is grounded both theoretically (in the physical, epistemic, social and affective dimensions of group activity), and contextually (using domain-specific concepts). We illustrate the approach from the automated analysis of healthcare simulations to train nurses, generating four visual proxies that fuse multimodal data into higher order patterns.\"</td>\n",
              "      <td>Caleb</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>1296637108</td>\n",
              "      <td>towards collaboration translucence: giving meaning to multimodal group data</td>\n",
              "      <td>Vanessa Echeverria</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER</td>\n",
              "      <td>POSE,LOGS,TRANS,EDA,ACT,PROS,INTER</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>CHI</td>\n",
              "      <td>Conference on Human Factors in Computing Systems</td>\n",
              "      <td>47</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Clayton/Caleb</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c8b7ef6-74ce-45ac-a789-9df4df748038')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6c8b7ef6-74ce-45ac-a789-9df4df748038 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6c8b7ef6-74ce-45ac-a789-9df4df748038');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-69d109e7-5434-47aa-b602-2c6c81c0879c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-69d109e7-5434-47aa-b602-2c6c81c0879c')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-69d109e7-5434-47aa-b602-2c6c81c0879c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_197d66cd-85f4-4705-95c3-294f50250a41\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_197d66cd-85f4-4705-95c3-294f50250a41 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}