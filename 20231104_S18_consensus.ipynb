{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Clayton Cohn<br>\n",
        "4 Nov 2023<br>\n",
        "OELE Lab<br>\n",
        "Vanderbilt University\n",
        "\n",
        "#<center> S18 Consensus and Cohen's *k*"
      ],
      "metadata": {
        "id": "9PKxx1Tapha2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction and Attribution\n",
        "\n",
        "This notebook was create by Clayton Cohn for the purpose of creating the MMLTE survey's final consensus document.\n",
        "\n",
        "In this notebook, we will:\n",
        "*   Map all extracted features to acronyms\n",
        "*   Export IRR spreadsheet for Cohen's *k*\n",
        "\n",
        "In the next notebook, we will:\n",
        "*   Calculate Cohen's k for the additional extracted features\n",
        "*   Export final consensus and spreadsheet\n",
        "\n",
        "The MMLTE survey project is a collaborative effor between Dr. Gautam Biswas, Clayton Cohn, Eduardo Davalos, Joyce Fonteles, Dr. Meiyi Ma, Caleb Vatral, and Hanchen (David) Wang."
      ],
      "metadata": {
        "id": "J01H3CtWqLU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Import"
      ],
      "metadata": {
        "id": "KQofVjXmqalW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdabTZPdpCs4",
        "outputId": "b1468f34-9b7a-4860-ba00-9c098ce51932"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "S18_PATH = \"drive/My Drive/Clayton/20230420_MMLTE/S18_Consensus.csv\""
      ],
      "metadata": {
        "id": "F7V8sEagq0wM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display max rows, columns, column length\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "df = pd.read_csv(S18_PATH,header=0)\n",
        "\n",
        "for col in df:\n",
        "  if col not in {\"Analysis Results (w/ multimodal advantages)\",\"Reviewer Notes\"}:\n",
        "    assert not df[col].isnull().values.any(), print(col)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tuftYmRbrDV5",
        "outputId": "107b043c-024f-4932-a670-321bb09c40e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums Mapped Modalities Mapped Analysis Methods  \\\n",
              "0                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "1                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "2                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "3             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "4             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "\n",
              "  Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0                LATE                     MLPALA   \n",
              "1                LATE                     MLPALA   \n",
              "2                LATE                     MLPALA   \n",
              "3              HYBRID                       BJET   \n",
              "4              HYBRID                       BJET   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "\n",
              "  Environment Setting Environment Sub     Participant Structure  \\\n",
              "0            physical            STEM  individual, multi-person   \n",
              "1            physical            STEM  individual, multi-person   \n",
              "2            physical            STEM  individual, multi-person   \n",
              "3            physical            STEM              multi-person   \n",
              "4            physical            STEM              multi-person   \n",
              "\n",
              "  Didactic Nature Level of Instruction or Training Analysis Approach  \\\n",
              "0   instructional                      unspecified       model-based   \n",
              "1   instructional                      unspecified       model-based   \n",
              "2   instructional                      unspecified       model-based   \n",
              "3   instructional                             K-12       model-based   \n",
              "4   instructional                             K-12       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b9fc9210-65b6-4b32-8665-284a28109352\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Sub</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>physical</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>physical</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>physical</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>physical</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>physical</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b9fc9210-65b6-4b32-8665-284a28109352')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b9fc9210-65b6-4b32-8665-284a28109352 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b9fc9210-65b6-4b32-8665-284a28109352');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-650f66a3-991c-4075-86e6-e5a27cd136cf\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-650f66a3-991c-4075-86e6-e5a27cd136cf')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-650f66a3-991c-4075-86e6-e5a27cd136cf button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Map to Acronyms"
      ],
      "metadata": {
        "id": "sq-5S8zItEr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Setting"
      ],
      "metadata": {
        "id": "eLiKIi0QtHmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_setting_vals = set()\n",
        "env_setting_list = [l.split(\", \") for l in list(df[\"Environment Setting\"])]\n",
        "for item in env_setting_list:\n",
        "  for val in item:\n",
        "    env_setting_vals.add(val.lower())\n",
        "\n",
        "env_setting_vals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7cKYDIgtLZJ",
        "outputId": "ec0284e7-8310-4c1f-d716-7f14e39e1c33"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'blended', 'physical', 'unspecified', 'virtual', 'virutal'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_setting_map = {\n",
        "    \"blended\": \"BLND\",\n",
        "    \"physical\": \"PHYS\",\n",
        "    \"virtual\": \"VIRT\",\n",
        "    \"virutal\": \"VIRT\",\n",
        "    \"unspecified\": \"UNSP\"\n",
        "}"
      ],
      "metadata": {
        "id": "xnQs3ZWDu5Cj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_env_setting_list = []\n",
        "\n",
        "for item in env_setting_list:\n",
        "  new_item = set()\n",
        "  for val in item:\n",
        "    new_item.add(env_setting_map[val.lower()])\n",
        "  new_env_setting_list.append(list(new_item))\n",
        "\n",
        "new_env_setting_list[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26h3gmGhtLbC",
        "outputId": "ba71b889-5996-40dd-c0a9-88d75e498e6c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['PHYS'], ['PHYS'], ['PHYS'], ['PHYS'], ['PHYS']]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_env_setting_list_combined = []\n",
        "\n",
        "for item in new_env_setting_list:\n",
        "  new_item = \", \".join(item)\n",
        "  new_env_setting_list_combined.append(new_item)\n",
        "\n",
        "new_env_setting_list_combined[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyauyk1UtG9d",
        "outputId": "e0488066-0e4b-475d-932c-6fb793e51845"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PHYS', 'PHYS', 'PHYS', 'PHYS', 'PHYS']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.insert(13, \"Environment Setting (mapped)\", new_env_setting_list_combined)\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZPE0OIAuyCn4",
        "outputId": "c7cbd90c-67c9-4c92-fb26-263f04a6d95a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "5  1469065963   \n",
              "6  1598166515   \n",
              "7  1598166515   \n",
              "8  1598166515   \n",
              "9  1877483551   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "5  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "6                                                            multimodal learning analytics for game-based learning   \n",
              "7                                                            multimodal learning analytics for game-based learning   \n",
              "8                                                            multimodal learning analytics for game-based learning   \n",
              "9                           motion-based educational games: using multi-modal data to predict player’s performance   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "5            Andy Nguyen  2022                                Learning   \n",
              "6         Andrew Emerson  2020                                Learning   \n",
              "7         Andrew Emerson  2020                                Learning   \n",
              "8         Andrew Emerson  2020                                Learning   \n",
              "9     Serena Lee-Cultura  2020                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums         Mapped Modalities  \\\n",
              "0                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "1                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "2                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "3             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "4             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "5             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "6                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "7                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "8                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "9               VIDEO,EYE,SENSOR  PULSE,TEMP,EDA,GAZE,POSE   \n",
              "\n",
              "  Mapped Analysis Methods Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0               CLS,CLUST                LATE                     MLPALA   \n",
              "1               CLS,CLUST                LATE                     MLPALA   \n",
              "2               CLS,CLUST                LATE                     MLPALA   \n",
              "3          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "4          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "5          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "6               CLS,STATS                 MID                       BJET   \n",
              "7               CLS,STATS                 MID                       BJET   \n",
              "8               CLS,STATS                 MID                       BJET   \n",
              "9                     CLS                 MID                        COG   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "5                   British Journal of Educational Technology            4   \n",
              "6                   British Journal of Educational Technology            5   \n",
              "7                   British Journal of Educational Technology            5   \n",
              "8                   British Journal of Educational Technology            5   \n",
              "9                                    IEEE Conference on Games            6   \n",
              "\n",
              "  Environment Setting Environment Setting (mapped) Environment Sub  \\\n",
              "0            physical                         PHYS            STEM   \n",
              "1            physical                         PHYS            STEM   \n",
              "2            physical                         PHYS            STEM   \n",
              "3            physical                         PHYS            STEM   \n",
              "4            physical                         PHYS            STEM   \n",
              "5            physical                         PHYS            STEM   \n",
              "6             virtual                         VIRT            STEM   \n",
              "7             virtual                         VIRT            STEM   \n",
              "8             virtual                         VIRT            STEM   \n",
              "9             blended                         BLND            STEM   \n",
              "\n",
              "      Participant Structure Didactic Nature Level of Instruction or Training  \\\n",
              "0  individual, multi-person   instructional                      unspecified   \n",
              "1  individual, multi-person   instructional                      unspecified   \n",
              "2  individual, multi-person   instructional                      unspecified   \n",
              "3              multi-person   instructional                             K-12   \n",
              "4              multi-person   instructional                             K-12   \n",
              "5              multi-person   instructional                             K-12   \n",
              "6                individual        informal                    undergraduate   \n",
              "7                individual        informal                    undergraduate   \n",
              "8                individual        informal                    undergraduate   \n",
              "9                individual   instructional                             K-12   \n",
              "\n",
              "  Analysis Approach  \\\n",
              "0       model-based   \n",
              "1       model-based   \n",
              "2       model-based   \n",
              "3       model-based   \n",
              "4       model-based   \n",
              "5       model-based   \n",
              "6       model-based   \n",
              "7       model-based   \n",
              "8       model-based   \n",
              "9       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "6                                                                                                       Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. The findings suggest that MMLA can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding   \n",
              "7                                                                                                                                                                                                                                                                                                                                                              Common case of multimodal outperform unimodal models, through the addition of gaze to classify student's posttest performance and interest.   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "9                                                                                                                                                                                           Authors conclude that the feature combination of gaze and physiological MMD provide the most accurate predictions of correct answers. They also show the feasibility of early prediction of children's performance by using half (as oppose to full) data lengths to extract features and predict correctness.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  \n",
              "5             Joyce/Eduardo      1&2            NaN  \n",
              "6                     Joyce        1            NaN  \n",
              "7                   Eduardo        2            NaN  \n",
              "8             Joyce/Eduardo      1&2            NaN  \n",
              "9                     Joyce        1            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c7f6987c-de46-4000-be3a-edf5761f919f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Setting (mapped)</th>\n",
              "      <th>Environment Sub</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>physical</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>physical</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>physical</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>physical</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>physical</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>physical</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>virtual</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual</td>\n",
              "      <td>informal</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. The findings suggest that MMLA can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>virtual</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual</td>\n",
              "      <td>informal</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Common case of multimodal outperform unimodal models, through the addition of gaze to classify student's posttest performance and interest.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>virtual</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual</td>\n",
              "      <td>informal</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1877483551</td>\n",
              "      <td>motion-based educational games: using multi-modal data to predict player’s performance</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR</td>\n",
              "      <td>PULSE,TEMP,EDA,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>COG</td>\n",
              "      <td>IEEE Conference on Games</td>\n",
              "      <td>6</td>\n",
              "      <td>blended</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Authors conclude that the feature combination of gaze and physiological MMD provide the most accurate predictions of correct answers. They also show the feasibility of early prediction of children's performance by using half (as oppose to full) data lengths to extract features and predict correctness.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c7f6987c-de46-4000-be3a-edf5761f919f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c7f6987c-de46-4000-be3a-edf5761f919f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c7f6987c-de46-4000-be3a-edf5761f919f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5c9db119-df7b-45d3-a316-d7085d633535\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5c9db119-df7b-45d3-a316-d7085d633535')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5c9db119-df7b-45d3-a316-d7085d633535 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=[\"Environment Setting\"], inplace=True)\n",
        "df.rename(columns={'Environment Setting (mapped)': 'Environment Setting'},inplace=True)\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zCtREdT_tP56",
        "outputId": "1d3d9c09-5a89-4d70-c861-8dc78cf2cddc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums Mapped Modalities Mapped Analysis Methods  \\\n",
              "0                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "1                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "2                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "3             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "4             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "\n",
              "  Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0                LATE                     MLPALA   \n",
              "1                LATE                     MLPALA   \n",
              "2                LATE                     MLPALA   \n",
              "3              HYBRID                       BJET   \n",
              "4              HYBRID                       BJET   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "\n",
              "  Environment Setting Environment Sub     Participant Structure  \\\n",
              "0                PHYS            STEM  individual, multi-person   \n",
              "1                PHYS            STEM  individual, multi-person   \n",
              "2                PHYS            STEM  individual, multi-person   \n",
              "3                PHYS            STEM              multi-person   \n",
              "4                PHYS            STEM              multi-person   \n",
              "\n",
              "  Didactic Nature Level of Instruction or Training Analysis Approach  \\\n",
              "0   instructional                      unspecified       model-based   \n",
              "1   instructional                      unspecified       model-based   \n",
              "2   instructional                      unspecified       model-based   \n",
              "3   instructional                             K-12       model-based   \n",
              "4   instructional                             K-12       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-748a4a32-6394-4ee5-b14c-102b9e0781df\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Sub</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-748a4a32-6394-4ee5-b14c-102b9e0781df')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-748a4a32-6394-4ee5-b14c-102b9e0781df button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-748a4a32-6394-4ee5-b14c-102b9e0781df');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d9a0634c-cd2f-4e77-86c3-590860d2cb6b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d9a0634c-cd2f-4e77-86c3-590860d2cb6b')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d9a0634c-cd2f-4e77-86c3-590860d2cb6b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Subject"
      ],
      "metadata": {
        "id": "cPuO7WARtLy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_subject_vals = set()\n",
        "env_subject_list = [l.split(\", \") for l in list(df[\"Environment Sub\"])]\n",
        "for item in env_subject_list:\n",
        "  for val in item:\n",
        "    env_subject_vals.add(val.lower())\n",
        "\n",
        "env_subject_vals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c279203-528f-4362-c3bd-731a3ba4bc49",
        "id": "57z5Pp2A4H88"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'humanities',\n",
              " 'language arts',\n",
              " 'other',\n",
              " 'psychomotor',\n",
              " 'psychomotor skills',\n",
              " 'stem',\n",
              " 'unspecified'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_subject_map = {\n",
        "    'humanities':\"HUM\",\n",
        "    'language arts':\"HUM\",\n",
        "    'other':\"OTH\",\n",
        "    'psychomotor':\"PSY\",\n",
        "    'psychomotor skills':\"PSY\",\n",
        "    'stem':\"STEM\",\n",
        "    'unspecified':\"UNSP\"\n",
        "}"
      ],
      "metadata": {
        "id": "1iAjUX594H8-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_env_subject_list = []\n",
        "\n",
        "for item in env_subject_list:\n",
        "  new_item = set()\n",
        "  for val in item:\n",
        "    new_item.add(env_subject_map[val.lower()])\n",
        "  new_env_subject_list.append(list(new_item))\n",
        "\n",
        "new_env_subject_list[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d5c8875-cb90-4195-cf6d-adcff7caea34",
        "id": "zQT-ez9F4H8-"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['STEM'], ['STEM'], ['STEM'], ['STEM'], ['STEM']]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_env_subject_list_combined = []\n",
        "\n",
        "for item in new_env_subject_list:\n",
        "  new_item = \", \".join(item)\n",
        "  new_env_subject_list_combined.append(new_item)\n",
        "\n",
        "new_env_subject_list_combined[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3537faeb-d8b1-459e-e120-0871f5fa5bd1",
        "id": "PZaYb6IW4H8-"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['STEM', 'STEM', 'STEM', 'STEM', 'STEM']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.insert(14, \"Environment Subject (mapped)\", new_env_subject_list_combined)\n",
        "df[15:25]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6a406502-d8a0-4c34-f5b8-6bea1b2b3b5d",
        "id": "YKV6OmPd4H8_"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          UUID  \\\n",
              "15  2070224207   \n",
              "16  2070224207   \n",
              "17  2070224207   \n",
              "18  2634033325   \n",
              "19  2634033325   \n",
              "20  2634033325   \n",
              "21  3051560548   \n",
              "22  3051560548   \n",
              "23  3051560548   \n",
              "24  3339002981   \n",
              "\n",
              "                                                                                                          Title  \\\n",
              "15                                detecting medical simulation errors with machine learning and multimodal data   \n",
              "16                                detecting medical simulation errors with machine learning and multimodal data   \n",
              "17                                detecting medical simulation errors with machine learning and multimodal data   \n",
              "18  controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "19  controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "20  controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "21                              temporal analysis of multimodal data to predict collaborative learning outcomes   \n",
              "22                              temporal analysis of multimodal data to predict collaborative learning outcomes   \n",
              "23                              temporal analysis of multimodal data to predict collaborative learning outcomes   \n",
              "24              estimation of success in collaborative learning based on multimodal learning analytics features   \n",
              "\n",
              "   Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "15    Daniele Di Mitri  2019                                Training   \n",
              "16    Daniele Di Mitri  2019                                Training   \n",
              "17    Daniele Di Mitri  2019                                Training   \n",
              "18        Xavier Ochoa  2020                                Training   \n",
              "19        Xavier Ochoa  2020                                Training   \n",
              "20        Xavier Ochoa  2020                                Training   \n",
              "21   Jennifer K. Olsen  2020                                Learning   \n",
              "22   Jennifer K. Olsen  2020                                Learning   \n",
              "23   Jennifer K. Olsen  2020                                Learning   \n",
              "24       Daniel Spikol  2017                                Learning   \n",
              "\n",
              "   Mapped Data Collection Mediums          Mapped Modalities  \\\n",
              "15              VIDEO,MOTION,LOGS                  POSE,LOGS   \n",
              "16              VIDEO,MOTION,LOGS                  POSE,LOGS   \n",
              "17              VIDEO,MOTION,LOGS                  POSE,LOGS   \n",
              "18                VIDEO,AUDIO,PPA              POSE,PROS,PPA   \n",
              "19                VIDEO,AUDIO,PPA              POSE,PROS,PPA   \n",
              "20                VIDEO,AUDIO,PPA              POSE,PROS,PPA   \n",
              "21                 LOGS,AUDIO,EYE  GAZE,LOGS,PROS,TRANS,QUAL   \n",
              "22                 LOGS,AUDIO,EYE  GAZE,LOGS,PROS,TRANS,QUAL   \n",
              "23                 LOGS,AUDIO,EYE  GAZE,LOGS,PROS,TRANS,QUAL   \n",
              "24           EYE,LOGS,VIDEO,AUDIO        GAZE,LOGS,PROS,POSE   \n",
              "\n",
              "   Mapped Analysis Methods Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "15                     CLS                 MID                       CAIM   \n",
              "16                     CLS                 MID                       CAIM   \n",
              "17                     CLS                 MID                       CAIM   \n",
              "18                   STATS                 OTH                       BJET   \n",
              "19                   STATS                 OTH                       BJET   \n",
              "20                   STATS                 OTH                       BJET   \n",
              "21                     REG                 MID                       BJET   \n",
              "22                     REG                 MID                       BJET   \n",
              "23                     REG                 MID                       BJET   \n",
              "24                     CLS                 MID                      ICALT   \n",
              "\n",
              "                                       Mapped Full Publication  Sort Number  \\\n",
              "15           Conference on Artificial Intelligence in Medicine           11   \n",
              "16           Conference on Artificial Intelligence in Medicine           11   \n",
              "17           Conference on Artificial Intelligence in Medicine           11   \n",
              "18                   British Journal of Educational Technology           12   \n",
              "19                   British Journal of Educational Technology           12   \n",
              "20                   British Journal of Educational Technology           12   \n",
              "21                   British Journal of Educational Technology           13   \n",
              "22                   British Journal of Educational Technology           13   \n",
              "23                   British Journal of Educational Technology           13   \n",
              "24  International Conference on Advanced Learning Technologies           14   \n",
              "\n",
              "   Environment Setting     Environment Sub Environment Subject (mapped)  \\\n",
              "15                BLND  psychomotor skills                          PSY   \n",
              "16                BLND  psychomotor skills                          PSY   \n",
              "17                BLND  psychomotor skills                          PSY   \n",
              "18                BLND          humanities                          HUM   \n",
              "19                BLND          humanities                          HUM   \n",
              "20                BLND          humanities                          HUM   \n",
              "21                VIRT                STEM                         STEM   \n",
              "22                VIRT                STEM                         STEM   \n",
              "23                VIRT                STEM                         STEM   \n",
              "24                VIRT                STEM                         STEM   \n",
              "\n",
              "   Participant Structure Didactic Nature Level of Instruction or Training  \\\n",
              "15            individual        training                    undergraduate   \n",
              "16            individual        training                    undergraduate   \n",
              "17            individual        training                    undergraduate   \n",
              "18            individual        informal                      unspecified   \n",
              "19            individual        training                      unspecified   \n",
              "20            individual        training                      unspecified   \n",
              "21          multi-person   instructional                             K-12   \n",
              "22          multi-person   instructional                             K-12   \n",
              "23          multi-person   instructional                             K-12   \n",
              "24          multi-person   instructional                    undergraduate   \n",
              "\n",
              "   Analysis Approach  \\\n",
              "15       model-based   \n",
              "16       model-based   \n",
              "17       model-based   \n",
              "18        model-free   \n",
              "19        model-free   \n",
              "20        model-free   \n",
              "21       model-based   \n",
              "22       model-based   \n",
              "23       model-based   \n",
              "24       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "15  Used each Chest Compression as training sample by masking/windowing of the original time series, then trained an LSTM network with all these samples and were able to classify accurately the target classes, however discarding the rest of the time-series they were not able to detect if a CC happened. Author asks Doctorial Consortium how, given the available data, could they train a classifier able to detect whether a CC happened or not.   \n",
              "16                                                                                                                                                                                                                                                                                                                               Trained an LSTM to predict ['too slow', 'on-point', 'too fast'] for Chest compression training. Achieved 70-75% accuracy.   \n",
              "17                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
              "18                      Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoidance of filled pauses)   \n",
              "19                                                                                                                                                                                                                                                                                                                                 Authors showcase that the training tool improved manually defined scores between an initial and second use of the tool.   \n",
              "20                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
              "21                                                                                                                                                Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.   \n",
              "22                                                                                                                                                                                                                                                                       Evaluating how multimodal features contribute to a model's performance to predict learning gains. Audio features introduce noise that negatively impacted the error of the model.   \n",
              "23                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
              "24                                                                                                                                          Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, authors found that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.   \n",
              "\n",
              "   Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "15                     Joyce        1            NaN  \n",
              "16                   Eduardo        2            NaN  \n",
              "17             Joyce/Eduardo      1&2            NaN  \n",
              "18                     Joyce        1            NaN  \n",
              "19                   Eduardo        2            NaN  \n",
              "20             Joyce/Eduardo      1&2            NaN  \n",
              "21                     Joyce        1            NaN  \n",
              "22                   Eduardo        2            NaN  \n",
              "23             Joyce/Eduardo      1&2            NaN  \n",
              "24                     Joyce        1            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9e79f2f5-02ca-457d-8e28-3840e20ec3b6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Sub</th>\n",
              "      <th>Environment Subject (mapped)</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2070224207</td>\n",
              "      <td>detecting medical simulation errors with machine learning and multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,LOGS</td>\n",
              "      <td>POSE,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>CAIM</td>\n",
              "      <td>Conference on Artificial Intelligence in Medicine</td>\n",
              "      <td>11</td>\n",
              "      <td>BLND</td>\n",
              "      <td>psychomotor skills</td>\n",
              "      <td>PSY</td>\n",
              "      <td>individual</td>\n",
              "      <td>training</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Used each Chest Compression as training sample by masking/windowing of the original time series, then trained an LSTM network with all these samples and were able to classify accurately the target classes, however discarding the rest of the time-series they were not able to detect if a CC happened. Author asks Doctorial Consortium how, given the available data, could they train a classifier able to detect whether a CC happened or not.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2070224207</td>\n",
              "      <td>detecting medical simulation errors with machine learning and multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,LOGS</td>\n",
              "      <td>POSE,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>CAIM</td>\n",
              "      <td>Conference on Artificial Intelligence in Medicine</td>\n",
              "      <td>11</td>\n",
              "      <td>BLND</td>\n",
              "      <td>psychomotor skills</td>\n",
              "      <td>PSY</td>\n",
              "      <td>individual</td>\n",
              "      <td>training</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Trained an LSTM to predict ['too slow', 'on-point', 'too fast'] for Chest compression training. Achieved 70-75% accuracy.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2070224207</td>\n",
              "      <td>detecting medical simulation errors with machine learning and multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,LOGS</td>\n",
              "      <td>POSE,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>CAIM</td>\n",
              "      <td>Conference on Artificial Intelligence in Medicine</td>\n",
              "      <td>11</td>\n",
              "      <td>BLND</td>\n",
              "      <td>psychomotor skills</td>\n",
              "      <td>PSY</td>\n",
              "      <td>individual</td>\n",
              "      <td>training</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>12</td>\n",
              "      <td>BLND</td>\n",
              "      <td>humanities</td>\n",
              "      <td>HUM</td>\n",
              "      <td>individual</td>\n",
              "      <td>informal</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-free</td>\n",
              "      <td>Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoidance of filled pauses)</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>12</td>\n",
              "      <td>BLND</td>\n",
              "      <td>humanities</td>\n",
              "      <td>HUM</td>\n",
              "      <td>individual</td>\n",
              "      <td>training</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-free</td>\n",
              "      <td>Authors showcase that the training tool improved manually defined scores between an initial and second use of the tool.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>12</td>\n",
              "      <td>BLND</td>\n",
              "      <td>humanities</td>\n",
              "      <td>HUM</td>\n",
              "      <td>individual</td>\n",
              "      <td>training</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-free</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3051560548</td>\n",
              "      <td>temporal analysis of multimodal data to predict collaborative learning outcomes</td>\n",
              "      <td>Jennifer K. Olsen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,EYE</td>\n",
              "      <td>GAZE,LOGS,PROS,TRANS,QUAL</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>13</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3051560548</td>\n",
              "      <td>temporal analysis of multimodal data to predict collaborative learning outcomes</td>\n",
              "      <td>Jennifer K. Olsen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,EYE</td>\n",
              "      <td>GAZE,LOGS,PROS,TRANS,QUAL</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>13</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Evaluating how multimodal features contribute to a model's performance to predict learning gains. Audio features introduce noise that negatively impacted the error of the model.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3051560548</td>\n",
              "      <td>temporal analysis of multimodal data to predict collaborative learning outcomes</td>\n",
              "      <td>Jennifer K. Olsen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,EYE</td>\n",
              "      <td>GAZE,LOGS,PROS,TRANS,QUAL</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>13</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>3339002981</td>\n",
              "      <td>estimation of success in collaborative learning based on multimodal learning analytics features</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,LOGS,VIDEO,AUDIO</td>\n",
              "      <td>GAZE,LOGS,PROS,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICALT</td>\n",
              "      <td>International Conference on Advanced Learning Technologies</td>\n",
              "      <td>14</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, authors found that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e79f2f5-02ca-457d-8e28-3840e20ec3b6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9e79f2f5-02ca-457d-8e28-3840e20ec3b6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9e79f2f5-02ca-457d-8e28-3840e20ec3b6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-452b0a1f-c39c-4855-84dd-a73e55c0217b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-452b0a1f-c39c-4855-84dd-a73e55c0217b')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-452b0a1f-c39c-4855-84dd-a73e55c0217b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=[\"Environment Sub\"], inplace=True)\n",
        "df.rename(columns={'Environment Subject (mapped)': 'Environment Subject'},inplace=True)\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "78DzU2sltP8c",
        "outputId": "697ab1f8-0b4e-417d-bbaa-1516d2ce16cb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums Mapped Modalities Mapped Analysis Methods  \\\n",
              "0                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "1                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "2                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "3             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "4             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "\n",
              "  Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0                LATE                     MLPALA   \n",
              "1                LATE                     MLPALA   \n",
              "2                LATE                     MLPALA   \n",
              "3              HYBRID                       BJET   \n",
              "4              HYBRID                       BJET   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "\n",
              "  Environment Setting Environment Subject     Participant Structure  \\\n",
              "0                PHYS                STEM  individual, multi-person   \n",
              "1                PHYS                STEM  individual, multi-person   \n",
              "2                PHYS                STEM  individual, multi-person   \n",
              "3                PHYS                STEM              multi-person   \n",
              "4                PHYS                STEM              multi-person   \n",
              "\n",
              "  Didactic Nature Level of Instruction or Training Analysis Approach  \\\n",
              "0   instructional                      unspecified       model-based   \n",
              "1   instructional                      unspecified       model-based   \n",
              "2   instructional                      unspecified       model-based   \n",
              "3   instructional                             K-12       model-based   \n",
              "4   instructional                             K-12       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-09a50a7b-f1e8-4aac-b088-16c5de89797c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09a50a7b-f1e8-4aac-b088-16c5de89797c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-09a50a7b-f1e8-4aac-b088-16c5de89797c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-09a50a7b-f1e8-4aac-b088-16c5de89797c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-30e31295-3a56-49d9-86ad-a78a8d3bb5e0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-30e31295-3a56-49d9-86ad-a78a8d3bb5e0')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-30e31295-3a56-49d9-86ad-a78a8d3bb5e0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Participant Structure"
      ],
      "metadata": {
        "id": "N5bV35kitRJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "participant_vals = set()\n",
        "participant_list = [l.split(\", \") for l in list(df[\"Participant Structure\"])]\n",
        "for item in participant_list:\n",
        "  for val in item:\n",
        "    participant_vals.add(val.lower())\n",
        "\n",
        "participant_vals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77f560e9-093f-458e-aec8-5a27a892587b",
        "id": "pztdO6nB8Yve"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'individual', 'multi-person', 'multi-student', 'mutli-person'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "participant_map = {\n",
        "    'individual':\"IND\",\n",
        "    'multi-person':\"MULTI\",\n",
        "    'multi-student':\"MULTI\",\n",
        "    'mutli-person':\"MULTI\"\n",
        "}"
      ],
      "metadata": {
        "id": "U4-JSx0k8Yvf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_participant_list = []\n",
        "\n",
        "for item in participant_list:\n",
        "  new_item = set()\n",
        "  for val in item:\n",
        "    new_item.add(participant_map[val.lower()])\n",
        "  new_participant_list.append(list(new_item))\n",
        "\n",
        "new_participant_list[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20dd11a2-5eaa-4607-cad0-50d69ddda2bc",
        "id": "Tl5T9KqP8Yvg"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['IND', 'MULTI'], ['IND', 'MULTI'], ['IND', 'MULTI'], ['MULTI'], ['MULTI']]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_participant_list_combined = []\n",
        "\n",
        "for item in new_participant_list:\n",
        "  new_item = \", \".join(item)\n",
        "  new_participant_list_combined.append(new_item)\n",
        "\n",
        "new_participant_list_combined[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "566276ca-08d5-45fb-84ab-00f1f41c74ce",
        "id": "IQMwaqq58Yvg"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['IND, MULTI', 'IND, MULTI', 'IND, MULTI', 'MULTI', 'MULTI']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.insert(15, \"Participant Structure (mapped)\", new_participant_list_combined)\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d92bc47c-2dfb-4003-fe46-55af8cb0f2e7",
        "id": "lqQoAUD78Yvg"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "5  1469065963   \n",
              "6  1598166515   \n",
              "7  1598166515   \n",
              "8  1598166515   \n",
              "9  1877483551   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "5  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "6                                                            multimodal learning analytics for game-based learning   \n",
              "7                                                            multimodal learning analytics for game-based learning   \n",
              "8                                                            multimodal learning analytics for game-based learning   \n",
              "9                           motion-based educational games: using multi-modal data to predict player’s performance   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "5            Andy Nguyen  2022                                Learning   \n",
              "6         Andrew Emerson  2020                                Learning   \n",
              "7         Andrew Emerson  2020                                Learning   \n",
              "8         Andrew Emerson  2020                                Learning   \n",
              "9     Serena Lee-Cultura  2020                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums         Mapped Modalities  \\\n",
              "0                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "1                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "2                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "3             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "4             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "5             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "6                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "7                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "8                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "9               VIDEO,EYE,SENSOR  PULSE,TEMP,EDA,GAZE,POSE   \n",
              "\n",
              "  Mapped Analysis Methods Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0               CLS,CLUST                LATE                     MLPALA   \n",
              "1               CLS,CLUST                LATE                     MLPALA   \n",
              "2               CLS,CLUST                LATE                     MLPALA   \n",
              "3          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "4          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "5          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "6               CLS,STATS                 MID                       BJET   \n",
              "7               CLS,STATS                 MID                       BJET   \n",
              "8               CLS,STATS                 MID                       BJET   \n",
              "9                     CLS                 MID                        COG   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "5                   British Journal of Educational Technology            4   \n",
              "6                   British Journal of Educational Technology            5   \n",
              "7                   British Journal of Educational Technology            5   \n",
              "8                   British Journal of Educational Technology            5   \n",
              "9                                    IEEE Conference on Games            6   \n",
              "\n",
              "  Environment Setting Environment Subject     Participant Structure  \\\n",
              "0                PHYS                STEM  individual, multi-person   \n",
              "1                PHYS                STEM  individual, multi-person   \n",
              "2                PHYS                STEM  individual, multi-person   \n",
              "3                PHYS                STEM              multi-person   \n",
              "4                PHYS                STEM              multi-person   \n",
              "5                PHYS                STEM              multi-person   \n",
              "6                VIRT                STEM                individual   \n",
              "7                VIRT                STEM                individual   \n",
              "8                VIRT                STEM                individual   \n",
              "9                BLND                STEM                individual   \n",
              "\n",
              "  Participant Structure (mapped) Didactic Nature  \\\n",
              "0                     IND, MULTI   instructional   \n",
              "1                     IND, MULTI   instructional   \n",
              "2                     IND, MULTI   instructional   \n",
              "3                          MULTI   instructional   \n",
              "4                          MULTI   instructional   \n",
              "5                          MULTI   instructional   \n",
              "6                            IND        informal   \n",
              "7                            IND        informal   \n",
              "8                            IND        informal   \n",
              "9                            IND   instructional   \n",
              "\n",
              "  Level of Instruction or Training Analysis Approach  \\\n",
              "0                      unspecified       model-based   \n",
              "1                      unspecified       model-based   \n",
              "2                      unspecified       model-based   \n",
              "3                             K-12       model-based   \n",
              "4                             K-12       model-based   \n",
              "5                             K-12       model-based   \n",
              "6                    undergraduate       model-based   \n",
              "7                    undergraduate       model-based   \n",
              "8                    undergraduate       model-based   \n",
              "9                             K-12       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "6                                                                                                       Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. The findings suggest that MMLA can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding   \n",
              "7                                                                                                                                                                                                                                                                                                                                                              Common case of multimodal outperform unimodal models, through the addition of gaze to classify student's posttest performance and interest.   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "9                                                                                                                                                                                           Authors conclude that the feature combination of gaze and physiological MMD provide the most accurate predictions of correct answers. They also show the feasibility of early prediction of children's performance by using half (as oppose to full) data lengths to extract features and predict correctness.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  \n",
              "5             Joyce/Eduardo      1&2            NaN  \n",
              "6                     Joyce        1            NaN  \n",
              "7                   Eduardo        2            NaN  \n",
              "8             Joyce/Eduardo      1&2            NaN  \n",
              "9                     Joyce        1            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-61abcf3a-f825-46ab-af93-af1a7bf7a113\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Participant Structure (mapped)</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual, multi-person</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>multi-person</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual</td>\n",
              "      <td>IND</td>\n",
              "      <td>informal</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. The findings suggest that MMLA can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual</td>\n",
              "      <td>IND</td>\n",
              "      <td>informal</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Common case of multimodal outperform unimodal models, through the addition of gaze to classify student's posttest performance and interest.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual</td>\n",
              "      <td>IND</td>\n",
              "      <td>informal</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1877483551</td>\n",
              "      <td>motion-based educational games: using multi-modal data to predict player’s performance</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR</td>\n",
              "      <td>PULSE,TEMP,EDA,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>COG</td>\n",
              "      <td>IEEE Conference on Games</td>\n",
              "      <td>6</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>individual</td>\n",
              "      <td>IND</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Authors conclude that the feature combination of gaze and physiological MMD provide the most accurate predictions of correct answers. They also show the feasibility of early prediction of children's performance by using half (as oppose to full) data lengths to extract features and predict correctness.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61abcf3a-f825-46ab-af93-af1a7bf7a113')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-61abcf3a-f825-46ab-af93-af1a7bf7a113 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-61abcf3a-f825-46ab-af93-af1a7bf7a113');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b524bf24-5221-4f6a-9e47-24a54beda306\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b524bf24-5221-4f6a-9e47-24a54beda306')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b524bf24-5221-4f6a-9e47-24a54beda306 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=[\"Participant Structure\"], inplace=True)\n",
        "df.rename(columns={'Participant Structure (mapped)': 'Participant Structure'},inplace=True)\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f20fda3-ac2c-4181-ad06-c6baaadb9505",
        "id": "AbuYCPAp8Yvh"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums Mapped Modalities Mapped Analysis Methods  \\\n",
              "0                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "1                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "2                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "3             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "4             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "\n",
              "  Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0                LATE                     MLPALA   \n",
              "1                LATE                     MLPALA   \n",
              "2                LATE                     MLPALA   \n",
              "3              HYBRID                       BJET   \n",
              "4              HYBRID                       BJET   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "\n",
              "  Environment Setting Environment Subject Participant Structure  \\\n",
              "0                PHYS                STEM            IND, MULTI   \n",
              "1                PHYS                STEM            IND, MULTI   \n",
              "2                PHYS                STEM            IND, MULTI   \n",
              "3                PHYS                STEM                 MULTI   \n",
              "4                PHYS                STEM                 MULTI   \n",
              "\n",
              "  Didactic Nature Level of Instruction or Training Analysis Approach  \\\n",
              "0   instructional                      unspecified       model-based   \n",
              "1   instructional                      unspecified       model-based   \n",
              "2   instructional                      unspecified       model-based   \n",
              "3   instructional                             K-12       model-based   \n",
              "4   instructional                             K-12       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d05f80ee-7732-410b-adfc-871a4034d7d5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d05f80ee-7732-410b-adfc-871a4034d7d5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d05f80ee-7732-410b-adfc-871a4034d7d5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d05f80ee-7732-410b-adfc-871a4034d7d5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f4632039-6b9c-460b-80e7-f1a60e94c5d4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f4632039-6b9c-460b-80e7-f1a60e94c5d4')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f4632039-6b9c-460b-80e7-f1a60e94c5d4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Didactic Nature"
      ],
      "metadata": {
        "id": "FnISC1-4tUsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "didactic_nature_vals = set()\n",
        "didactic_nature_list = [l.split(\", \") for l in list(df[\"Didactic Nature\"])]\n",
        "for item in didactic_nature_list:\n",
        "  for val in item:\n",
        "    didactic_nature_vals.add(val.lower())\n",
        "\n",
        "didactic_nature_vals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "452f2399-2049-4620-aab1-25ba818d5e94",
        "id": "qDvGwBXSBgns"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'informal',\n",
              " 'instructional',\n",
              " 'instrutional',\n",
              " 'insturctional',\n",
              " 'training',\n",
              " 'unspecified'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "didactic_nature_map = {\n",
        "    'informal':\"INF\",\n",
        "    'instructional':\"INSTR\",\n",
        "    'instrutional':\"INSTR\",\n",
        "    'insturctional':\"INSTR\",\n",
        "    'training':\"TRAIN\",\n",
        "    'unspecified':\"UNSP\"\n",
        "}"
      ],
      "metadata": {
        "id": "6Qv_m16LBgnt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_didactic_nature_list = []\n",
        "\n",
        "for item in didactic_nature_list:\n",
        "  new_item = set()\n",
        "  for val in item:\n",
        "    new_item.add(didactic_nature_map[val.lower()])\n",
        "  new_didactic_nature_list.append(list(new_item))\n",
        "\n",
        "new_didactic_nature_list[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "928892b1-f462-4734-d5ef-f1950a260e35",
        "id": "GP9sTeVgBgnt"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['INSTR'], ['INSTR'], ['INSTR'], ['INSTR'], ['INSTR']]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_didactic_nature_list_combined = []\n",
        "\n",
        "for item in new_didactic_nature_list:\n",
        "  new_item = \", \".join(item)\n",
        "  new_didactic_nature_list_combined.append(new_item)\n",
        "\n",
        "new_didactic_nature_list_combined[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bacdec15-ae3e-423a-b57c-8c0af0418da2",
        "id": "V_ccHOQCBgnt"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['INSTR', 'INSTR', 'INSTR', 'INSTR', 'INSTR']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.insert(16, \"Didactic Nature (mapped)\", new_didactic_nature_list_combined)\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0880953d-8917-4627-ed1c-b8c5535738db",
        "id": "rwITA26kBgnu"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "5  1469065963   \n",
              "6  1598166515   \n",
              "7  1598166515   \n",
              "8  1598166515   \n",
              "9  1877483551   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "5  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "6                                                            multimodal learning analytics for game-based learning   \n",
              "7                                                            multimodal learning analytics for game-based learning   \n",
              "8                                                            multimodal learning analytics for game-based learning   \n",
              "9                           motion-based educational games: using multi-modal data to predict player’s performance   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "5            Andy Nguyen  2022                                Learning   \n",
              "6         Andrew Emerson  2020                                Learning   \n",
              "7         Andrew Emerson  2020                                Learning   \n",
              "8         Andrew Emerson  2020                                Learning   \n",
              "9     Serena Lee-Cultura  2020                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums         Mapped Modalities  \\\n",
              "0                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "1                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "2                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "3             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "4             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "5             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "6                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "7                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "8                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "9               VIDEO,EYE,SENSOR  PULSE,TEMP,EDA,GAZE,POSE   \n",
              "\n",
              "  Mapped Analysis Methods Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0               CLS,CLUST                LATE                     MLPALA   \n",
              "1               CLS,CLUST                LATE                     MLPALA   \n",
              "2               CLS,CLUST                LATE                     MLPALA   \n",
              "3          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "4          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "5          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "6               CLS,STATS                 MID                       BJET   \n",
              "7               CLS,STATS                 MID                       BJET   \n",
              "8               CLS,STATS                 MID                       BJET   \n",
              "9                     CLS                 MID                        COG   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "5                   British Journal of Educational Technology            4   \n",
              "6                   British Journal of Educational Technology            5   \n",
              "7                   British Journal of Educational Technology            5   \n",
              "8                   British Journal of Educational Technology            5   \n",
              "9                                    IEEE Conference on Games            6   \n",
              "\n",
              "  Environment Setting Environment Subject Participant Structure  \\\n",
              "0                PHYS                STEM            IND, MULTI   \n",
              "1                PHYS                STEM            IND, MULTI   \n",
              "2                PHYS                STEM            IND, MULTI   \n",
              "3                PHYS                STEM                 MULTI   \n",
              "4                PHYS                STEM                 MULTI   \n",
              "5                PHYS                STEM                 MULTI   \n",
              "6                VIRT                STEM                   IND   \n",
              "7                VIRT                STEM                   IND   \n",
              "8                VIRT                STEM                   IND   \n",
              "9                BLND                STEM                   IND   \n",
              "\n",
              "  Didactic Nature Didactic Nature (mapped) Level of Instruction or Training  \\\n",
              "0   instructional                    INSTR                      unspecified   \n",
              "1   instructional                    INSTR                      unspecified   \n",
              "2   instructional                    INSTR                      unspecified   \n",
              "3   instructional                    INSTR                             K-12   \n",
              "4   instructional                    INSTR                             K-12   \n",
              "5   instructional                    INSTR                             K-12   \n",
              "6        informal                      INF                    undergraduate   \n",
              "7        informal                      INF                    undergraduate   \n",
              "8        informal                      INF                    undergraduate   \n",
              "9   instructional                    INSTR                             K-12   \n",
              "\n",
              "  Analysis Approach  \\\n",
              "0       model-based   \n",
              "1       model-based   \n",
              "2       model-based   \n",
              "3       model-based   \n",
              "4       model-based   \n",
              "5       model-based   \n",
              "6       model-based   \n",
              "7       model-based   \n",
              "8       model-based   \n",
              "9       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "6                                                                                                       Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. The findings suggest that MMLA can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding   \n",
              "7                                                                                                                                                                                                                                                                                                                                                              Common case of multimodal outperform unimodal models, through the addition of gaze to classify student's posttest performance and interest.   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "9                                                                                                                                                                                           Authors conclude that the feature combination of gaze and physiological MMD provide the most accurate predictions of correct answers. They also show the feasibility of early prediction of children's performance by using half (as oppose to full) data lengths to extract features and predict correctness.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  \n",
              "5             Joyce/Eduardo      1&2            NaN  \n",
              "6                     Joyce        1            NaN  \n",
              "7                   Eduardo        2            NaN  \n",
              "8             Joyce/Eduardo      1&2            NaN  \n",
              "9                     Joyce        1            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-91d2a6d6-3ef3-4adb-afb2-0164db055218\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Didactic Nature (mapped)</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>instructional</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>informal</td>\n",
              "      <td>INF</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. The findings suggest that MMLA can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>informal</td>\n",
              "      <td>INF</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Common case of multimodal outperform unimodal models, through the addition of gaze to classify student's posttest performance and interest.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>informal</td>\n",
              "      <td>INF</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1877483551</td>\n",
              "      <td>motion-based educational games: using multi-modal data to predict player’s performance</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR</td>\n",
              "      <td>PULSE,TEMP,EDA,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>COG</td>\n",
              "      <td>IEEE Conference on Games</td>\n",
              "      <td>6</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>instructional</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Authors conclude that the feature combination of gaze and physiological MMD provide the most accurate predictions of correct answers. They also show the feasibility of early prediction of children's performance by using half (as oppose to full) data lengths to extract features and predict correctness.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91d2a6d6-3ef3-4adb-afb2-0164db055218')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-91d2a6d6-3ef3-4adb-afb2-0164db055218 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-91d2a6d6-3ef3-4adb-afb2-0164db055218');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-86228b8e-8495-41c9-a243-31e2a144808d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-86228b8e-8495-41c9-a243-31e2a144808d')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-86228b8e-8495-41c9-a243-31e2a144808d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=[\"Didactic Nature\"], inplace=True)\n",
        "df.rename(columns={'Didactic Nature (mapped)': 'Didactic Nature'},inplace=True)\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5dcd083d-21a0-4ce1-aef3-fc0c8bcc8011",
        "id": "6JeUhJEyBgnu"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums Mapped Modalities Mapped Analysis Methods  \\\n",
              "0                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "1                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "2                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "3             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "4             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "\n",
              "  Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0                LATE                     MLPALA   \n",
              "1                LATE                     MLPALA   \n",
              "2                LATE                     MLPALA   \n",
              "3              HYBRID                       BJET   \n",
              "4              HYBRID                       BJET   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "\n",
              "  Environment Setting Environment Subject Participant Structure  \\\n",
              "0                PHYS                STEM            IND, MULTI   \n",
              "1                PHYS                STEM            IND, MULTI   \n",
              "2                PHYS                STEM            IND, MULTI   \n",
              "3                PHYS                STEM                 MULTI   \n",
              "4                PHYS                STEM                 MULTI   \n",
              "\n",
              "  Didactic Nature Level of Instruction or Training Analysis Approach  \\\n",
              "0           INSTR                      unspecified       model-based   \n",
              "1           INSTR                      unspecified       model-based   \n",
              "2           INSTR                      unspecified       model-based   \n",
              "3           INSTR                             K-12       model-based   \n",
              "4           INSTR                             K-12       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6ad03495-73b5-4f1c-9fce-ee8d358f53f1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K-12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ad03495-73b5-4f1c-9fce-ee8d358f53f1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6ad03495-73b5-4f1c-9fce-ee8d358f53f1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6ad03495-73b5-4f1c-9fce-ee8d358f53f1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3446e5fd-587f-4f82-ad73-6949861aff89\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3446e5fd-587f-4f82-ad73-6949861aff89')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3446e5fd-587f-4f82-ad73-6949861aff89 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Level of Instruction or Training"
      ],
      "metadata": {
        "id": "bWm_2QFGtZMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "level_vals = set()\n",
        "level_list = [l.split(\", \") for l in list(df[\"Level of Instruction or Training\"])]\n",
        "for item in level_list:\n",
        "  for val in item:\n",
        "    level_vals.add(val.lower())\n",
        "\n",
        "level_vals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1679d7a3-44ea-45cb-dc4b-443f882f0310",
        "id": "g2apl3flFGtL"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'graduate',\n",
              " 'k-12',\n",
              " 'k12',\n",
              " 'professional',\n",
              " 'professional development',\n",
              " 'undergraduate',\n",
              " 'undisclosed',\n",
              " 'university',\n",
              " 'unspecified'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "level_map = {\n",
        "    'graduate':\"UNI\",\n",
        "    'k-12':\"K12\",\n",
        "    'k12':\"K12\",\n",
        "    'professional':\"PROF\",\n",
        "    'professional development':\"PROF\",\n",
        "    'undergraduate':\"UNI\",\n",
        "    'undisclosed':\"UNSP\",\n",
        "    'university':\"UNI\",\n",
        "    'unspecified':\"UNSP\"\n",
        "}"
      ],
      "metadata": {
        "id": "-HO79ErmFGtM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_level_list = []\n",
        "\n",
        "for item in level_list:\n",
        "  new_item = set()\n",
        "  for val in item:\n",
        "    new_item.add(level_map[val.lower()])\n",
        "  new_level_list.append(list(new_item))\n",
        "\n",
        "new_level_list[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f04acc13-dc63-4144-aa5b-07aed5bb523d",
        "id": "lh0wbfmAFGtM"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['UNSP'], ['UNSP'], ['UNSP'], ['K12'], ['K12']]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_level_list_combined = []\n",
        "\n",
        "for item in new_level_list:\n",
        "  new_item = \", \".join(item)\n",
        "  new_level_list_combined.append(new_item)\n",
        "\n",
        "new_level_list_combined[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d36114b4-c1c3-4d40-a82a-b6ebbbf6565c",
        "id": "V5kcXZSvFGtN"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['UNSP', 'UNSP', 'UNSP', 'K12', 'K12']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.insert(17, \"Level of Instruction or Training (mapped)\", new_level_list_combined)\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3f2bd79-cc0e-4522-e09e-6776343eca01",
        "id": "u1RL2FJeFGtN"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "5  1469065963   \n",
              "6  1598166515   \n",
              "7  1598166515   \n",
              "8  1598166515   \n",
              "9  1877483551   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "5  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "6                                                            multimodal learning analytics for game-based learning   \n",
              "7                                                            multimodal learning analytics for game-based learning   \n",
              "8                                                            multimodal learning analytics for game-based learning   \n",
              "9                           motion-based educational games: using multi-modal data to predict player’s performance   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "5            Andy Nguyen  2022                                Learning   \n",
              "6         Andrew Emerson  2020                                Learning   \n",
              "7         Andrew Emerson  2020                                Learning   \n",
              "8         Andrew Emerson  2020                                Learning   \n",
              "9     Serena Lee-Cultura  2020                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums         Mapped Modalities  \\\n",
              "0                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "1                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "2                    VIDEO,AUDIO            POSE,GAZE,PROS   \n",
              "3             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "4             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "5             VIDEO,AUDIO,SENSOR                  QUAL,EDA   \n",
              "6                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "7                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "8                 VIDEO,LOGS,EYE      AFFECT,GAZE,LOGS,PPA   \n",
              "9               VIDEO,EYE,SENSOR  PULSE,TEMP,EDA,GAZE,POSE   \n",
              "\n",
              "  Mapped Analysis Methods Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0               CLS,CLUST                LATE                     MLPALA   \n",
              "1               CLS,CLUST                LATE                     MLPALA   \n",
              "2               CLS,CLUST                LATE                     MLPALA   \n",
              "3          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "4          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "5          PATT,CLS,CLUST              HYBRID                       BJET   \n",
              "6               CLS,STATS                 MID                       BJET   \n",
              "7               CLS,STATS                 MID                       BJET   \n",
              "8               CLS,STATS                 MID                       BJET   \n",
              "9                     CLS                 MID                        COG   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "5                   British Journal of Educational Technology            4   \n",
              "6                   British Journal of Educational Technology            5   \n",
              "7                   British Journal of Educational Technology            5   \n",
              "8                   British Journal of Educational Technology            5   \n",
              "9                                    IEEE Conference on Games            6   \n",
              "\n",
              "  Environment Setting Environment Subject Participant Structure  \\\n",
              "0                PHYS                STEM            IND, MULTI   \n",
              "1                PHYS                STEM            IND, MULTI   \n",
              "2                PHYS                STEM            IND, MULTI   \n",
              "3                PHYS                STEM                 MULTI   \n",
              "4                PHYS                STEM                 MULTI   \n",
              "5                PHYS                STEM                 MULTI   \n",
              "6                VIRT                STEM                   IND   \n",
              "7                VIRT                STEM                   IND   \n",
              "8                VIRT                STEM                   IND   \n",
              "9                BLND                STEM                   IND   \n",
              "\n",
              "  Didactic Nature Level of Instruction or Training  \\\n",
              "0           INSTR                      unspecified   \n",
              "1           INSTR                      unspecified   \n",
              "2           INSTR                      unspecified   \n",
              "3           INSTR                             K-12   \n",
              "4           INSTR                             K-12   \n",
              "5           INSTR                             K-12   \n",
              "6             INF                    undergraduate   \n",
              "7             INF                    undergraduate   \n",
              "8             INF                    undergraduate   \n",
              "9           INSTR                             K-12   \n",
              "\n",
              "  Level of Instruction or Training (mapped) Analysis Approach  \\\n",
              "0                                      UNSP       model-based   \n",
              "1                                      UNSP       model-based   \n",
              "2                                      UNSP       model-based   \n",
              "3                                       K12       model-based   \n",
              "4                                       K12       model-based   \n",
              "5                                       K12       model-based   \n",
              "6                                       UNI       model-based   \n",
              "7                                       UNI       model-based   \n",
              "8                                       UNI       model-based   \n",
              "9                                       K12       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "6                                                                                                       Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. The findings suggest that MMLA can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding   \n",
              "7                                                                                                                                                                                                                                                                                                                                                              Common case of multimodal outperform unimodal models, through the addition of gaze to classify student's posttest performance and interest.   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "9                                                                                                                                                                                           Authors conclude that the feature combination of gaze and physiological MMD provide the most accurate predictions of correct answers. They also show the feasibility of early prediction of children's performance by using half (as oppose to full) data lengths to extract features and predict correctness.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  \n",
              "5             Joyce/Eduardo      1&2            NaN  \n",
              "6                     Joyce        1            NaN  \n",
              "7                   Eduardo        2            NaN  \n",
              "8             Joyce/Eduardo      1&2            NaN  \n",
              "9                     Joyce        1            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-71c6d6cd-2f46-4066-8b9b-c9925a2dd937\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Level of Instruction or Training (mapped)</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>unspecified</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K-12</td>\n",
              "      <td>K12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K-12</td>\n",
              "      <td>K12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K-12</td>\n",
              "      <td>K12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>UNI</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. The findings suggest that MMLA can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>UNI</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Common case of multimodal outperform unimodal models, through the addition of gaze to classify student's posttest performance and interest.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>5</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>undergraduate</td>\n",
              "      <td>UNI</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1877483551</td>\n",
              "      <td>motion-based educational games: using multi-modal data to predict player’s performance</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR</td>\n",
              "      <td>PULSE,TEMP,EDA,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>COG</td>\n",
              "      <td>IEEE Conference on Games</td>\n",
              "      <td>6</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K-12</td>\n",
              "      <td>K12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Authors conclude that the feature combination of gaze and physiological MMD provide the most accurate predictions of correct answers. They also show the feasibility of early prediction of children's performance by using half (as oppose to full) data lengths to extract features and predict correctness.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71c6d6cd-2f46-4066-8b9b-c9925a2dd937')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-71c6d6cd-2f46-4066-8b9b-c9925a2dd937 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-71c6d6cd-2f46-4066-8b9b-c9925a2dd937');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-adf915a9-ee56-41ba-abcc-119a9750eec6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-adf915a9-ee56-41ba-abcc-119a9750eec6')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-adf915a9-ee56-41ba-abcc-119a9750eec6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=[\"Level of Instruction or Training\"], inplace=True)\n",
        "df.rename(columns={'Level of Instruction or Training (mapped)': 'Level of Instruction or Training'},inplace=True)\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6f51c76b-7d0e-4c69-dba4-a98f1de0e982",
        "id": "0AjtquG0FGtO"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums Mapped Modalities Mapped Analysis Methods  \\\n",
              "0                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "1                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "2                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "3             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "4             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "\n",
              "  Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0                LATE                     MLPALA   \n",
              "1                LATE                     MLPALA   \n",
              "2                LATE                     MLPALA   \n",
              "3              HYBRID                       BJET   \n",
              "4              HYBRID                       BJET   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "\n",
              "  Environment Setting Environment Subject Participant Structure  \\\n",
              "0                PHYS                STEM            IND, MULTI   \n",
              "1                PHYS                STEM            IND, MULTI   \n",
              "2                PHYS                STEM            IND, MULTI   \n",
              "3                PHYS                STEM                 MULTI   \n",
              "4                PHYS                STEM                 MULTI   \n",
              "\n",
              "  Didactic Nature Level of Instruction or Training Analysis Approach  \\\n",
              "0           INSTR                             UNSP       model-based   \n",
              "1           INSTR                             UNSP       model-based   \n",
              "2           INSTR                             UNSP       model-based   \n",
              "3           INSTR                              K12       model-based   \n",
              "4           INSTR                              K12       model-based   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-71d6a3d4-9ead-4d09-bedb-e3a27bd5ef06\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>model-based</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71d6a3d4-9ead-4d09-bedb-e3a27bd5ef06')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-71d6a3d4-9ead-4d09-bedb-e3a27bd5ef06 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-71d6a3d4-9ead-4d09-bedb-e3a27bd5ef06');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-38e9fa62-6ac8-4fca-8bcb-044d49b979c0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-38e9fa62-6ac8-4fca-8bcb-044d49b979c0')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-38e9fa62-6ac8-4fca-8bcb-044d49b979c0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis Approach"
      ],
      "metadata": {
        "id": "cTliFsnntcM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analysis_vals = set()\n",
        "analysis_list = [l.split(\", \") for l in list(df[\"Analysis Approach\"])]\n",
        "for item in analysis_list:\n",
        "  for val in item:\n",
        "    analysis_vals.add(val.lower())\n",
        "\n",
        "analysis_vals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f75f96e1-1572-495d-f99c-8934135325ef",
        "id": "aBHrBPl8LRGv"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mixed',\n",
              " 'model based',\n",
              " 'model free',\n",
              " 'model-based',\n",
              " 'model-free',\n",
              " 'uses 3rd party model but for augmenting qualitative'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analysis_map = {\n",
        "    'mixed':\"MB, MF\",\n",
        "    'model based':\"MB\",\n",
        "    'model free':\"MF\",\n",
        "    'model-based':\"MB\",\n",
        "    'model-free':\"MF\",\n",
        "    'uses 3rd party model but for augmenting qualitative':\"MB\"\n",
        "}"
      ],
      "metadata": {
        "id": "UdTjLge3LRGz"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_analysis_list = []\n",
        "\n",
        "for item in analysis_list:\n",
        "  new_item = set()\n",
        "  for val in item:\n",
        "    new_item.add(analysis_map[val.lower()])\n",
        "  new_analysis_list.append(list(new_item))\n",
        "\n",
        "new_analysis_list[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a47b4c3-d82c-44ae-d320-c182f4b96fb7",
        "id": "yUT2MuH4LRG0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['MB'], ['MB'], ['MB'], ['MB'], ['MB']]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_analysis_list_combined = []\n",
        "\n",
        "for item in new_analysis_list:\n",
        "  new_item = \", \".join(item)\n",
        "  new_analysis_list_combined.append(new_item)\n",
        "\n",
        "new_analysis_list_combined[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3811713-4475-4e0a-edcb-11d2c063b72f",
        "id": "kPJRVFRpLRG0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['MB', 'MB', 'MB', 'MB', 'MB']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.insert(18, \"Analysis Approach (mapped)\", new_analysis_list_combined)\n",
        "df[15:25]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "678734f8-28d3-45e3-a350-262f1ccbe158",
        "id": "ApmfgKOvLRG1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          UUID  \\\n",
              "15  2070224207   \n",
              "16  2070224207   \n",
              "17  2070224207   \n",
              "18  2634033325   \n",
              "19  2634033325   \n",
              "20  2634033325   \n",
              "21  3051560548   \n",
              "22  3051560548   \n",
              "23  3051560548   \n",
              "24  3339002981   \n",
              "\n",
              "                                                                                                          Title  \\\n",
              "15                                detecting medical simulation errors with machine learning and multimodal data   \n",
              "16                                detecting medical simulation errors with machine learning and multimodal data   \n",
              "17                                detecting medical simulation errors with machine learning and multimodal data   \n",
              "18  controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "19  controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "20  controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "21                              temporal analysis of multimodal data to predict collaborative learning outcomes   \n",
              "22                              temporal analysis of multimodal data to predict collaborative learning outcomes   \n",
              "23                              temporal analysis of multimodal data to predict collaborative learning outcomes   \n",
              "24              estimation of success in collaborative learning based on multimodal learning analytics features   \n",
              "\n",
              "   Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "15    Daniele Di Mitri  2019                                Training   \n",
              "16    Daniele Di Mitri  2019                                Training   \n",
              "17    Daniele Di Mitri  2019                                Training   \n",
              "18        Xavier Ochoa  2020                                Training   \n",
              "19        Xavier Ochoa  2020                                Training   \n",
              "20        Xavier Ochoa  2020                                Training   \n",
              "21   Jennifer K. Olsen  2020                                Learning   \n",
              "22   Jennifer K. Olsen  2020                                Learning   \n",
              "23   Jennifer K. Olsen  2020                                Learning   \n",
              "24       Daniel Spikol  2017                                Learning   \n",
              "\n",
              "   Mapped Data Collection Mediums          Mapped Modalities  \\\n",
              "15              VIDEO,MOTION,LOGS                  POSE,LOGS   \n",
              "16              VIDEO,MOTION,LOGS                  POSE,LOGS   \n",
              "17              VIDEO,MOTION,LOGS                  POSE,LOGS   \n",
              "18                VIDEO,AUDIO,PPA              POSE,PROS,PPA   \n",
              "19                VIDEO,AUDIO,PPA              POSE,PROS,PPA   \n",
              "20                VIDEO,AUDIO,PPA              POSE,PROS,PPA   \n",
              "21                 LOGS,AUDIO,EYE  GAZE,LOGS,PROS,TRANS,QUAL   \n",
              "22                 LOGS,AUDIO,EYE  GAZE,LOGS,PROS,TRANS,QUAL   \n",
              "23                 LOGS,AUDIO,EYE  GAZE,LOGS,PROS,TRANS,QUAL   \n",
              "24           EYE,LOGS,VIDEO,AUDIO        GAZE,LOGS,PROS,POSE   \n",
              "\n",
              "   Mapped Analysis Methods Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "15                     CLS                 MID                       CAIM   \n",
              "16                     CLS                 MID                       CAIM   \n",
              "17                     CLS                 MID                       CAIM   \n",
              "18                   STATS                 OTH                       BJET   \n",
              "19                   STATS                 OTH                       BJET   \n",
              "20                   STATS                 OTH                       BJET   \n",
              "21                     REG                 MID                       BJET   \n",
              "22                     REG                 MID                       BJET   \n",
              "23                     REG                 MID                       BJET   \n",
              "24                     CLS                 MID                      ICALT   \n",
              "\n",
              "                                       Mapped Full Publication  Sort Number  \\\n",
              "15           Conference on Artificial Intelligence in Medicine           11   \n",
              "16           Conference on Artificial Intelligence in Medicine           11   \n",
              "17           Conference on Artificial Intelligence in Medicine           11   \n",
              "18                   British Journal of Educational Technology           12   \n",
              "19                   British Journal of Educational Technology           12   \n",
              "20                   British Journal of Educational Technology           12   \n",
              "21                   British Journal of Educational Technology           13   \n",
              "22                   British Journal of Educational Technology           13   \n",
              "23                   British Journal of Educational Technology           13   \n",
              "24  International Conference on Advanced Learning Technologies           14   \n",
              "\n",
              "   Environment Setting Environment Subject Participant Structure  \\\n",
              "15                BLND                 PSY                   IND   \n",
              "16                BLND                 PSY                   IND   \n",
              "17                BLND                 PSY                   IND   \n",
              "18                BLND                 HUM                   IND   \n",
              "19                BLND                 HUM                   IND   \n",
              "20                BLND                 HUM                   IND   \n",
              "21                VIRT                STEM                 MULTI   \n",
              "22                VIRT                STEM                 MULTI   \n",
              "23                VIRT                STEM                 MULTI   \n",
              "24                VIRT                STEM                 MULTI   \n",
              "\n",
              "   Didactic Nature Level of Instruction or Training Analysis Approach  \\\n",
              "15           TRAIN                              UNI       model-based   \n",
              "16           TRAIN                              UNI       model-based   \n",
              "17           TRAIN                              UNI       model-based   \n",
              "18             INF                             UNSP        model-free   \n",
              "19           TRAIN                             UNSP        model-free   \n",
              "20           TRAIN                             UNSP        model-free   \n",
              "21           INSTR                              K12       model-based   \n",
              "22           INSTR                              K12       model-based   \n",
              "23           INSTR                              K12       model-based   \n",
              "24           INSTR                              UNI       model-based   \n",
              "\n",
              "   Analysis Approach (mapped)  \\\n",
              "15                         MB   \n",
              "16                         MB   \n",
              "17                         MB   \n",
              "18                         MF   \n",
              "19                         MF   \n",
              "20                         MF   \n",
              "21                         MB   \n",
              "22                         MB   \n",
              "23                         MB   \n",
              "24                         MB   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "15  Used each Chest Compression as training sample by masking/windowing of the original time series, then trained an LSTM network with all these samples and were able to classify accurately the target classes, however discarding the rest of the time-series they were not able to detect if a CC happened. Author asks Doctorial Consortium how, given the available data, could they train a classifier able to detect whether a CC happened or not.   \n",
              "16                                                                                                                                                                                                                                                                                                                               Trained an LSTM to predict ['too slow', 'on-point', 'too fast'] for Chest compression training. Achieved 70-75% accuracy.   \n",
              "17                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
              "18                      Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoidance of filled pauses)   \n",
              "19                                                                                                                                                                                                                                                                                                                                 Authors showcase that the training tool improved manually defined scores between an initial and second use of the tool.   \n",
              "20                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
              "21                                                                                                                                                Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.   \n",
              "22                                                                                                                                                                                                                                                                       Evaluating how multimodal features contribute to a model's performance to predict learning gains. Audio features introduce noise that negatively impacted the error of the model.   \n",
              "23                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
              "24                                                                                                                                          Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, authors found that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.   \n",
              "\n",
              "   Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "15                     Joyce        1            NaN  \n",
              "16                   Eduardo        2            NaN  \n",
              "17             Joyce/Eduardo      1&2            NaN  \n",
              "18                     Joyce        1            NaN  \n",
              "19                   Eduardo        2            NaN  \n",
              "20             Joyce/Eduardo      1&2            NaN  \n",
              "21                     Joyce        1            NaN  \n",
              "22                   Eduardo        2            NaN  \n",
              "23             Joyce/Eduardo      1&2            NaN  \n",
              "24                     Joyce        1            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-41a42623-e965-4d25-ad6d-3e5aab3071c6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Approach (mapped)</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2070224207</td>\n",
              "      <td>detecting medical simulation errors with machine learning and multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,LOGS</td>\n",
              "      <td>POSE,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>CAIM</td>\n",
              "      <td>Conference on Artificial Intelligence in Medicine</td>\n",
              "      <td>11</td>\n",
              "      <td>BLND</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>model-based</td>\n",
              "      <td>MB</td>\n",
              "      <td>Used each Chest Compression as training sample by masking/windowing of the original time series, then trained an LSTM network with all these samples and were able to classify accurately the target classes, however discarding the rest of the time-series they were not able to detect if a CC happened. Author asks Doctorial Consortium how, given the available data, could they train a classifier able to detect whether a CC happened or not.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2070224207</td>\n",
              "      <td>detecting medical simulation errors with machine learning and multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,LOGS</td>\n",
              "      <td>POSE,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>CAIM</td>\n",
              "      <td>Conference on Artificial Intelligence in Medicine</td>\n",
              "      <td>11</td>\n",
              "      <td>BLND</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>model-based</td>\n",
              "      <td>MB</td>\n",
              "      <td>Trained an LSTM to predict ['too slow', 'on-point', 'too fast'] for Chest compression training. Achieved 70-75% accuracy.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2070224207</td>\n",
              "      <td>detecting medical simulation errors with machine learning and multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,LOGS</td>\n",
              "      <td>POSE,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>CAIM</td>\n",
              "      <td>Conference on Artificial Intelligence in Medicine</td>\n",
              "      <td>11</td>\n",
              "      <td>BLND</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>model-based</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>12</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>model-free</td>\n",
              "      <td>MF</td>\n",
              "      <td>Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoidance of filled pauses)</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>12</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>model-free</td>\n",
              "      <td>MF</td>\n",
              "      <td>Authors showcase that the training tool improved manually defined scores between an initial and second use of the tool.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>12</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>model-free</td>\n",
              "      <td>MF</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3051560548</td>\n",
              "      <td>temporal analysis of multimodal data to predict collaborative learning outcomes</td>\n",
              "      <td>Jennifer K. Olsen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,EYE</td>\n",
              "      <td>GAZE,LOGS,PROS,TRANS,QUAL</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>13</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>MB</td>\n",
              "      <td>Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3051560548</td>\n",
              "      <td>temporal analysis of multimodal data to predict collaborative learning outcomes</td>\n",
              "      <td>Jennifer K. Olsen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,EYE</td>\n",
              "      <td>GAZE,LOGS,PROS,TRANS,QUAL</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>13</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>MB</td>\n",
              "      <td>Evaluating how multimodal features contribute to a model's performance to predict learning gains. Audio features introduce noise that negatively impacted the error of the model.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3051560548</td>\n",
              "      <td>temporal analysis of multimodal data to predict collaborative learning outcomes</td>\n",
              "      <td>Jennifer K. Olsen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,EYE</td>\n",
              "      <td>GAZE,LOGS,PROS,TRANS,QUAL</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>13</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>model-based</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>3339002981</td>\n",
              "      <td>estimation of success in collaborative learning based on multimodal learning analytics features</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,LOGS,VIDEO,AUDIO</td>\n",
              "      <td>GAZE,LOGS,PROS,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICALT</td>\n",
              "      <td>International Conference on Advanced Learning Technologies</td>\n",
              "      <td>14</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>model-based</td>\n",
              "      <td>MB</td>\n",
              "      <td>Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, authors found that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41a42623-e965-4d25-ad6d-3e5aab3071c6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-41a42623-e965-4d25-ad6d-3e5aab3071c6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-41a42623-e965-4d25-ad6d-3e5aab3071c6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cacb38ad-c6e6-43cd-98d3-9a816852e84c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cacb38ad-c6e6-43cd-98d3-9a816852e84c')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cacb38ad-c6e6-43cd-98d3-9a816852e84c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=[\"Analysis Approach\"], inplace=True)\n",
        "df.rename(columns={'Analysis Approach (mapped)': 'Analysis Approach'},inplace=True)\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d2654d2-ee74-4a1b-d2b6-49caf7c23f38",
        "id": "MrxpbYtRLRG2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums Mapped Modalities Mapped Analysis Methods  \\\n",
              "0                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "1                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "2                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "3             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "4             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "\n",
              "  Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0                LATE                     MLPALA   \n",
              "1                LATE                     MLPALA   \n",
              "2                LATE                     MLPALA   \n",
              "3              HYBRID                       BJET   \n",
              "4              HYBRID                       BJET   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "\n",
              "  Environment Setting Environment Subject Participant Structure  \\\n",
              "0                PHYS                STEM            IND, MULTI   \n",
              "1                PHYS                STEM            IND, MULTI   \n",
              "2                PHYS                STEM            IND, MULTI   \n",
              "3                PHYS                STEM                 MULTI   \n",
              "4                PHYS                STEM                 MULTI   \n",
              "\n",
              "  Didactic Nature Level of Instruction or Training Analysis Approach  \\\n",
              "0           INSTR                             UNSP                MB   \n",
              "1           INSTR                             UNSP                MB   \n",
              "2           INSTR                             UNSP                MB   \n",
              "3           INSTR                              K12                MB   \n",
              "4           INSTR                              K12                MB   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                                                                                                                   The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                                                                                                          Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.   \n",
              "4                                                                                                             The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-876d9f87-08c4-43cf-bbc6-b963ccbdcac5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-876d9f87-08c4-43cf-bbc6-b963ccbdcac5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-876d9f87-08c4-43cf-bbc6-b963ccbdcac5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-876d9f87-08c4-43cf-bbc6-b963ccbdcac5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a71309d1-e031-4070-9dc7-dd5eb7e1a208\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a71309d1-e031-4070-9dc7-dd5eb7e1a208')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a71309d1-e031-4070-9dc7-dd5eb7e1a208 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify New Columns"
      ],
      "metadata": {
        "id": "GZ2_pJfuLDct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Environment Setting:\", set(df[\"Environment Setting\"]))\n",
        "print(\"Environment Subject\", set(df[\"Environment Subject\"]))\n",
        "print(\"Participant Structure\", set(df[\"Participant Structure\"]))\n",
        "print(\"Didactic Nature\", set(df[\"Didactic Nature\"]))\n",
        "print(\"Level of Instruction or Training\", set(df[\"Level of Instruction or Training\"]))\n",
        "print(\"Analysis Approach\", set(df[\"Analysis Approach\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKTz6-vRLGEB",
        "outputId": "c4795141-1253-404a-ce9f-71b44d20438e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment Setting: {'VIRT', 'BLND', 'VIRT, PHYS', 'UNSP', 'PHYS'}\n",
            "Environment Subject {'HUM, OTH, STEM', 'HUM, STEM', 'STEM', 'OTH', 'HUM', 'UNSP', 'PSY'}\n",
            "Participant Structure {'IND', 'IND, MULTI', 'MULTI'}\n",
            "Didactic Nature {'TRAIN, INSTR', 'INSTR', 'TRAIN', 'UNSP', 'INF'}\n",
            "Level of Instruction or Training {'UNI', 'UNI, K12', 'PROF', 'UNI, UNSP', 'K12', 'UNI, PROF', 'UNSP'}\n",
            "Analysis Approach {'MB, MF', 'MF, MB', 'MF', 'MB'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save IRR/Consensus File"
      ],
      "metadata": {
        "id": "9dteGBJ6LGXR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RZA6eF36PGFK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D-Cq5XF0PGHq"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-EfKYF6vPGKN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vJPzryE7PGMc"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CseAVCrNPGVB"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}