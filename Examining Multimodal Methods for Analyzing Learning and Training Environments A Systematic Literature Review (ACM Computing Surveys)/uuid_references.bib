
@inproceedings{2181637610,
	address = {London, UK},
	title = {Toward {Using} {Multi}-{Modal} {Learning} {Analytics} to {Support} and {Measure} {Collaboration} in {Co}-{Located} {Dyads}},
	abstract = {This paper describes an empirical study where the productive interactions of small collaborative learning groups in response to two collaboration interventions were evaluated through traditional and multi-modal data collection methods. We asked 42 pairs (N= 84) of participants to program a robot to solve a series of mazes. Participants had no prior programming experience, and we used a block-based environment with pre-made functions as well as video tutorials to scaffold the activity. We explored 2 interventions to support their collaboration: a real-time visualization of their verbal contribution and a short verbal explanation of the benefits of collaboration for learning. This paper describes our experimental design, the effect of the interventions, preliminary results from the Kinect sensor, and our future plans to analyze additional sensor data. We conclude by highlighting the importance of capturing and supporting 21st century skills (i.e., collaboration and effective communication) in small groups of students.},
	language = {en},
	booktitle = {{ICLS} 2018},
	publisher = {International Society of the Learning Sciences},
	author = {Starr, Emma L and Reilly, Joseph M and Schneider, Bertrand},
	year = {2018},
	pages = {448--455},
}

@inproceedings{1118315889,
	address = {Philadelphia, PA USA},
	title = {Using {Multimodal} {Learning} {Analytics} to {Identify} {Aspects} of {Collaboration} in {Project}-{Based} {Learning}},
	volume = {1},
	abstract = {Collaborative learning activities are a key part of education and are part of many common teaching approaches including problem-based learning, inquiry-based learning, and project-based learning. However, in open-ended collaborative small group work where learners make unique solutions to tasks that involve robotics, electronics, programming, and design artefacts evidence on the effectiveness of using these learning activities are hard to find. The paper argues that multimodal learning analytics (MMLA) can offer novel methods that can generate unique information about what happens when students are engaged in collaborative, project-based learning activities. Through the use of multimodal learning analytics platform, we collected various streams of data, processed and extracted multimodal interactions to answer the following question: which features of MMLA are good predictors of collaborative problem-solving in open-ended tasks in project-based learning? Manual entered scores of CPS were regressed using machine-learning methods. The answer to the question provides potential ways to automatically identify aspects of collaboration in projectbased learning.},
	language = {en},
	booktitle = {Making a {Difference}: {Prioritizing} {Equity} and {Access} in {CSCL}},
	publisher = {International Society of the Learning Sciences},
	author = {Spikol, Daniel and Ruffaldi, Emanuele and Cukurova, Mutlu},
	year = {2017},
	pages = {263--270},
}

@inproceedings{3308658121,
	address = {Buffalo, NY, USA},
	title = {Exploring {Collaboration} {Using} {Motion} {Sensors} and {Multi}- {Modal} {Learning} {Analytics}},
	abstract = {In this paper, we describe the analysis of multimodal data collected on small collaborative learning groups. In a previous study [1], we asked pairs (N=84) with no programming experience to program a robot to solve a series of mazes. The quality of the dyad’s collaboration was evaluated, and two interventions were implemented to support collaborative learning. In the current study, we present the analysis of KinectTM and speech data gathered on dyads during the programming task. We first show how certain movements and patterns of gestures correlate positively with collaboration and learning gains. We next use clustering algorithms to find prototypical body positions of participants and relate amount of time spent in certain postures with learning gains as in Schneider \& Blikstein’s work [2]. Finally, we examine measures of proxemics and physical orientation within the dyads to explore how to detect good collaboration. We discuss the relevance of these results to designing and assessing collaborative small group activities and outline future work related to other collected sensor data.},
	language = {en},
	booktitle = {Proceedings of the 11th {International} {Conference} on {Educational} {Data} {Mining}},
	publisher = {International Educational Data Mining Society},
	author = {Reilly, Joseph M and Ravenell, Milan and Schneider, Bertrand},
	month = jul,
	year = {2018},
	pages = {333--339},
}

@inproceedings{85990093,
	address = {Stockholm, Sweden},
	title = {Multimodal {Markers} of {Persuasive} {Speech}: {Designing} a {Virtual} {Debate} {Coach}},
	shorttitle = {Multimodal {Markers} of {Persuasive} {Speech}},
	url = {https://www.isca-speech.org/archive/interspeech_2017/petukhova17_interspeech.html},
	doi = {10.21437/Interspeech.2017-98},
	abstract = {The study presented in this paper is carried out to support debate performance assessment in the context of debate skills training. The perception of good performance as a debater is inﬂuenced by how believable and convincing the debater’s argumentation is. We identiﬁed a number of features that are useful for explaining perceived properties of persuasive speech and for deﬁning rules and strategies to produce and assess debate performance. We collected and analysed multimodal and multisensory data of the trainees debate behaviour, and contrasted it with those of skilled professional debaters. Observational, correlation and machine learning studies were performed to identify multimodal markers of persuasive speech and link them to experts’ assessments. A combination of multimodal in- and outof-domain debate data, and various non-verbal, prosodic, lexical, linguistic and structural features has been computed based on our analysis and assessed used to , and several classiﬁcation procedures has been applied achieving an accuracy of 0.79 on spoken debate data.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Petukhova, Volha and Raju, Manoj and Bunt, Harry},
	month = aug,
	year = {2017},
	pages = {142--146},
}

@inproceedings{2070224207,
	address = {Poznan, Poland},
	title = {Detecting {Medical} {Simulation} {Errors} with {Machine} learning and {Multimodal} {Data}},
	abstract = {In this doctoral consortium paper, we introduce the CPR Tutor, an intelligent tutoring system for cardiopulmonary resuscitation (CPR) training based on the analysis of multimodal data. Using a multisensor setup, the CPR Tutor tracks the CPR execution of the trainee and generates automatic adaptive feedback to improve the trainee’s performance. This research work is part of a PhD project entitled “Multimodal Tutor: adaptive feedback from multimodal experience capturing”, a project which investigates how to use multimodal and multi-sensor data to generate personalised feedback for training psycho-motor skills at the workplace or during medical simulations. In the CPR Tutor, we use Microsoft Kinect and Myo to track trainee’s body position and the ResusciAnne QCPR manikin to get correct CPR performance metrics. We then use a validated approach, the Multimodal Pipeline, for the collection, storage, processing, annotation of multimodal data. This paper describes the preliminary results obtained in the ﬁrst design of the CPR Tutor.},
	language = {en},
	booktitle = {17th {Conference} on {Artificial} {Intelligence} in {Medicine}},
	publisher = {Springer International Publishing},
	author = {Mitri, Daniele Di},
	year = {2019},
	pages = {1--6},
}

@inproceedings{518268671,
	address = {Brighton, UK},
	title = {Using {Multimodal} {Learning} {Analytics} to {Explore} {Collaboration} in a {Sustainability} {Co}-located {Tabletop} {Game}},
	abstract = {Serious Games (SGs) are particularly suitable to foster collaboration in complex domains that challenge formal education approaches. However, their effectiveness depends on their features as much as on the ability to assess their impacts on players, and analysing collaboration in games remains by and large an open problem. Research has traditionally used rich unimodal data to examine collaboration processes in games (e.g., video content analysis of verbal exchanges). Despite providing relevant semantic information, this can make data coding and analysis difficult and time-consuming. Furthermore, unimodal approaches can only partially capture complex processes defined by multiple interacting variables, such as collaboration. Recent research highlighted the potentialities offered by multimodal learning analytics (MMLA) to address these issues. MMLA integrates multiple types of data captured both in and out of the game system through different modalities to analyse complex processes. Although it has been highlighted as particularly suitable to investigate collaboration, research on MMLA in SGs is still scarce. This work contributes to the state-of-the-art by leveraging MMLA to explore collaboration indicators in a multiplayer, co-located SG for education in sustainable development. Our results corroborate the MMLA effectiveness in analysing complex collaborative dynamics, and identify key multimodal analytics useful to investigate collaboration in SGs.},
	language = {en},
	booktitle = {15th {European} {Conference} on {Game}-{Based} {Learning}},
	publisher = {Academic Conferences LTD},
	author = {López, María Ximena and Strada, Francesco and Bottino, Andrea and Fabricatore, Carlo},
	month = dec,
	year = {2021},
	pages = {482--489},
}

@article{3783339081,
	title = {A {Novel} {Method} for the {In}-{Depth} {Multimodal} {Analysis} of {Student} {Learning} {Trajectories} in {Intelligent} {Tutoring} {Systems}},
	volume = {5},
	issn = {1929-7750},
	url = {https://learning-analytics.info/index.php/JLA/article/view/5423},
	doi = {10.18608/jla.2018.51.4},
	abstract = {Temporal analyses are critical to understanding learning processes, yet understudied in education research. Data from different sources are often collected at different grain sizes, which are difficult to integrate. Making sense of data at many levels of analysis, including the most detailed levels, is highly time-consuming. In this paper, we describe a generalizable approach for more efficient yet rich sensemaking of temporal data during student use of intelligent tutoring systems. This multi-step approach involves using coarse-grain temporality — learning trajectories across knowledge components — to identify and further explore “focal” moments worthy of more finegrain, context-rich analysis. We discuss the application of this approach to data collected from a classroom study in which students engaged in a Chemistry Virtual Lab tutoring system. We show that the application of this multistep approach efficiently led to interpretable and actionable insights while making use of the richness of the available data. This method is generalizable to many types of datasets and can help handle large volumes of rich data at multiple levels of granularity. We argue that it can be a valuable approach to tackling some of the most prohibitive methodological challenges involved in temporal learning analytics.},
	language = {en},
	number = {1},
	urldate = {2023-08-07},
	journal = {Journal of Learning Analytics},
	author = {Liu, Ran and Stamper, John C and Davenport, Jodi},
	month = apr,
	year = {2018},
	pages = {41--54},
}

@inproceedings{853680639,
	address = {Montreal, CA},
	title = {Sensor-based {Data} {Fusion} for {Multimodal} {Affect} {Detection} in {Game}-based {Learning} {Environments}},
	volume = {2592},
	abstract = {Affect detection is central to educational data mining because of its potential contribution to predicting learning processes and outcomes. Using multiple modalities has been shown to increase the performance of affect detection. With the rise of sensor-based modalities due to their relatively low cost and high level of flexibility, there has been a marked increase in research efforts pertaining to sensor-based, multimodal systems for affective computing problems. In this paper, we demonstrate the impact that multimodal systems can have when using Microsoft Kinect-based posture data and electrodermal activity data for the analysis of affective states displayed by students engaged with a game-based learning environment. We compare the effectiveness of both support vector machines and deep neural networks as affect classifiers. Additionally, we evaluate different types of data fusion to determine which method for combining the separate modalities yields the highest classification rate. Results indicate that multimodal approaches outperform unimodal baseline classifiers, and feature-level concatenation offers the highest performance among the data fusion techniques.},
	language = {en},
	booktitle = {Proceedings of the {EDM} and {Games} {Workshop} at the 12th {International} {Conference} on {Educational} {Data} {Mining}},
	publisher = {International Educational Data Mining Society},
	author = {Henderson, Nathan L and Rowe, Jonathan P and Mott, Bradford W and Lester, James C},
	month = jul,
	year = {2019},
	pages = {1--7},
}

@article{483140962,
	title = {Investigating multimodal affect sensing in an {Affective} {Tutoring} {System} using unobtrusive sensors.},
	volume = {29},
	journal = {Psychology of Programming Interest Group},
	author = {{Fwa, Hua Leong} and {Lindsay Marshall}},
	month = oct,
	year = {2018},
	pages = {78--85},
}

@article{1426267857,
	title = {Affect, {Support}, and {Personal} {Factors}: {Multimodal} {Causal} {Models} of {One}-on-one {Coaching}},
	volume = {13},
	abstract = {Human one-on-one coaching involves complex multimodal interactions. Successful coaching requires teachers to closely monitor students’ cognitive-affective states and provide support of optimal type, timing, and amount. However, most of the existing human tutoring studies focus primarily on verbal interactions and have yet to incorporate the rich aspects of multimodal cognitive-affective experiences. Meanwhile, the research community lacks principled methods to fully exploit complex multimodal data to uncover the causal relationships between coaching supports, students’ cognitive-affective experiences, and their stable individual factors. We explore an analytical framework that is explainable and amenable to incorporating domain knowledge. The proposed framework combines statistical approaches in Sparse Multiple Canonical Correlation, causal discovery, and inference methods for observations. We demonstrate this framework using a multimodal one-on-one math problem-solving coaching dataset collected in naturalistic home environments involving parents and young children. The insights derived from our analyses may inform the design of effective technology-inspired interventions that are personalized and adaptive.},
	language = {en},
	number = {3},
	journal = {Journal of Educational Data Mining},
	author = {Chen, Lujie Karen},
	year = {2021},
	pages = {36--68},
}

@article{2936220551,
	title = {Multi-source and multimodal data fusion for predicting academic performance in blended learning university courses},
	volume = {89},
	issn = {00457906},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045790620307606},
	doi = {10.1016/j.compeleceng.2020.106908},
	abstract = {In this paper we apply data fusion approaches for predicting the final academic performance of university students using multiple-source, multimodal data from blended learning environments. We collect and preprocess data about first-year university students from different sources: theory classes, practical sessions, on-line Moodle sessions, and a final exam. Our objective is to discover which data fusion approach produces the best results using our data. We carry out experiments by applying four different data fusion approaches and six classification algorithms. The results show that the best predictions are produced using ensembles and selecting the best attributes approach with discretized data. The best prediction models show us that the level of attention in theory classes, scores in Moodle quizzes, and the level of activity in Moodle forums are the best set of attributes for predicting students’ final performance in our courses.},
	language = {en},
	urldate = {2023-08-07},
	journal = {Computers \& Electrical Engineering},
	author = {Chango, Wilson and Cerezo, Rebeca and Romero, Cristóbal},
	month = jan,
	year = {2021},
	pages = {106908},
}

@article{3051560548,
	title = {Temporal analysis of multimodal data to predict collaborative learning outcomes},
	volume = {51},
	issn = {0007-1013, 1467-8535},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/bjet.12982},
	doi = {10.1111/bjet.12982},
	abstract = {The analysis of multiple data streams is a long-standing practice within educational research. Both multimodal data analysis and temporal analysis have been applied successfully, but in the area of collaborative learning, very few studies have investigated specific advantages of multiple modalities versus a single modality, especially combined with temporal analysis. In this paper, we investigate how both the use of multimodal data and moving from averages and counts to temporal aspects in a collaborative setting provides a better prediction of learning gains. To address these questions, we analyze multimodal data collected from 25 9–11-year-old dyads using a fractions intelligent tutoring system. Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality. Our work contributes to the understanding of how analyzing multimodal data in temporal manner provides additional information around the collaborative learning process.},
	language = {en},
	number = {5},
	urldate = {2023-08-07},
	journal = {British Journal of Educational Technology},
	author = {Olsen, Jennifer K. and Sharma, Kshitij and Rummel, Nikol and Aleven, Vincent},
	month = sep,
	year = {2020},
	pages = {1527--1547},
}

@article{3093310941,
	title = {Embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders},
	volume = {12},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0182151},
	doi = {10.1371/journal.pone.0182151},
	abstract = {Social skills training, performed by human trainers, is a well-established method for obtaining appropriate skills in social interaction. Previous work automated the process of social skills training by developing a dialogue system that teaches social communication skills through interaction with a computer avatar. Even though previous work that simulated social skills training only considered acoustic and linguistic information, human social skills trainers take into account visual and other non-verbal features. In this paper, we create and evaluate a social skills training system that closes this gap by considering the audiovisual features of the smiling ratio and the head pose (yaw and pitch). In addition, the previous system was only tested with graduate students; in this paper, we applied our system to children or young adults with autism spectrum disorders. For our experimental evaluation, we recruited 18 members from the general population and 10 people with autism spectrum disorders and gave them our proposed multimodal system to use. An experienced human social skills trainer rated the social skills of the users. We evaluated the system’s effectiveness by comparing pre- and post-training scores and identified significant improvement in their social skills using our proposed multimodal system. Computer-based social skills training is useful for people who experience social difficulties. Such a system can be used by teachers, therapists, and social skills trainers for rehabilitation and the supplemental use of human-based training anywhere and anytime.},
	language = {en},
	number = {8},
	urldate = {2023-08-07},
	journal = {PLOS ONE},
	author = {Tanaka, Hiroki and Negoro, Hideki and Iwasaka, Hidemi and Nakamura, Satoshi},
	editor = {Sakakibara, Manabu},
	month = aug,
	year = {2017},
	pages = {e0182151},
}

@article{3095923626,
	title = {A {Multimodal} {Analysis} of {Making}},
	volume = {28},
	issn = {1560-4292, 1560-4306},
	url = {http://link.springer.com/10.1007/s40593-017-0160-1},
	doi = {10.1007/s40593-017-0160-1},
	abstract = {This paper presents three multimodal learning analytic approaches from a hands-on learning activity. We use video, audio, gesture and bio-physiology data from a two-condition study (N = 20), to identify correlations between the multimodal data, experimental condition, and two learning outcomes: design quality and learning. The three approaches incorporate: 1) human-annotated coding of video data, 2) automated coding of gesture, audio and bio-physiological data and, 3) concatenated humanannotated and automatically annotated data. Within each analysis we employ the same machine learning and sequence mining techniques. Ultimately we find that each approach provides different affordances depending on the similarity metric and the dependent variable. For example, the analysis based on human-annotated data found strong correlations among multimodal behaviors, experimental condition, success and learning, when we relaxed constraints on temporal similarity. The second approach performed well when comparing students’ multimodal behaviors as a time series, but was less effective using the temporally relaxed similarity metric. The take-away is that there are several strategies for doing multimodal learning analytics, and that many of these approaches can provide a meaningful glimpse into a complex data set, glimpses that may be difficult to identify using traditional approaches.},
	language = {en},
	number = {3},
	urldate = {2023-08-07},
	journal = {International Journal of Artificial Intelligence in Education},
	author = {Worsley, Marcelo and Blikstein, Paulo},
	month = sep,
	year = {2018},
	pages = {385--419},
}

@article{3135645357,
	title = {Multimodal teaching analytics: {Automated} extraction of orchestration graphs from wearable sensor data},
	volume = {34},
	issn = {02664909},
	shorttitle = {Multimodal teaching analytics},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/jcal.12232},
	doi = {10.1111/jcal.12232},
	abstract = {The pedagogical modelling of everyday classroom practice is an interesting kind of evidence, both for educational research and teachers' own professional development. This paper explores the usage of wearable sensors and machine learning techniques to automatically extract orchestration graphs (teaching activities and their social plane over time) on a dataset of 12 classroom sessions enacted by two different teachers in different classroom settings. The dataset included mobile eye‐tracking as well as audiovisual and accelerometry data from sensors worn by the teacher. We evaluated both time‐independent and timeaware models, achieving median F1 scores of about 0.7–0.8 on leave‐one‐session‐out k‐fold cross‐validation. Although these results show the feasibility of this approach, they also highlight the need for larger datasets, recorded in a wider variety of classroom settings, to provide automated tagging of classroom practice that can be used in everyday practice across multiple teachers.},
	language = {en},
	number = {2},
	urldate = {2023-08-07},
	journal = {Journal of Computer Assisted Learning},
	author = {Prieto, L.P. and Sharma, K. and Kidzinski, Ł. and Rodríguez-Triana, M.J. and Dillenbourg, P.},
	month = apr,
	year = {2018},
	pages = {193--203},
}

@article{3146393211,
	title = {Mobile {Mixed} {Reality} for {Experiential} {Learning} and {Simulation} in {Medical} and {Health} {Sciences} {Education}},
	volume = {9},
	issn = {2078-2489},
	url = {http://www.mdpi.com/2078-2489/9/2/31},
	doi = {10.3390/info9020031},
	abstract = {New accessible learning methods delivered through mobile mixed reality are becoming possible in education, shifting pedagogy from the use of two dimensional images and videos to facilitating learning via interactive mobile environments. This is especially important in medical and health education, where the required knowledge acquisition is typically much more experiential, self-directed, and hands-on than in many other disciplines. Presented are insights obtained from the implementation and testing of two mobile mixed reality interventions across two Australian higher education classrooms in medicine and health sciences, concentrating on student perceptions of mobile mixed reality for learning physiology and anatomy in a face-to-face medical and health science classroom and skills acquisition in airways management focusing on direct laryngoscopy with foreign body removal in a distance paramedic science classroom. This is unique because most studies focus on a single discipline, focusing on either skills or the learner experience and a single delivery modality rather than linking cross-discipline knowledge acquisition and the development of a student’s tangible skills across multimodal classrooms. Outcomes are presented from post-intervention student interviews and discipline academic observation, which highlight improvements in learner motivation and skills, but also demonstrated pedagogical challenges to overcome with mobile mixed reality learning.},
	language = {en},
	number = {2},
	urldate = {2023-08-07},
	journal = {Information},
	author = {Birt, James and Stromberga, Zane and Cowling, Michael and Moro, Christian},
	month = jan,
	year = {2018},
	pages = {31},
}

@inproceedings{3309250332,
	address = {Sydney New South Wales Australia},
	title = {({Dis})engagement matters: identifying efficacious learning practices with multimodal learning analytics},
	isbn = {978-1-4503-6400-3},
	shorttitle = {({Dis})engagement matters},
	url = {https://dl.acm.org/doi/10.1145/3170358.3170420},
	doi = {10.1145/3170358.3170420},
	abstract = {Video analysis is a staple of the education research community. For many contemporary education researchers, participation in the video coding process serves as a rite of passage. However, recent developments in multimodal learning analytics may help to accelerate and enhance this process by providing researchers with a more nuanced glimpse into a set of learning experiences. As an example of how to use multimodal learning analytics towards these ends, this paper includes a preliminary analysis from 54 college students, who completed two engineering design tasks in pairs. Gesture, speech and electro-dermal activation data were collected as students completed these tasks. The gesture data was used to learn a set of canonical clusters (N=4). A decision tree was trained based on individual students’ cluster frequencies, and pre-post learning gains. The nodes in the decision tree were then used to identify a subset of video segments that were human coded based on prior work in learning analytics and engineering design. The combination of machine learning and human inference helps elucidate the practices that seem to correlate with student learning. In particular, both engagement and disengagement seem to correlate with student learning, albeit in a somewhat nuanced fashion.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Learning} {Analytics} and {Knowledge}},
	publisher = {ACM},
	author = {Worsley, Marcelo},
	month = mar,
	year = {2018},
	pages = {365--369},
}

@inproceedings{3339002981,
	address = {Timisoara, Romania},
	title = {Estimation of {Success} in {Collaborative} {Learning} {Based} on {Multimodal} {Learning} {Analytics} {Features}},
	isbn = {978-1-5386-3870-5},
	url = {http://ieeexplore.ieee.org/document/8001779/},
	doi = {10.1109/ICALT.2017.122},
	abstract = {Multimodal learning analytics provides researchers new tools and techniques to capture different types of data from complex learning activities in dynamic learning environments. This paper investigates high-ﬁdelity synchronised multimodal recordings of small groups of learners interacting from diverse sensors that include computer vision, user generated content, and data from the learning objects (like physical computing components or laboratory equipment). We processed and extracted different aspects of the students’ interactions to answer the following question: which features of student group work are good predictors of team success in open-ended tasks with physical computing? The answer to the question provides ways to automatically identify the students performance during the learning activities.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {2017 {IEEE} 17th {International} {Conference} on {Advanced} {Learning} {Technologies} ({ICALT})},
	publisher = {IEEE},
	author = {Spikol, Daniel and Ruffaldi, Emanuele and Landolfi, Lorenzo and Cukurova, Mutlu},
	month = jul,
	year = {2017},
	pages = {269--273},
}

@article{3398902089,
	title = {What multimodal data can tell us about the students’ regulation of their learning process?},
	volume = {72},
	issn = {09594752},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095947521830416X},
	doi = {10.1016/j.learninstruc.2019.04.004},
	language = {en},
	urldate = {2023-08-07},
	journal = {Learning and Instruction},
	author = {Järvelä, Sanna and Malmberg, Jonna and Haataja, Eetu and Sobocinski, Marta and Kirschner, Paul A.},
	month = apr,
	year = {2021},
	pages = {101203},
}

@article{3408664396,
	title = {Multimodal {Student} {Engagement} {Recognition} in {Prosocial} {Games}},
	volume = {10},
	issn = {2475-1502, 2475-1510},
	url = {https://ieeexplore.ieee.org/document/8015151/},
	doi = {10.1109/TCIAIG.2017.2743341},
	abstract = {In this paper, we address the problem of recognizing student engagement in prosocial games by exploiting engagement cues from different input modalities. Since engagement is a multifaceted phenomenon with different dimensions, i.e., behavioral, cognitive, and affective, we propose the modeling of student engagement using real-time data from both the students and the game. More speciﬁcally, we apply body motion and facial expression analysis to identify the affective state of students, while we extract features related to their cognitive and behavioral engagement based on the analysis of their interaction with the game. For the automatic recognition of engagement, we adopt a machine learning approach based on artiﬁcial neural networks, while for the annotation of the engagement data, we introduce a novel approach based on the use of games with different degrees of challenge in conjunction with a retrospective self-reporting method. To evaluate the proposed methodology, we conducted real-life experiments in 4 classes, in 3 primary schools, with 72 students and 144 gameplay recordings in total. Experimental results show the great potential of the proposed methodology, which improves the classiﬁcation accuracy of the three distinct dimensions with a detection rate of 85\%. A detailed analysis of the role of each component of the Game Engagement Questionnaire, i.e., immersion, presence, ﬂow, and absorption, in the classiﬁcation process is also presented in this paper.},
	language = {en},
	number = {3},
	urldate = {2023-08-07},
	journal = {IEEE Transactions on Games},
	author = {Psaltis, Athanasios and Apostolakis, Konstantinos C. and Dimitropoulos, Kosmas and Daras, Petros},
	month = sep,
	year = {2018},
	pages = {292--303},
}

@inproceedings{3448122334,
	address = {Glasgow Scotland Uk},
	title = {Investigating the {Impact} of a {Real}-time, {Multimodal} {Student} {Engagement} {Analytics} {Technology} in {Authentic} {Classrooms}},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300534},
	doi = {10.1145/3290605.3300534},
	abstract = {We developed a real-time, multimodal Student Engagement Analytics Technology so that teachers can provide just-in-time personalized support to students who risk disengagement. To investigate the impact of the technology, we ran an exploratory semester-long study with a teacher in two classrooms. We used a multi-method approach consisting of a quasi-experimental design to evaluate the impact of the technology and a case study design to understand the environmental and social factors surrounding the classroom setting. The results show that the technology had a significant impact on the teacher’s classroom practices (i.e., increased scaffolding to the students) and student engagement (i.e., less boredom). These results suggest that the technology has the potential to support teachers’ role of being a coach in technology-mediated learning environments.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Aslan, Sinem and Alyuz, Nese and Tanriover, Cagri and Mete, Sinem E. and Okur, Eda and D'Mello, Sidney K. and Arslan Esme, Asli},
	month = may,
	year = {2019},
	pages = {1--12},
}

@article{3625722965,
	title = {Table {Tennis} {Tutor}: {Forehand} {Strokes} {Classification} {Based} on {Multimodal} {Data} and {Neural} {Networks}},
	volume = {21},
	issn = {1424-8220},
	shorttitle = {Table {Tennis} {Tutor}},
	url = {https://www.mdpi.com/1424-8220/21/9/3121},
	doi = {10.3390/s21093121},
	abstract = {Beginner table-tennis players require constant real-time feedback while learning the fundamental techniques. However, due to various constraints such as the mentor’s inability to be around all the time, expensive sensors and equipment for sports training, beginners are unable to get the immediate real-time feedback they need during training. Sensors have been widely used to train beginners and novices for various skills development, including psychomotor skills. Sensors enable the collection of multimodal data which can be utilised with machine learning to classify training mistakes, give feedback, and further improve the learning outcomes. In this paper, we introduce the Table Tennis Tutor (T3), a multi-sensor system consisting of a smartphone device with its built-in sensors for collecting motion data and a Microsoft Kinect for tracking body position. We focused on the forehand stroke mistake detection. We collected a dataset recording an experienced table tennis player performing 260 short forehand strokes (correct) and mimicking 250 long forehand strokes (mistake). We analysed and annotated the multimodal data for training a recurrent neural network that classiﬁes correct and incorrect strokes. To investigate the accuracy level of the aforementioned sensors, three combinations were validated in this study: smartphone sensors only, the Kinect only, and both devices combined. The results of the study show that smartphone sensors alone perform sub-par than the Kinect, but similar with better precision together with the Kinect. To further strengthen T3’s potential for training, an expert interview session was held virtually with a table tennis coach to investigate the coach’s perception of having a real-time feedback system to assist beginners during training sessions. The outcome of the interview shows positive expectations and provided more inputs that can be beneﬁcial for the future implementations of the T3.},
	language = {en},
	number = {9},
	urldate = {2023-08-07},
	journal = {Sensors},
	author = {Mat Sanusi, Khaleel Asyraaf and Mitri, Daniele Di and Limbu, Bibeg and Klemke, Roland},
	month = apr,
	year = {2021},
	pages = {3121},
}

@article{3637456466,
	title = {Impact of inquiry interventions on students in e-learning and classroom environments using affective computing framework},
	volume = {30},
	issn = {0924-1868, 1573-1391},
	url = {http://link.springer.com/10.1007/s11257-019-09254-3},
	doi = {10.1007/s11257-019-09254-3},
	abstract = {Effective teaching strategies improve the students’ learning rate within academic learning time. Inquiry-based instruction is one of the effective teaching strategies used in the classrooms. But these teaching strategies are not adapted in other learning environments like intelligent tutoring systems, including auto tutors. In this paper, we propose an automatic inquiry-based instruction teaching strategy, i.e., inquiry intervention using students’ affective states. The proposed model contains two modules: the ﬁrst module consists of the proposed framework for predicting the unobtrusive multimodal students’ affective states (teacher-centric attentive and in-attentive states) using the facial expressions, hand gestures and body postures. The second module consists of the proposed automated inquiry-based instruction teaching strategy to compare the learning outcomes with and without inquiry intervention using affective state transitions for both an individual and a group of students. The proposed system is tested on four different learning environments, namely: e-learning, ﬂipped classroom, classroom and webinar environments. Unobtrusive recognition of students’ affective states is performed using deep learning architectures. After student-independent tenfold crossvalidation, we obtained the students’ affective state classiﬁcation accuracy of 77\% and object localization accuracy of 81\% using students’ faces, hand gestures and body postures. The overall experimental results demonstrate that there is a positive correlation with r = 0.74 between students’ affective states and their performance. Proposed inquiry intervention improved the students’ performance as there is a decrease of 65\%, 43\%, 43\%, and 53\% in overall in-attentive affective state instances using the inquiry interventions in e-learning, ﬂipped classroom, classroom and webinar environments, respectively.},
	language = {en},
	number = {5},
	urldate = {2023-08-07},
	journal = {User Modeling and User-Adapted Interaction},
	author = {Ashwin, T. S. and Guddeti, Ram Mohana Reddy},
	month = nov,
	year = {2020},
	pages = {759--801},
}

@inproceedings{3660066725,
	address = {Athens Greece},
	title = {Children’s {Play} and {Problem} {Solving} in {Motion}-{Based} {Educational} {Games}: {Synergies} between {Human} {Annotations} and {Multi}-{Modal} {Data}},
	isbn = {978-1-4503-8452-0},
	shorttitle = {Children’s {Play} and {Problem} {Solving} in {Motion}-{Based} {Educational} {Games}},
	url = {https://dl.acm.org/doi/10.1145/3459990.3460702},
	doi = {10.1145/3459990.3460702},
	abstract = {Identifying and supporting children’s play and problem solving behaviour is important for designing educational technologies. This can inform feedback mechanisms to scaffold learning (provide hints or progress information), and assist facilitators (teachers, parents) in supporting children. Traditionally, researchers manually code video to dissect children’s nuanced play and problem solving behaviour. Advancements in sensing technologies and their respective MultiModal Data (MMD), afford observation of invisible states (cognitive, affective, physiological), and provide opportunities to inspect internal processes experienced during learning and play. However, limited research combines traditional video annotations and MMD to understand children’s behaviour as they interact with educational technology. To address this concern, we collected data from webcam, wristband, eye-trackers, and Kinect, as 26 children, aged 10-12, played a Motion-Based Educational Games (MBEG). Results showed significant differences in children’s experience during play and problem solving episodes, and motivate design considerations aimed to facilitate children’s interactions with MBEG.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Interaction {Design} and {Children}},
	publisher = {ACM},
	author = {Lee-Cultura, Serena and Sharma, Kshitij and Cosentino, Giulia and Papavlasopoulou, Sofia and Giannakos, Michail},
	month = jun,
	year = {2021},
	pages = {408--420},
}

@inproceedings{3754172825,
	address = {Online USA},
	title = {Detecting {Impasse} {During} {Collaborative} {Problem} {Solving} with {Multimodal} {Learning} {Analytics}},
	isbn = {978-1-4503-9573-1},
	url = {https://dl.acm.org/doi/10.1145/3506860.3506865},
	doi = {10.1145/3506860.3506865},
	abstract = {Collaborative problem solving has numerous benefits for learners, such as improving higher-level reasoning and developing critical thinking. While learners engage in collaborative activities, they often experience impasse, a potentially brief encounter with differing opinions or insufficient ideas to progress. Impasses provide valuable opportunities for learners to critically discuss the problem and re-evaluate their existing knowledge. Yet, despite the increasing research efforts on developing multimodal modeling techniques to analyze collaborative problem solving, there is limited research on detecting impasse in collaboration. This paper investigates multimodal detection of impasse by analyzing 46 middle school learners’ collaborative dialogue—including speech and facial behaviors—during a coding task. We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements in the video modality are the most significant unimodal indicators of impasse. We also trained several multimodal models and found that combining indicators from these three modalities provided the best impasse detection performance. To the best of our knowledge, this work is the first to explore multimodal modeling of impasse during the collaborative problem solving process. This line of research contributes to the development of real-time adaptive support for collaboration.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {{LAK22}: 12th {International} {Learning} {Analytics} and {Knowledge} {Conference}},
	publisher = {ACM},
	author = {Ma, Yingbo and Celepkolu, Mehmet and Boyer, Kristy Elizabeth},
	month = mar,
	year = {2022},
	pages = {45--55},
}

@article{3796180663,
	title = {Learning linkages: {Integrating} data streams of multiple modalities and timescales},
	volume = {35},
	issn = {0266-4909, 1365-2729},
	shorttitle = {Learning linkages},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/jcal.12315},
	doi = {10.1111/jcal.12315},
	abstract = {Increasingly, student work is being conducted on computers and online, producing vast amounts of learning‐related data. The educational analytics fields have produced many insights about learning based solely on tutoring systems' automatically logged data, or “log data.” But log data leave out important contextual information about the learning experience. For example, a student working at a computer might be working independently with few outside influences. Alternatively, he or she might be in a lively classroom, with other students around, talking and offering suggestions. Tools that capture these other experiences have potential to augment and complement log data. However, the collection of rich, multimodal data streams and the increased complexity and heterogeneity in the resulting data pose many challenges to researchers. Here, we present two empirical studies that take advantage of multimodal data sources to enrich our understanding of student learning. We leverage and extend quantitative models of student learning to incorporate insights derived jointly from data collected in multiple modalities (log data, video, and high‐fidelity audio) and contexts (individual vs. collaborative classroom learning). We discuss the unique benefits of multimodal data and present methods that take advantage of such benefits while easing the burden on researchers' time and effort.},
	language = {en},
	number = {1},
	urldate = {2023-08-07},
	journal = {Journal of Computer Assisted Learning},
	author = {Liu, Ran and Stamper, John and Davenport, Jodi and Crossley, Scott and McNamara, Danielle and Nzinga, Kalonji and Sherin, Bruce},
	month = feb,
	year = {2019},
	pages = {99--109},
}

@article{3796643912,
	title = {An evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities},
	volume = {51},
	issn = {0007-1013, 1467-8535},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/bjet.13010},
	doi = {10.1111/bjet.13010},
	abstract = {Artificial intelligence tools for education (AIEd) have been used to automate the provision of learning support to mainstream learners. One of the most innovative approaches in this field is the use of data and machine learning for the detection of a student’s affective state, to move them out of negative states that inhibit learning, into positive states such as engagement. In spite of their obvious potential to provide the personalisation that would give extra support for learners with intellectual disabilities, little work on AIEd systems that utilise affect recognition currently addresses this group. Our system used multimodal sensor data and machine learning to first identify three affective states linked to learning (engagement, frustration, boredom) and second determine the presentation of learning content so that the learner is maintained in an optimal affective state and rate of learning is maximised. To evaluate this adaptive learning system, 67 participants aged between 6 and 18 years acting as their own control took part in a series of sessions using the system. Sessions alternated between using the system with both affect detection and learning achievement to drive the selection of learning content (intervention) and using learning achievement alone (control) to drive the selection of learning content. Lack of boredom was the state with the strongest link to achievement, with both frustration and engagement positively related to achievement. There was significantly more engagement and less boredom in intervention than control sessions, but no significant difference in achievement. These results suggest that engagement does increase when activities are tailored to the personal needs and emotional state of the learner and that the system was promoting affective states that in turn promote learning. However, longer exposure is necessary to determine the effect on learning.},
	language = {en},
	number = {5},
	urldate = {2023-08-07},
	journal = {British Journal of Educational Technology},
	author = {Standen, Penelope J. and Brown, David J. and Taheri, Mohammad and Galvez Trigo, Maria J. and Boulton, Helen and Burton, Andrew and Hallewell, Madeline J. and Lathe, James G. and Shopland, Nicholas and Blanco Gonzalez, Maria A. and Kwiatkowska, Gosia M. and Milli, Elena and Cobello, Stefano and Mazzucato, Annaleda and Traversi, Marco and Hortal, Enrique},
	month = sep,
	year = {2020},
	pages = {1748--1765},
}

@article{3856280479,
	title = {Children’s play and problem-solving in motion-based learning technologies using a multi-modal mixed methods approach},
	volume = {31},
	issn = {22128689},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2212868921000647},
	doi = {10.1016/j.ijcci.2021.100355},
	abstract = {Motion-Based Learning Technologies (MBLT) offer a promising approach for integrating play and problem-solving behaviour within children’s learning. The proliferation of sensor technology has driven the field of learning technology towards the development of tools and methods that may benefit from the produced Multi-Modal Data (MMD). Such data can be used to uncover cognitive, affective and physiological processes during learning activities. Combining MMD with more traditionally exercised assessment tools, such as video content analysis, provides a more holistic understanding of children’s learning experiences and has the potential to enable the design of educational technologies capable of harmonising children’s cognitive, affective and physiological processes, while promoting appropriately balanced play and problem-solving efforts. However, the use of an MMD mixed methods approach that combines qualitative and MMD data to understand children’s behaviours during engagement with MBLT is rather unexplored. We present an in-situ study where 26 children, ages 10–12, solved a motion-based sorting task for learning geometry. We continuously and unobtrusively monitored children’s learning experiences using MMD collection via eye-trackers, wristbands, Kinect joint tracking, and a web camera. We devised SP3, a novel observational scheme that can be used to understand children’s solo interactions with MBLT, and applied it to identify and extract children’s evoked play and problem-solving behaviour. Collective analysis of the MMD and video codes provided explanations of children’s task performance through consideration of their holistic learning experience. Lastly, we applied predictive modelling to identify the synergies between various MMD measurements and children’s play and problem-solving behaviours. This research sheds light on the opportunities offered in the confluence of video coding (a traditional method in learning sciences) and MMD (an emerging method that leverages sensors proliferation) for investigating children’s behaviour with MBLT.},
	language = {en},
	urldate = {2023-08-07},
	journal = {International Journal of Child-Computer Interaction},
	author = {Lee-Cultura, Serena and Sharma, Kshitij and Giannakos, Michail},
	month = mar,
	year = {2022},
	pages = {100355},
}

@article{4019205162,
	title = {Introducing {Low}-{Cost} {Sensors} into the {Classroom} {Settings}: {Improving} the {Assessment} in {Agile} {Practices} with {Multimodal} {Learning} {Analytics}},
	volume = {19},
	issn = {1424-8220},
	shorttitle = {Introducing {Low}-{Cost} {Sensors} into the {Classroom} {Settings}},
	url = {https://www.mdpi.com/1424-8220/19/15/3291},
	doi = {10.3390/s19153291},
	abstract = {Currently, the improvement of core skills appears as one of the most signiﬁcant educational challenges of this century. However, assessing the development of such skills is still a challenge in real classroom environments. In this context, Multimodal Learning Analysis techniques appear as an attractive alternative to complement the development and evaluation of core skills. This article presents an exploratory study that analyzes the collaboration and communication of students in a Software Engineering course, who perform a learning activity simulating Scrum with Lego R bricks. Data from the Scrum process was captured, and multidirectional microphones were used in the retrospective ceremonies. Social network analysis techniques were applied, and a correlational analysis was carried out with all the registered information. The results obtained allowed the detection of important relationships and characteristics of the collaborative and Non-Collaborative groups, with productivity, effort, and predominant personality styles in the groups. From all the above, we can conclude that the Multimodal Learning Analysis techniques offer considerable feasibilities to support the process of skills development in students.},
	language = {en},
	number = {15},
	urldate = {2023-08-07},
	journal = {Sensors},
	author = {Cornide-Reyes, Hector and Noël, René and Riquelme, Fabián and Gajardo, Matías and Cechinel, Cristian and Mac Lean, Roberto and Becerra, Carlos and Villarroel, Rodolfo and Munoz, Roberto},
	month = jul,
	year = {2019},
	pages = {3291},
}

@article{4035649049,
	title = {Storytelling {With} {Learner} {Data}: {Guiding} {Student} {Reflection} on {Multimodal} {Team} {Data}},
	volume = {14},
	issn = {1939-1382, 2372-0050},
	shorttitle = {Storytelling {With} {Learner} {Data}},
	url = {https://ieeexplore.ieee.org/document/9632388/},
	doi = {10.1109/TLT.2021.3131842},
	abstract = {There is growing interest in creating learning analytics feedback interfaces that support students directly. While dashboards and other visualizations are proliferating, the evidence is that many fail to provide meaningful insights that help students reﬂect productively. The contribution of this article is qualitative and quantitative evidence from two studies evaluating a multimodal teamwork analytics tool in authentic clinical teamwork simulations. Collocated activity data are rendered to help nursing students reﬂect on errors and stressrelated incidents during simulations. The user interface explicitly guides student reﬂection using data storytelling principles, tuned to the intended learning outcomes. The results demonstrate the potential of interfaces that “tell one data story at a time,” by helping students to identify misconceptions and errors; think about strategies they might use to address errors, and reﬂect on their arousal levels. The results also illuminate broader issues around automated formative assessment, and the intelligibility and accountability of learning analytics.},
	language = {en},
	number = {5},
	urldate = {2023-08-07},
	journal = {IEEE Transactions on Learning Technologies},
	author = {Fernandez-Nieto, Gloria Milena and Echeverria, Vanessa and Shum, Simon Buckingham and Mangaroska, Katerina and Kitto, Kirsty and Palominos, Evelyn and Axisa, Carmen and Martinez-Maldonado, Roberto},
	month = oct,
	year = {2021},
	pages = {695--708},
}

@article{4278392816,
	title = {Multimodal data as a means to understand the learning experience},
	volume = {48},
	issn = {02684012},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0268401218312751},
	doi = {10.1016/j.ijinfomgt.2019.02.003},
	abstract = {Most work in the design of learning technology uses click-streams as their primary data source for modelling \& predicting learning behaviour. In this paper we set out to quantify what, if any, advantages do physiological sensing techniques provide for the design of learning technologies. We conducted a lab study with 251 game sessions and 17 users focusing on skill development (i.e., user's ability to master complex tasks). We collected click-stream data, as well as eye-tracking, electroencephalography (EEG), video, and wristband data during the experiment. Our analysis shows that traditional click-stream models achieve 39\% error rate in predicting learning performance (and 18\% when we perform feature selection), while for fused multimodal the error drops up to 6\%. Our work highlights the limitations of standalone click-stream models, and quantiﬁes the expected beneﬁts of using a variety of multimodal data coming from physiological sensing. Our ﬁndings help shape the future of learning technology research by pointing out the substantial beneﬁts of physiological sensing.},
	language = {en},
	urldate = {2023-08-07},
	journal = {International Journal of Information Management},
	author = {Giannakos, Michail N. and Sharma, Kshitij and Pappas, Ilias O. and Kostakos, Vassilis and Velloso, Eduardo},
	month = oct,
	year = {2019},
	pages = {108--119},
}

@article{4277812050,
	title = {Improving prediction of students’ performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources},
	volume = {33},
	issn = {1042-1726, 1867-1233},
	url = {https://link.springer.com/10.1007/s12528-021-09298-8},
	doi = {10.1007/s12528-021-09298-8},
	abstract = {The aim of this study was to predict university students’ learning performance using different sources of performance and multimodal data from an Intelligent Tutoring System. We collected and preprocessed data from 40 students from different multimodal sources: learning strategies from system logs, emotions from videos of facial expressions, allocation and fixations of attention from eye tracking, and performance on posttests of domain knowledge. Our objective was to test whether the prediction could be improved by using attribute selection and classification ensembles. We carried out three experiments by applying six classification algorithms to numerical and discretized preprocessed multimodal data. The results show that the best predictions were produced using ensembles and selecting the best attributes approach with numerical data.},
	language = {en},
	number = {3},
	urldate = {2023-08-07},
	journal = {Journal of Computing in Higher Education},
	author = {Chango, Wilson and Cerezo, Rebeca and Sanchez-Santillan, Miguel and Azevedo, Roger and Romero, Cristóbal},
	month = dec,
	year = {2021},
	pages = {614--634},
}

@article{3809293172,
	title = {Blending learning analytics and embodied design to model students’ comprehension of measurement using their actions, speech, and gestures},
	volume = {32},
	issn = {22128689},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2212868921000866},
	doi = {10.1016/j.ijcci.2021.100391},
	abstract = {Although interdisciplinary collaborations are becoming increasingly common, researchers typically use data analysis methods specific to their field in order to uncover how students learn. We present affordances of integrating theories of embodied cognition and design with machine-learning methods to study student learning in mathematics and inform the design of embodied learning activities. By increasing such collaborative research efforts, learning scientists can incorporate regularization in computational models and ultimately draw reliable conclusions to further inform theory and practice through the design of technology-augmented learning activities. To illustrate this point, we explored students’ conceptual understanding of measurement since limited research has identified measurement estimation strategies that should be emphasized in classroom instruction. By uniquely applying machine-learning methods to a small, multimodal dataset from a study on student behavior in mathematics, we identified behavioral profiles, patterns in speech, and specific actions and gestures that are predictive of performance. These findings may be used to inform the design of embodied learning activities for measurement. We discuss the contribution of these findings to the field of embodied design, and the affordances and challenges of conducting collaborative research in the learning sciences.},
	language = {en},
	urldate = {2023-08-07},
	journal = {International Journal of Child-Computer Interaction},
	author = {Closser, Avery H. and Erickson, John A. and Smith, Hannah and Varatharaj, Ashvini and Botelho, Anthony F.},
	month = jun,
	year = {2022},
	pages = {100391},
}

@incollection{3009548670,
	address = {Cham},
	title = {Real-{Time} {Multimodal} {Feedback} with the {CPR} {Tutor}},
	volume = {12163},
	isbn = {978-3-030-52236-0 978-3-030-52237-7},
	url = {http://link.springer.com/10.1007/978-3-030-52237-7_12},
	abstract = {We developed the CPR Tutor, a real-time multimodal feedback system for cardiopulmonary resuscitation (CPR) training. The CPR Tutor detects mistakes using recurrent neural networks for real-time time-series classiﬁcation. From a multimodal data stream consisting of kinematic and electromyographic data, the CPR Tutor system automatically detects the chest compressions, which are then classiﬁed and assessed according to ﬁve performance indicators. Based on this assessment, the CPR Tutor provides audio feedback to correct the most critical mistakes and improve the CPR performance. To test the validity of the CPR Tutor, we ﬁrst collected the data corpus from 10 experts used for model training. Hence, to test the impact of the feedback functionality, we ran a user study involving 10 participants. The CPR Tutor pushes forward the current state of the art of real-time multimodal tutors by providing: 1) an architecture design, 2) a methodological approach to design multimodal feedback and 3) a ﬁeld study on real-time feedback for CPR training.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Artificial {Intelligence} in {Education}},
	publisher = {Springer International Publishing},
	author = {Di Mitri, Daniele and Schneider, Jan and Trebing, Kevin and Sopka, Sasa and Specht, Marcus and Drachsler, Hendrik},
	editor = {Bittencourt, Ig Ibert and Cukurova, Mutlu and Muldner, Kasia and Luckin, Rose and Millán, Eva},
	year = {2020},
	pages = {141--152},
}

@inproceedings{2879332689,
	address = {Honolulu HI USA},
	title = {From {Data} to {Insights}: {A} {Layered} {Storytelling} {Approach} for {Multimodal} {Learning} {Analytics}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {From {Data} to {Insights}},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376148},
	doi = {10.1145/3313831.3376148},
	abstract = {Significant progress to integrate and analyse multimodal data has been carried out in the last years. Yet, little research has tackled the challenge of visualising and supporting the sensemaking of multimodal data to inform teaching and learning. It is naïve to expect that simply by rendering multiple data streams visually, a teacher or learner will be able to make sense of them. This paper introduces an approach to unravel the complexity of multimodal data by organising it into meaningful layers that explain critical insights to teachers and students. The approach is illustrated through the design of two data storytelling prototypes in the context of nursing simulation. Two authentic studies with educators and students identified the potential of the approach to create learning analytics interfaces that communicate insights on team performance, as well as concerns in terms of accountability and automated insights discovery.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Martinez-Maldonado, Roberto and Echeverria, Vanessa and Fernandez Nieto, Gloria and Buckingham Shum, Simon},
	month = apr,
	year = {2020},
	pages = {1--15},
}

@incollection{2836996318,
	address = {Cham},
	title = {Predicting {Learners}’ {Emotions} in {Mobile} {MOOC} {Learning} via a {Multimodal} {Intelligent} {Tutor}},
	volume = {10858},
	isbn = {978-3-319-91463-3 978-3-319-91464-0},
	url = {http://link.springer.com/10.1007/978-3-319-91464-0_15},
	abstract = {Massive Open Online Courses (MOOCs) are a promising approach for scalable knowledge dissemination. However, they also face major challenges such as low engagement, low retention rate, and lack of personalization. We propose AttentiveLearner2, a multimodal intelligent tutor running on unmodiﬁed smartphones, to supplement today’s clickstream-based learning analytics for MOOCs. AttentiveLearner2 uses both the front and back cameras of a smartphone as two complementary and ﬁne-grained feedback channels in real time: the back camera monitors learners’ photoplethysmography (PPG) signals and the front camera tracks their facial expressions during MOOC learning. AttentiveLearner2 implicitly infers learners’ aﬀective and cognitive states during learning from their PPG signals and facial expressions. Through a 26-participant user study, we found that: (1) AttentiveLearner2 can detect 6 emotions in mobile MOOC learning reliably with high accuracy (average accuracy = 84.4\%); (2) the detected emotions can predict learning outcomes (best R2 = 50.6\%); and (3) it is feasible to track both PPG signals and facial expressions in real time in a scalable manner on today’s unmodiﬁed smartphones.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Intelligent {Tutoring} {Systems}},
	publisher = {Springer International Publishing},
	author = {Pham, Phuong and Wang, Jingtao},
	editor = {Nkambou, Roger and Azevedo, Roger and Vassileva, Julita},
	year = {2018},
	pages = {150--159},
}

@article{2634033325,
	title = {Controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting},
	volume = {51},
	issn = {0007-1013, 1467-8535},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/bjet.12987},
	doi = {10.1111/bjet.12987},
	abstract = {Developing oral presentation skills requires both practice and expert feedback. Several systems have been developed during the last 20 years to provide ample practice opportunities and automated feedback for novice presenters. However, a comprehensive literature review discovered that none of those systems have been adequately evaluated in real learning settings. This work is the first randomised controlled evaluation of the impact that one of these systems has in developing oral presentation skills during a real semester-long learning activity with 180 students. The main findings are that (1) the development of different dimensions of the oral presentations are not affected equally by the automated feedback and (2) there is a small but statistically significant effect of the use of the tool when a subsequent presentation is evaluated by a human expert.},
	language = {en},
	number = {5},
	urldate = {2023-08-07},
	journal = {British Journal of Educational Technology},
	author = {Ochoa, Xavier and Dominguez, Federico},
	month = sep,
	year = {2020},
	pages = {1615--1630},
}

@article{2609260641,
	title = {Visualizing {Collaboration} in {Teamwork}: {A} {Multimodal} {Learning} {Analytics} {Platform} for {Non}-{Verbal} {Communication}},
	volume = {12},
	issn = {2076-3417},
	shorttitle = {Visualizing {Collaboration} in {Teamwork}},
	url = {https://www.mdpi.com/2076-3417/12/15/7499},
	doi = {10.3390/app12157499},
	abstract = {Developing communication skills in collaborative contexts is of special interest for educational institutions, since these skills are crucial to forming competent professionals for today’s world. New and accessible technologies open a way to analyze collaborative activities in face-to-face and non-face-to-face situations, where collaboration and student attitudes are difﬁcult to measure using traditional methods. In this context, Multimodal Learning Analytics (MMLA) appear as an alternative to complement the evaluation and feedback of core skills. We present a MMLA platform to support collaboration assessment based on the capture and classiﬁcation of non-verbal communication interactions. The developed platform integrates hardware and software, including machine learning techniques, to detect spoken interactions and body postures from video and audio recordings. The captured data is presented in a set of visualizations, designed to help teachers to obtain insights about the collaboration of a team. We performed a case study to explore if the visualizations were useful to represent different behavioral indicators of collaboration in different teamwork situations: a collaborative situation and a competitive situation. We discussed the results of the case study in a focus group with three teachers, to get insights in the usefulness of our proposal. The results show that the measurements and visualizations are helpful to understand differences in collaboration, conﬁrming the feasibility the MMLA approach for assessing and providing collaboration insights based on non-verbal communication.},
	language = {en},
	number = {15},
	urldate = {2023-08-07},
	journal = {Applied Sciences},
	author = {Noël, René and Miranda, Diego and Cechinel, Cristian and Riquelme, Fabián and Primo, Tiago Thompsen and Munoz, Roberto},
	month = jul,
	year = {2022},
	pages = {7499},
}

@inproceedings{2497456347,
	address = {Sydney New South Wales Australia},
	title = {The {RAP} system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors},
	isbn = {978-1-4503-6400-3},
	shorttitle = {The {RAP} system},
	url = {https://dl.acm.org/doi/10.1145/3170358.3170406},
	doi = {10.1145/3170358.3170406},
	abstract = {Developing communication skills in higher education students could be a challenge to professors due to the time needed to provide formative feedback. This work presents RAP, a scalable system to provide automatic feedback to entry-level students to develop basic oral presentation skills. The system improves the state-ofthe-art by analyzing posture, gaze, volume, filled pauses and the slides of the presenters through data captured by very low-cost sensors. The system also provides an off-line feedback report with multimodal recordings of their performance. An initial evaluation of the system indicates that the system’s feedback highly agrees with human feedback and that students considered that feedback useful to develop their oral presentation skills.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Learning} {Analytics} and {Knowledge}},
	publisher = {ACM},
	author = {Ochoa, Xavier and Domínguez, Federico and Guamán, Bruno and Maya, Ricardo and Falcones, Gabriel and Castells, Jaime},
	month = mar,
	year = {2018},
	pages = {360--364},
}

@inproceedings{2456887548,
	address = {Glasgow UK},
	title = {An unobtrusive and multimodal approach for behavioral engagement detection of students},
	isbn = {978-1-4503-5557-5},
	url = {https://dl.acm.org/doi/10.1145/3139513.3139521},
	doi = {10.1145/3139513.3139521},
	abstract = {In this paper, we investigate detection of students’ behavioral engagement states (On-Task vs. Off-Task) in authentic classroom settings. We propose a multimodal detection approach, based on three unobtrusive modalities readily available in a 1:1 learning scenario where learning technologies are incorporated. These modalities are: (1)Appearance: upper-body video captured using a camera; (2) Context-Performance: students’ interaction and performance data related to learning content; and (3) Mouse: data related to mouse movements during learning process. For each modality, separate unimodal classifiers were trained, and decision-level fusion was applied to obtain final behavioral engagement states. We also analyzed each modality based on Instructional and Assessment sections separately (i.e., Instructional where a student is reading an article or watching an instructional video vs. Assessment where a student is solving exercises on the digital learning platform). We carried out various experiments on a dataset collected in an authentic classroom, where students used laptops equipped with cameras and they consumed learning content for Math on a digital learning platform. The dataset included multimodal data of 17 students who attended a Math course for 13 sessions (40 minutes each). The results indicate that it is beneficial to have separate classification pipelines for Instructional and Assessment sections: For Instructional, using only Appearance modality yields an F1-measure of 0.74, compared to fused performance of 0.70. For Assessment, fusing all three modalities (F1-measure of 0.89) provide a prominent improvement over the best performing unimodality (i.e., 0.81 for Appearance).},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Proceedings of the 1st {ACM} {SIGCHI} {International} {Workshop} on {Multimodal} {Interaction} for {Education}},
	publisher = {ACM},
	author = {Alyuz, Nese and Okur, Eda and Genc, Utku and Aslan, Sinem and Tanriover, Cagri and Esme, Asli Arslan},
	month = nov,
	year = {2017},
	pages = {26--32},
}

@article{2345021698,
	title = {Exploring {Collaborative} {Writing} of {User} {Stories} {With} {Multimodal} {Learning} {Analytics}: {A} {Case} {Study} on a {Software} {Engineering} {Course}},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Exploring {Collaborative} {Writing} of {User} {Stories} {With} {Multimodal} {Learning} {Analytics}},
	url = {https://ieeexplore.ieee.org/document/8496762/},
	doi = {10.1109/ACCESS.2018.2876801},
	abstract = {Software engineering is the application of principles used in engineering design, development, testing, deployment, and management of software systems. One of the software engineering’s approaches, highly used in new industries, is agile development. User stories are a commonly used notation to capture user requirements in agile development. Nevertheless, for the elaboration of user stories, a high level of collaboration with the client is necessary. This professional skill is rarely measured or evaluated in educational contexts. The present work approaches collaboration in software engineering students through multimodal learning analytics, modeling, and evaluating students’ collaboration while they are writing user stories. For that, we used multidirectional microphones in order to derive social network analysis metrics related to collaboration (permanence and prompting) together with human-annotated information (quality of the stories and productivity). Results show that groups with a lower productivity in writing user stories and less professional experience in managing software requirements present a non-collaborative behavior more frequently, and that teams with a fewer number of interventions are more likely to produce a greater number of user stories. Moreover, although low experience subjects produced more user stories, a greater productivity of the most experienced subjects was not statistically veriﬁed. We believe that these types of initiatives will allow the measurement and early development of such skills in university students.},
	language = {en},
	urldate = {2023-08-07},
	journal = {IEEE Access},
	author = {Noel, Rene and Riquelme, Fabian and Lean, Roberto Mac and Merino, Erick and Cechinel, Cristian and Barcelos, Thiago S. and Villarroel, Rodolfo and Munoz, Roberto},
	year = {2018},
	pages = {67783--67798},
}

@article{2273914836,
	title = {Many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities},
	volume = {16},
	issn = {1556-1607, 1556-1615},
	url = {https://link.springer.com/10.1007/s11412-021-09358-2},
	doi = {10.1007/s11412-021-09358-2},
	abstract = {Understanding the way learners engage with learning technologies, and its relation with their learning, is crucial for motivating design of effective learning interventions. Assessing the learners’ state of engagement, however, is non-trivial. Research suggests that performance is not always a good indicator of learning, especially with open-ended constructivist activities. In this paper, we describe a combined multi-modal learning analytics and interaction analysis method that uses video, audio and log data to identify multi-modal collaborative learning behavioral profiles of 32 dyads as they work on an open-ended task around interactive tabletops with a robot mediator. These profiles, which we name Expressive Explorers, Calm Tinkerers, and Silent Wanderers, confirm previous collaborative learning findings. In particular, the amount of speech interaction and the overlap of speech between a pair of learners are behavior patterns that strongly distinguish between learning and non-learning pairs. Delving deeper, findings suggest that overlapping speech between learners can indicate engagement that is conducive to learning. When we more broadly consider learner affect and actions during the task, we are better able to characterize the range of behavioral profiles exhibited among those who learn. Specifically, we discover two behavioral dimensions along which those who learn vary, namely, problem solving strategy (actions) and emotional expressivity (affect). This finding suggests a relation between problem solving strategy and emotional behavior; one strategy leads to more frustration compared to another. These findings have implications for the design of real-time learning interventions that support productive collaborative learning in open-ended tasks.},
	language = {en},
	number = {4},
	urldate = {2023-08-07},
	journal = {International Journal of Computer-Supported Collaborative Learning},
	author = {Nasir, Jauwairia and Kothiyal, Aditi and Bruno, Barbara and Dillenbourg, Pierre},
	month = dec,
	year = {2021},
	pages = {485--523},
}

@article{2155422499,
	title = {A multimodal analysis of pair work engagement episodes: {Implications} for {EMI} lecturer training},
	volume = {58},
	issn = {14751585},
	shorttitle = {A multimodal analysis of pair work engagement episodes},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1475158522000443},
	doi = {10.1016/j.jeap.2022.101124},
	abstract = {Lecturers’ abilities to use semiotic resources to construct meaning and to create engagement play an important role in university classrooms where English is the medium of instruction (EMI). The main focus of this study is on how EMI lecturers enrolled in a professional development program use semiotic and interpersonal resources to engage students through pair work activities. Two analyses were conducted on a dataset of twelve micro-teaching sessions extracted from an EMI teacher training corpus. The first analysis identified the moves and pedagogical functions lec­ turers instantiated while carrying out engagement episodes (EEs). The findings of this analysis served to design the “Pair work engagement episodes framework”, which includes five basic moves: 1) contextualizing, 2) setting up, 3) monitoring, 4) eliciting, and 5) summarizing. The second analysis illustrated how the pedagogical functions found in each move of four EEs were constructed multimodally through verbal and non-verbal communicative modes (i.e., spoken, written, non-verbal materials, space, and posture). The pair work EEs framework and the multimodal analysis lend support to strategies that may be implemented in EMI professional development programs to enhance lecturers’ multimodal interactional competence.},
	language = {en},
	urldate = {2023-08-07},
	journal = {Journal of English for Academic Purposes},
	author = {Morell, Teresa and Beltrán-Palanques, Vicent and Norte, Natalia},
	month = jul,
	year = {2022},
	pages = {101124},
}

@article{2055153191,
	title = {Round or rectangular tables for collaborative problem solving? {A} multimodal learning analytics study},
	volume = {51},
	issn = {0007-1013, 1467-8535},
	shorttitle = {Round or rectangular tables for collaborative problem solving?},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/bjet.12988},
	doi = {10.1111/bjet.12988},
	abstract = {The current knowledge of the effects of the physical environment on learners’ behaviour in collaborative problem-solving tasks is underexplored. This paper aims to critically examine the potential of multimodal learning analytics, using new data sets, in studying how the shapes of shared tables affect the learners’ behaviour when collaborating in terms of patterns of participation and indicators related to physical social interactions. The research presented in this paper investigates this question considering the potential interplay with contextual aspects (level of education) and learning design decisions (group size). Three dependent variables (distance between students, range of movement and level of participation) are tested using quantitative and qualitative analyses of data collected using a motion capture system and video recordings. Results show that the use of round tables (vs rectangular tables) leads to higher levels of on-task participation in the case of elementary school students. For university students, different table shapes seem to have a limited impact on their levels of participation in collaborative problem solving. The analysis shows significant differences regarding the relationship between group size and the distance between students, but there is no substantial evidence that group size affects the level of participation. The findings support previous research highlighting the importance of studying the role of the physical environment as an element of learning design and the potential of multimodal learning analytics in approaching these studies.},
	language = {en},
	number = {5},
	urldate = {2023-08-07},
	journal = {British Journal of Educational Technology},
	author = {Vujovic, Milica and Hernández‐Leo, Davinia and Tassani, Simone and Spikol, Daniel},
	month = sep,
	year = {2020},
	pages = {1597--1614},
}

@inproceedings{2000036002,
	address = {Frankfurt Germany},
	title = {Predicting learners' effortful behaviour in adaptive assessment using multimodal data},
	isbn = {978-1-4503-7712-6},
	url = {https://dl.acm.org/doi/10.1145/3375462.3375498},
	doi = {10.1145/3375462.3375498},
	abstract = {Many factors influence learners’ performance on an activity beyond the knowledge required. Learners’ on-task effort has been acknowledged for strongly relating to their educational outcomes, reflecting how actively they are engaged in that activity. However, effort is not directly observable. Multimodal data can provide additional insights into the learning processes and may allow for effort estimation. This paper presents an approach for the classification of effort in an adaptive assessment context. Specifically, the behaviour of 32 students was captured during an adaptive self-assessment activity, using logs and physiological data (i.e., eye-tracking, EEG, wristband and facial expressions). We applied k-means to the multimodal data to cluster students’ behavioural patterns. Next, we predicted students’ effort to complete the upcoming task, based on the discovered behavioural patterns using a combination of Hidden Markov Models (HMMs) and the Viterbi algorithm. We also compared the results with other state-of-the-art classification algorithms (SVM, Random Forest). Our findings provide evidence that HMMs can encode the relationship between effort and behaviour (captured by the multimodal data) in a more efficient way than the other methods. Foremost, a practical implication of the approach is that the derived HMMs also pinpoint the moments to provide preventive/prescriptive feedback to the learners in real-time, by building-upon the relationship between behavioural patterns and the effort the learners are putting in.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Proceedings of the {Tenth} {International} {Conference} on {Learning} {Analytics} \& {Knowledge}},
	publisher = {ACM},
	author = {Sharma, Kshitij and Papamitsiou, Zacharoula and Olsen, Jennifer K. and Giannakos, Michail},
	month = mar,
	year = {2020},
	pages = {480--489},
}

@inproceedings{1886134458,
	address = {San Jose, CA, USA},
	title = {Personalizing {Computer} {Science} {Education} by {Leveraging} {Multimodal} {Learning} {Analytics}},
	isbn = {978-1-5386-1174-6},
	url = {https://ieeexplore.ieee.org/document/8658596/},
	doi = {10.1109/FIE.2018.8658596},
	abstract = {This Research Full Paper implements a framework that harness sources of programming learning analytics on three computer programming courses a Higher Education Institution. The platform, called PredictCS, automatically detects lowerperforming or “at-risk” students in programming courses and automatically and adaptively sends them feedback. This system has been progressively adopted at the classroom level to improve personalized learning. A visual analytics dashboard is developed and accessible to Faculty. This contains information about the models deployed and insights extracted from student’s data. By leveraging historical student data we built predictive models using student characteristics, prior academic history, logged interactions between students and online resources, and students’ progress in programming laboratory work. Predictions were generated every week during the semester’s classes. In addition, during the second half of the semester, students who opted-in received pseudo real-time personalised feedback. Notiﬁcations were personalised based on students’ predicted performance on the course and included a programming suggestion from a topstudent in the class if any programs submitted had failed to meet the speciﬁed criteria. As a result, this helped students who corrected their programs to learn more and reduced the gap between lower and higher-performing students.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {2018 {IEEE} {Frontiers} in {Education} {Conference} ({FIE})},
	publisher = {IEEE},
	author = {Azcona, David and Hsiao, I-Han and Smeaton, Alan F.},
	month = oct,
	year = {2018},
	pages = {1--9},
}

@inproceedings{1877483551,
	address = {Osaka, Japan},
	title = {Motion-{Based} {Educational} {Games}: {Using} {Multi}-{Modal} {Data} to {Predict} {Player}’s {Performance}},
	isbn = {978-1-72814-533-4},
	shorttitle = {Motion-{Based} {Educational} {Games}},
	url = {https://ieeexplore.ieee.org/document/9231892/},
	doi = {10.1109/CoG47356.2020.9231892},
	abstract = {Multi-Modal Data (MMD) can help educational games researchers understand the synergistic relationship between player’s movement and their learning experiences, and consequently uncover insights that may lead to improved design of movement-based game technologies for learning. Predicting player performance fosters opportunities to cultivate heightened educational experiences and outcomes. However, predicting player’s performance utilising player-generated MMD during their interactions with educational Motion-Based Touchless Games (MBTG) is challenging. To bridge this gap, we implemented an in-situ study where 26 users, age 11, played 2 maths MBTGs in a single 20-30 minute session. We collected player’s MMD (i.e., gaze data from eye-tracking glasses, physiological data from wristbands, and skeleton data from Kinect) produced during game-play. To investigate the potential of MMD for predicting player’s academic performance, we used machine learning techniques and MMD derived from player’s game-play. This allowed us to identify the MMD features that drive rapid highly accurate predictions of players’ academic performance in educational MBTGs. This might allow us to provide realtime proactive feedback to the player to support them through their educational gaming experience. Our analysis compared two data lengths corresponding to half and full duration of the player’s question solving time. We showed that all combinations of extracted features associated with gaze, physiological, and skeleton data, predicted student performance more accurately than the majority baseline. Additionally, the most accurate prediction of player’s performance derived from the combination of gaze and physiological data for both full and half data lengths. Our ﬁndings emphasise the signiﬁcance of using MMD for realtime performance prediction in educational MBTG and offer implications for practice.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {2020 {IEEE} {Conference} on {Games} ({CoG})},
	publisher = {IEEE},
	author = {Lee-Cultura, Serena and Sharma, Kshitij and Papavlasopoulou, Sofia and Giannakos, Michail},
	month = aug,
	year = {2020},
	pages = {17--24},
}

@incollection{1847468084,
	address = {Cham},
	title = {Computationally {Augmented} {Ethnography}: {Emotion} {Tracking} and {Learning} in {Museum} {Games}},
	volume = {1112},
	isbn = {978-3-030-33231-0 978-3-030-33232-7},
	shorttitle = {Computationally {Augmented} {Ethnography}},
	url = {http://link.springer.com/10.1007/978-3-030-33232-7_12},
	abstract = {In this paper, we describe a way of using multi-modal learning analytics to augment qualitative data. We extract facial expressions that may indicate particular emotions from videos of dyads playing an interactive tabletop game built for a museum. From this data, we explore the correlation between students’ understanding of the biological and complex systems concepts showcased in the learning environment and their facial expressions. First, we show how information retrieval techniques can be used on facial expression features to investigate emotional variation during key moments of the interaction. Second, we connect these features to moments of learning identiﬁed by traditional qualitative methods. Finally, we present an initial pilot using these methods in concert to identify key moments in multiple modalities. We end with a discussion of our preliminary ﬁndings on interweaving machine and human analytical approaches.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Advances in {Quantitative} {Ethnography}},
	publisher = {Springer International Publishing},
	author = {Martin, Kit and Wang, Emily Q. and Bain, Connor and Worsley, Marcelo},
	editor = {Eagan, Brendan and Misfeldt, Morten and Siebert-Evenstone, Amanda},
	year = {2019},
	pages = {141--153},
}

@inproceedings{1770989706,
	address = {Frankfurt Germany},
	title = {Focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving},
	isbn = {978-1-4503-7712-6},
	shorttitle = {Focused or stuck together},
	url = {https://dl.acm.org/doi/10.1145/3375462.3375467},
	doi = {10.1145/3375462.3375467},
	abstract = {Collaborative problem solving (CPS) in virtual environments is an increasingly important context of 21st century learning. However, our understanding of this complex and dynamic phenomenon is still limited. Here, we examine unimodal primitives (activity on the screen, speech, and body movements), and their multimodal combinations during remote CPS. We analyze two datasets where 116 triads collaboratively engaged in a challenging visual programming task using video conferencing software. We investigate how UI-interactions, behavioral primitives, and multimodal patterns were associated with teams’ subjective and objective performance outcomes. We found that idling with limited speech (i.e., silence or backchannel feedback only) and without movement was negatively correlated with task performance and with participants’ subjective perceptions of the collaboration. However, being silent and focused during solution execution was positively correlated with task performance. Results illustrate that in some cases, multimodal patterns improved the predictions and improved explanatory power over the unimodal primitives. We discuss how the findings can inform the design of real-time interventions for remote CPS.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Proceedings of the {Tenth} {International} {Conference} on {Learning} {Analytics} \& {Knowledge}},
	publisher = {ACM},
	author = {Vrzakova, Hana and Amon, Mary Jean and Stewart, Angela and Duran, Nicholas D. and D'Mello, Sidney K.},
	month = mar,
	year = {2020},
	pages = {295--304},
}

@article{1763513559,
	title = {Keep {Me} in the {Loop}: {Real}-{Time} {Feedback} with {Multimodal} {Data}},
	volume = {32},
	issn = {1560-4292, 1560-4306},
	shorttitle = {Keep {Me} in the {Loop}},
	url = {https://link.springer.com/10.1007/s40593-021-00281-z},
	doi = {10.1007/s40593-021-00281-z},
	abstract = {This paper describes the CPR Tutor, a real-time multimodal feedback system for cardiopulmonary resuscitation (CPR) training. The CPR Tutor detects training mistakes using recurrent neural networks. The CPR Tutor automatically recognises and assesses the quality of the chest compressions according to five CPR performance indicators. It detects training mistakes in real-time by analysing a multimodal data stream consisting of kinematic and electromyographic data. Based on this assessment, the CPR Tutor provides audio feedback to correct the most critical mistakes and improve the CPR performance. The mistake detection models of the CPR Tutor were trained using a dataset from 10 experts. Hence, we tested the validity of the CPR Tutor and the impact of its feedback functionality in a user study involving additional 10 participants. The CPR Tutor pushes forward the current state of the art of real-time multimodal tutors by providing: (1) an architecture design, (2) a methodological approach for delivering real-time feedback using multimodal data and (3) a field study on real-time feedback for CPR training. This paper details the results of a field study by quantitatively measuring the impact of the CPR Tutor feedback on the performance indicators and qualitatively analysing the participants’ questionnaire answers.},
	language = {en},
	number = {4},
	urldate = {2023-08-07},
	journal = {International Journal of Artificial Intelligence in Education},
	author = {Di Mitri, Daniele and Schneider, Jan and Drachsler, Hendrik},
	month = dec,
	year = {2022},
	pages = {1093--1118},
}

@article{1637690235,
	title = {Supervised machine learning in multimodal learning analytics for estimating success in project-based learning},
	volume = {34},
	issn = {02664909},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/jcal.12263},
	doi = {10.1111/jcal.12263},
	abstract = {Multimodal learning analytics provides researchers new tools and techniques to capture different types of data from complex learning activities in dynamic learning environments. This paper investigates the use of diverse sensors, including computer vision, user-generated content, and data from the learning objects (physical computing components), to record high-fidelity synchronised multimodal recordings of small groups of learners interacting. We processed and extracted different aspects of the students' interactions to answer the following question: Which features of student group work are good predictors of team success in open-ended tasks with physical computing? To answer this question, we have explored different supervised machine learning approaches (traditional and deep learning techniques) to analyse the data coming from multiple sources. The results illustrate that state-of-the-art computational techniques can be used to generate insights into the "black box" of learning in students' project-based activities. The features identified from the analysis show that distance between learners' hands and faces is a strong predictor of students' artefact quality, which can indicate the value of student collaboration. Our research shows that new and promising approaches such as neural networks, and more traditional regression approaches can both be used to classify multimodal learning analytics data, and both have advantages and disadvantages depending on the research questions and contexts being investigated. The work presented here is a significant contribution towards developing techniques to automatically identify the key aspects of students success in project-based learning environments, and to ultimately help teachers provide appropriate and timely support to students in these fundamental aspects.},
	language = {en},
	number = {4},
	urldate = {2023-08-07},
	journal = {Journal of Computer Assisted Learning},
	author = {Spikol, Daniel and Ruffaldi, Emanuele and Dabisias, Giacomo and Cukurova, Mutlu},
	month = aug,
	year = {2018},
	pages = {366--377},
}

@inproceedings{1609706685,
	address = {Vancouver British Columbia Canada},
	title = {Learning pulse: a machine learning approach for predicting performance in self-regulated learning using multimodal data},
	isbn = {978-1-4503-4870-6},
	shorttitle = {Learning pulse},
	url = {https://dl.acm.org/doi/10.1145/3027385.3027447},
	doi = {10.1145/3027385.3027447},
	abstract = {Learning Pulse explores whether using a machine learning approach on multimodal data such as heart rate, step count, weather condition and learning activity can be used to predict learning performance in self-regulated learning settings. An experiment was carried out lasting eight weeks involving PhD students as participants, each of them wearing a Fitbit HR wristband and having their application on their computer recorded during their learning and working activities throughout the day. A software infrastructure for collecting multimodal learning experiences was implemented. As part of this infrastructure a Data Processing Application was developed to pre-process, analyse and generate predictions to provide feedback to the users about their learning performance. Data from diﬀerent sources were stored using the xAPI standard into a cloud-based Learning Record Store. The participants of the experiment were asked to rate their learning experience through an Activity Rating Tool indicating their perceived level of productivity, stress, challenge and abilities. These self-reported performance indicators were used as markers to train a Linear Mixed Eﬀect Model to generate learner-speciﬁc predictions of the learning performance. We discuss the advantages and the limitations of the used approach, highlighting further development points.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Proceedings of the {Seventh} {International} {Learning} {Analytics} \& {Knowledge} {Conference}},
	publisher = {ACM},
	author = {Di Mitri, Daniele and Scheffel, Maren and Drachsler, Hendrik and Börner, Dirk and Ternier, Stefaan and Specht, Marcus},
	month = mar,
	year = {2017},
	pages = {188--197},
}

@article{1598166515,
	title = {Multimodal learning analytics for game‐based learning},
	volume = {51},
	issn = {0007-1013, 1467-8535},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/bjet.12992},
	doi = {10.1111/bjet.12992},
	abstract = {A distinctive feature of game-based learning environments is their capacity to create learning experiences that are both effective and engaging. Recent advances in sensorbased technologies such as facial expression analysis and gaze tracking have introduced the opportunity to leverage multimodal data streams for learning analytics. Learning analytics informed by multimodal data captured during students’ interactions with game-based learning environments hold significant promise for developing a deeper understanding of game-based learning, designing game-based learning environments to detect maladaptive behaviors and informing adaptive scaffolding to support individualized learning. This paper introduces a multimodal learning analytics approach that incorporates student gameplay, eye tracking and facial expression data to predict student posttest performance and interest after interacting with a game-based learning environment, Crystal Island. We investigated the degree to which separate and combined modalities (ie, gameplay, facial expressions of emotions and eye gaze) captured from students (n = 65) were predictive of student posttest performance and interest after interacting with Crystal Island. Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. We discuss the synergistic effects of combining modalities for predicting both student interest and posttest performance. The findings suggest that multimodal learning analytics can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding.},
	language = {en},
	number = {5},
	urldate = {2023-08-07},
	journal = {British Journal of Educational Technology},
	author = {Emerson, Andrew and Cloude, Elizabeth B. and Azevedo, Roger and Lester, James},
	month = sep,
	year = {2020},
	pages = {1505--1526},
}

@inproceedings{1581261659,
	address = {Virtual Event Netherlands},
	title = {Early {Prediction} of {Visitor} {Engagement} in {Science} {Museums} with {Multimodal} {Learning} {Analytics}},
	isbn = {978-1-4503-7581-8},
	url = {https://dl.acm.org/doi/10.1145/3382507.3418890},
	doi = {10.1145/3382507.3418890},
	abstract = {Modeling visitor engagement is a key challenge in informal learning environments, such as museums and science centers. Devising predictive models of visitor engagement that accurately forecast salient features of visitor behavior, such as dwell time, holds significant potential for enabling adaptive learning environments and visitor analytics for museums and science centers. In this paper, we introduce a multimodal early prediction approach to modeling visitor engagement with interactive science museum exhibits. We utilize multimodal sensor data—including eye gaze, facial expression, posture, and interaction log data—captured during visitor interactions with an interactive museum exhibit for environmental science education, to induce predictive models of visitor dwell time. We investigate machine learning techniques (random forest, support vector machine, Lasso regression, gradient boosting trees, and multi-layer perceptron) to induce multimodal predictive models of visitor engagement with data from 85 museum visitors. Results from a series of ablation experiments suggest that incorporating additional modalities into predictive models of visitor engagement improves model accuracy. In addition, the models show improved predictive performance over time, demonstrating that increasingly accurate predictions of visitor dwell time can be achieved as more evidence becomes available from visitor interactions with interactive science museum exhibits. These findings highlight the efficacy of multimodal data for modeling museum exhibit visitor engagement.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Proceedings of the 2020 {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {ACM},
	author = {Emerson, Andrew and Henderson, Nathan and Rowe, Jonathan and Min, Wookhee and Lee, Seung and Minogue, James and Lester, James},
	month = oct,
	year = {2020},
	pages = {107--116},
}

@article{1576545447,
	title = {Artificial intelligence and multimodal data in the service of human decision‐making: {A} case study in debate tutoring},
	volume = {50},
	issn = {0007-1013, 1467-8535},
	shorttitle = {Artificial intelligence and multimodal data in the service of human decision‐making},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/bjet.12829},
	doi = {10.1111/bjet.12829},
	abstract = {The question: “What is an appropriate role for AI?” is the subject of much discussion and interest. Arguments about whether AI should be a human replacing technology or a human assisting technology frequently take centre stage. Education is no exception when it comes to questions about the role that AI should play, and as with many other professional areas, the exact role of AI in education is not easy to predict. Here, we argue that one potential role for AI in education is to provide opportunities for human intelligence augmentation, with AI supporting us in decision-making processes, rather than replacing us through automation. To provide empirical evidence to support our argument, we present a case study in the context of debate tutoring, in which we use prediction and classification models to increase the transparency of the intuitive decision-making processes of expert tutors for advanced reflections and feedback. Furthermore, we compare the accuracy of unimodal and multimodal classification models of expert human tutors’ decisions about the social and emotional aspects of tutoring while evaluating trainees. Our results show that multimodal data leads to more accurate classification models in the context we studied.},
	language = {en},
	number = {6},
	urldate = {2023-08-07},
	journal = {British Journal of Educational Technology},
	author = {Cukurova, Mutlu and Kent, Carmel and Luckin, Rosemary},
	month = nov,
	year = {2019},
	pages = {3032--3046},
}

@article{1469065963,
	title = {Examining socially shared regulation and shared physiological arousal events with multimodal learning analytics},
	volume = {54},
	issn = {0007-1013, 1467-8535},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/bjet.13280},
	doi = {10.1111/bjet.13280},
	language = {en},
	number = {1},
	urldate = {2023-08-07},
	journal = {British Journal of Educational Technology},
	author = {Nguyen, Andy and Järvelä, Sanna and Rosé, Carolyn and Järvenoja, Hanna and Malmberg, Jonna},
	month = jan,
	year = {2023},
	pages = {293--312},
}

@incollection{1374035721,
	address = {Cham},
	title = {{AttentiveLearner2}: {A} {Multimodal} {Approach} for {Improving} {MOOC} {Learning} on {Mobile} {Devices}},
	volume = {10331},
	isbn = {978-3-319-61424-3 978-3-319-61425-0},
	shorttitle = {{AttentiveLearner2}},
	url = {http://link.springer.com/10.1007/978-3-319-61425-0_64},
	abstract = {We propose AttentiveLearner2, a multimodal mobile learning system for MOOCs running on unmodiﬁed smartphones. AttentiveLearner2 uses both the front and back cameras of a smartphone as two complementary and ﬁne-grained feedback channels in real time: the back camera monitors learners’ photoplethysmography (PPG) signals and the front camera tracks their facial expressions during MOOC learning. AttentiveLearner2 implicitly infers learners’ affective and cognitive states during learning by analyzing learners’ PPG signals and facial expressions. In a 26-participant user study, we found that it is feasible to detect 6 types of emotion during learning via collected PPG signals and facial expressions and these modalities are complement with each other.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Artificial {Intelligence} in {Education}},
	publisher = {Springer International Publishing},
	author = {Pham, Phuong and Wang, Jingtao},
	editor = {André, Elisabeth and Baker, Ryan and Hu, Xiangen and Rodrigo, Ma. Mercedes T. and Du Boulay, Benedict},
	year = {2017},
	pages = {561--564},
}

@incollection{1345598079,
	address = {Cham},
	title = {Intermodality in {Multimodal} {Learning} {Analytics} for {Cognitive} {Theory} {Development}: {A} {Case} from {Embodied} {Design} for {Mathematics} {Learning}},
	isbn = {978-3-031-08075-3 978-3-031-08076-0},
	shorttitle = {Intermodality in {Multimodal} {Learning} {Analytics} for {Cognitive} {Theory} {Development}},
	url = {https://link.springer.com/10.1007/978-3-031-08076-0_6},
	abstract = {Multimodal Learning Analytics (MMLA) grant us insight into learners’ physiological, cognitive, and behavioral activity as it unfolds. In this chapter, we query the relations among modalities, intermodality, in the context of a designbased research program studying the relations between learning to move in new ways and learning to think in new ways. In the ﬁrst part, we reﬂect on how different methods have afforded purchase on the investigation, development, and elaboration of theoretical claims about the multimodal enactment of cognitive events, culminating in the use of Recurrence Quantiﬁcation Analysis (RQA) to quantify the microgenesis of stable new patterns in hand movement and gaze. In the second part, we analyze an RQA case study spanning across hand and gaze modalities to examine the emergence of intermodal coordination at a critical moment in the mathematical task. We conclude with implications and open questions around intermodality in embodied learning.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {The {Multimodal} {Learning} {Analytics} {Handbook}},
	publisher = {Springer International Publishing},
	author = {Tancredi, Sofia and Abdu, Rotem and Balasubramaniam, Ramesh and Abrahamson, Dor},
	editor = {Giannakos, Michail and Spikol, Daniel and Di Mitri, Daniele and Sharma, Kshitij and Ochoa, Xavier and Hammad, Rawad},
	year = {2022},
	pages = {133--158},
}

@incollection{1326191931,
	address = {Cham},
	title = {Multimodal {Learning} {Analytics} in a {Laboratory} {Classroom}},
	volume = {158},
	isbn = {978-3-030-13742-7 978-3-030-13743-4},
	url = {http://link.springer.com/10.1007/978-3-030-13743-4_8},
	abstract = {Sophisticated research approaches and tools can help researchers to investigate the complex processes involved in learning in various settings. The use of video technology to record classroom practices, in particular, can be a powerful way for capturing and studying learning and related phenomena within a social setting such as the classroom. This chapter outlines several multimodal techniques to analyze the learning activities in a laboratory classroom. The video and audio recordings were processed automatically to obtain information rather than requiring manual coding. Moreover, these automated techniques are able to extract information with an efﬁciency that is beyond the capabilities of human-coders, providing the means to deal analytically with the multiple modalities that characterize the classroom. Once generated, the information provided by the different modalities is used to explain and predict high-level constructs such as students’ attention and engagement. This chapter not only presents the results of the analysis, but also describes the setting, hardware and software needed to replicate this analytical approach.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Machine {Learning} {Paradigms}},
	publisher = {Springer International Publishing},
	author = {Chan, Man Ching Esther and Ochoa, Xavier and Clarke, David},
	editor = {Virvou, Maria and Alepis, Efthimios and Tsihrintzis, George A. and Jain, Lakhmi C.},
	year = {2020},
	pages = {131--156},
}

@article{1315379489,
	title = {Multimodal {Engagement} {Analysis} {From} {Facial} {Videos} in the {Classroom}},
	volume = {14},
	issn = {1949-3045, 2371-9850},
	url = {https://ieeexplore.ieee.org/document/9613750/},
	doi = {10.1109/TAFFC.2021.3127692},
	abstract = {Student engagement is a key component of learning and teaching, resulting in a plethora of automated methods to measure it. Whereas most of the literature explores student engagement analysis using computer-based learning often in the lab, we focus on using classroom instruction in authentic learning environments. We collected audiovisual recordings of secondary school classes over a one and a half month period, acquired continuous engagement labeling per student (N=15) in repeated sessions, and explored computer vision methods to classify engagement from facial videos. We learned deep embeddings for attentional and affective features by training Attention-Net for head pose estimation and Affect-Net for facial expression recognition using previously-collected large-scale datasets. We used these representations to train engagement classifiers on our data, in individual and multiple channel settings, considering temporal dependencies. The best performing engagement classifiers achieved student-independent AUCs of .620 and .720 for grades 8 and 12, respectively, with attention-based features outperforming affective features. Score-level fusion either improved the engagement classifiers or was on par with the best performing modality. We also investigated the effect of personalization and found that only 60 seconds of person-specific data, selected by margin uncertainty of the base classifier, yielded an average AUC improvement of .084.},
	language = {en},
	number = {2},
	urldate = {2023-08-07},
	journal = {IEEE Transactions on Affective Computing},
	author = {Sümer, Ömer and Goldberg, Patricia and D’Mello, Sidney and Gerjets, Peter and Trautwein, Ulrich and Kasneci, Enkelejda},
	month = apr,
	year = {2023},
	pages = {1012--1027},
}

@inproceedings{1296637108,
	address = {Glasgow Scotland Uk},
	title = {Towards {Collaboration} {Translucence}: {Giving} {Meaning} to {Multimodal} {Group} {Data}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {Towards {Collaboration} {Translucence}},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300269},
	doi = {10.1145/3290605.3300269},
	abstract = {Collocated, face-to-face teamwork remains a pervasive mode of working, which is hard to replicate online. Team members’ embodied, multimodal interaction with each other and artefacts has been studied by researchers, but due to its complexity, has remained opaque to automated analysis. However, the ready availability of sensors makes it increasingly affordable to instrument work spaces to study teamwork and groupwork. The possibility of visualising key aspects of a collaboration has huge potential for both academic and professional learning, but a frontline challenge is the enrichment of quantitative data streams with the qualitative insights needed to make sense of them. In response, we introduce the concept of collaboration translucence, an approach to make visible selected features of group activity. This is grounded both theoretically (in the physical, epistemic, social and affective dimensions of group activity), and contextually (using domain-specific concepts). We illustrate the approach from the automated analysis of healthcare simulations to train nurses, generating four visual proxies that fuse multimodal data into higher order patterns.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Echeverria, Vanessa and Martinez-Maldonado, Roberto and Buckingham Shum, Simon},
	month = may,
	year = {2019},
	pages = {1--16},
}

@incollection{1019093033,
	address = {Cham},
	title = {{PRIME}: {Block}-{Wise} {Missingness} {Handling} for {Multi}-modalities in {Intelligent} {Tutoring} {Systems}},
	volume = {11962},
	isbn = {978-3-030-37733-5 978-3-030-37734-2},
	shorttitle = {{PRIME}},
	url = {http://link.springer.com/10.1007/978-3-030-37734-2_6},
	abstract = {Block-wise missingness in multimodal data poses a challenging barrier for the analysis over it, which is quite common in practical scenarios such as the multimedia intelligent tutoring systems (ITSs). In this work, we collected data from 194 undergraduates via a biology ITS which involves three modalities: student-system logﬁles, facial expressions, and eye tracking. However, only 32 out of the 194 students had all three modalities and 83\% of them were missing the facial expression data, eye tracking data, or both. To handle such a block-wise missing problem, we propose a Progressively Reﬁned Imputation for Multi-modalities by auto-Encoder (PRIME), which trains the model based on single, pairwise, and entire modalities for imputation in a progressive manner, and therefore enables us to maximally utilize all the available data. We have evaluated PRIME against single-modality log-only (without missingness handling) and ﬁve state-of-the-art missing data handling methods on one important yet challenging student modeling task: to predict students’ learning gains. Our results show that using multimodal data as a result of missing data handling yields better prediction performance than using logﬁles only, and PRIME outperforms other baseline methods for both learning gain prediction and data reconstruction tasks.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {{MultiMedia} {Modeling}},
	publisher = {Springer International Publishing},
	author = {Yang, Xi and Kim, Yeo-Jin and Taub, Michelle and Azevedo, Roger and Chi, Min},
	editor = {Ro, Yong Man and Cheng, Wen-Huang and Kim, Junmo and Chu, Wei-Ta and Cui, Peng and Choi, Jung-Woo and Hu, Min-Chun and De Neve, Wesley},
	year = {2020},
	pages = {63--75},
}

@inproceedings{957160695,
	address = {Glasgow UK},
	title = {Virtual debate coach design: assessing multimodal argumentation performance},
	isbn = {978-1-4503-5543-8},
	shorttitle = {Virtual debate coach design},
	url = {https://dl.acm.org/doi/10.1145/3136755.3136775},
	doi = {10.1145/3136755.3136775},
	abstract = {This paper discusses the design and evaluation of a coaching system used to train young politicians to apply appropriate multimodal rhetoric devices to improve their debate skills. The presented study is carried out to develop debate performance assessment methods and interaction models underlying a Virtual Debate Coach (VDC) application. We identify a number of criteria associated with three questions: (1) how convincing is a debater’s argumentation; (2) how well are debate arguments structured; and (3) how well is an argument delivered. We collected and analysed multimodal data of trainees’ debate behaviour, and contrasted it with that of skilled professional debaters. Observational, correlation and machine learning experiments were performed to identify multimodal correlates of convincing debate performance and link them to experts’ assessments. A rich set of prosodic, motion, linguistic and structural features was considered for the system to operate on. The VDC system was positively evaluated in a trainee-based setting.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Proceedings of the 19th {ACM} {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {ACM},
	author = {Petukhova, Volha and Mayer, Tobias and Malchanau, Andrei and Bunt, Harry},
	month = nov,
	year = {2017},
	pages = {41--50},
}

@inproceedings{818492192,
	address = {Vancouver British Columbia Canada},
	title = {Understanding student learning trajectories using multimodal learning analytics within an embodied-interaction learning environment},
	isbn = {978-1-4503-4870-6},
	url = {https://dl.acm.org/doi/10.1145/3027385.3027429},
	doi = {10.1145/3027385.3027429},
	abstract = {The aim of this paper is to show how multimodal learning analytics (MMLA) can help understand how elementary students explore the concept of feedback loops while controlling an embodied simulation of a predator-prey ecosystem using hand movements as an interface with the computer simulation. We represent student motion patterns from fine-grained logs of hands and gaze data, and then map these observed motion patterns against levels of student performance to make inferences about how embodiment plays a role in the learning process. Results show five distinct motion sequences in students’ embodied interactions, and these motion patterns are statistically associated with initial and post-tutorial levels of students’ understanding of feedback loops. Analysis of student gaze also shows distinctive patterns as to how low- and high-performing students attended to information presented in the simulation. Using MMLA, we show how students’ explanations of feedback loops look differently according to cluster membership, which provides evidence that embodiment interacts with conceptual understanding.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Proceedings of the {Seventh} {International} {Learning} {Analytics} \& {Knowledge} {Conference}},
	publisher = {ACM},
	author = {Andrade, Alejandro},
	month = mar,
	year = {2017},
	pages = {70--79},
}

@inproceedings{804659204,
	address = {Wollongong, NSW},
	title = {Towards {Smart} {Educational} {Recommendations} with {Reinforcement} {Learning} in {Classroom}},
	isbn = {978-1-5386-6522-0},
	url = {https://ieeexplore.ieee.org/document/8615217/},
	doi = {10.1109/TALE.2018.8615217},
	abstract = {In this paper, we propose to construct a cyberphysical-social system that uses multiple sensors such as cameras and a quiz creator to track the learning process of the students and applies reinforcement learning techniques to provide learning guidance based on the multi-modal sensing data in smart classroom. More specifically, the smart learning recommendation system measures the heartbeats, quiz scores, blinks and facial expressions of each student to formulate the learning states and applies reinforcement learning to recommend the effective learning activities for students based on their current learning states. The interactive learning recommendation process in a smart classroom with multiple sensors can be modeled as a Markov decision process. Our simulation results have preliminarily demonstrated the effectiveness of this smart learning recommendation system. This work may provide insights into constructing a future intelligent learning environment for enriched personalized experiences.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {2018 {IEEE} {International} {Conference} on {Teaching}, {Assessment}, and {Learning} for {Engineering} ({TALE})},
	publisher = {IEEE},
	author = {Liu, Su and Chen, Ye and Huang, Hui and Xiao, Liang and Hei, Xiaojun},
	month = dec,
	year = {2018},
	pages = {1079--1084},
}

@incollection{666050348,
	address = {Cham},
	title = {Multicraft: {A} {Multimodal} {Interface} for {Supporting} and {Studying} {Learning} in {Minecraft}},
	volume = {12790},
	isbn = {978-3-030-77413-4 978-3-030-77414-1},
	shorttitle = {Multicraft},
	url = {https://link.springer.com/10.1007/978-3-030-77414-1_10},
	abstract = {In this paper, we present work on bringing multimodal interaction to Minecraft. The platform, Multicraft, incorporates speech-based input, eye tracking, and natural language understanding to facilitate more equitable gameplay in Minecraft. We tested the platform with elementary, middle school students and college students through a collection of studies. Students found each of the provided modalities to be a compelling way to play Minecraft. Additionally, we discuss the ways that these different types of multimodal data can be used to identify the meaningful spatial reasoning practices that students demonstrate while playing Minecraft. Collectively, this paper emphasizes the opportunity to bridge a multimodal interface with a means for collecting rich data that can better support diverse learners in non-traditional learning environments.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {{HCI} in {Games}: {Serious} and {Immersive} {Games}},
	publisher = {Springer International Publishing},
	author = {Worsley, Marcelo and Mendoza Tudares, Kevin and Mwiti, Timothy and Zhen, Mitchell and Jiang, Marc},
	editor = {Fang, Xiaowen},
	year = {2021},
	pages = {113--131},
}

@article{566043228,
	title = {Automatic {Student} {Engagement} in {Online} {Learning} {Environment} {Based} on {Neural} {Turing} {Machine}},
	volume = {11},
	issn = {20103689},
	url = {http://www.ijiet.org/show-151-1748-1.html},
	doi = {10.18178/ijiet.2021.11.3.1497},
	abstract = {With the continuous and rapid growth of online courses, online learners’ engagement recognition has become a novel research topic in the field of computer vision and pattern recognition. While a few attempts to automatic engagement recognition has been studied in the literature, learning a robust engagement measure is still a challenging task. To address it, we propose a new automatic engagement recognition method based on Neural Turing Machine in this paper. In particular, we firstly extract student’s eye gaze features, facial action unit features, head pose features, and body pose features respectively, then combine these multi modal features into the final feature of our recognition task. Moreover, we propose the engagement recognition framework based on the idea of Neural Turing Machine to learn the weight of each short video feature. In consequence, the feature fused by different weights will be applied to identify the students’ engagement in learning online courses. Empirically, we show improved performance over state of the art methods to automatic engagement recognition on DAiSEE dataset.},
	language = {en},
	number = {3},
	urldate = {2023-08-07},
	journal = {International Journal of Information and Education Technology},
	author = {{Capital Normal University, Beijing, China} and Ma, Xiaoyang and Xu, Min and Dong, Yao and Sun, Zhong},
	year = {2021},
	pages = {107--111},
}

@inproceedings{433919853,
	address = {Braga Portugal},
	title = {Understanding {Fun} in {Learning} to {Code}: {A} {Multi}-{Modal} {Data} approach},
	isbn = {978-1-4503-9197-9},
	shorttitle = {Understanding {Fun} in {Learning} to {Code}},
	url = {https://dl.acm.org/doi/10.1145/3501712.3529716},
	doi = {10.1145/3501712.3529716},
	abstract = {The role of fun in learning, and specifically in learning to code, is critical but not yet fully understood. Fun is typically measured by post session questionnaires, which are coarse-grained, evaluating activities that sometimes last an hour, a day or longer. Here we examine how fun impacts learning during a coding activity, combining continuous physiological response data from wristbands and facial expressions from facial camera videos, along with self-reported measures (i.e. knowledge test and reported fun). Data were collected from primary school students (N = 53) in a single-occasion, two-hours long coding workshop, with the BBC micro:bits. We found that a) sadness, anger and stress are negatively, and arousal is positively related to students’ relative learning gain (RLG), b) experienced fun is positively related to students’ RLG and c) RLG and fun are related to certain physiological markers derived from the physiological response data.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Interaction {Design} and {Children}},
	publisher = {ACM},
	author = {Tisza, Gabriella and Sharma, Kshitij and Papavlasopoulou, Sofia and Markopoulos, Panos and Giannakos, Michail},
	month = jun,
	year = {2022},
	pages = {274--287},
}

@article{205660768,
	title = {Multimodal learning analytics to investigate cognitive load during online problem solving},
	volume = {51},
	issn = {0007-1013, 1467-8535},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/bjet.12958},
	doi = {10.1111/bjet.12958},
	abstract = {To have insight into cognitive load (CL) during online complex problem solving, this study aimed at measuring CL through physiological data. This study experimentally manipulated intrinsic and extraneous load of exercises in the domain of statistics, resulting in four conditions: high complex with hints, low complex with hints, high complex without hints and low complex without hints. The study had a within-subjectdesign in which 67 students solved the exercises in a randomized order. Self-reported CL was combined with physiological data, namely, galvanic skin response (GSR), skin temperature (ST), heart rate (HR) and heart rate variability (HRV). Multiple imputation was used for handling missing data from resp. 16 and 19 students for GSR/ST and HR/ HRV. First, differences between conditions in view of physiological data were examined. Second, we investigated how much variance of self-reported CL and task performance was explained by physiological data. Finally, we investigated which features can be used to assess (objective) CL. Results revealed no significant differences between the manipulated conditions in terms of physiological data. Nonetheless, HR and ST were significantly related to self-reported CL, whereas ST to task performance. Additionally, this study revealed the potential of ST and HR to assess high CL.},
	language = {en},
	number = {5},
	urldate = {2023-08-07},
	journal = {British Journal of Educational Technology},
	author = {Larmuseau, Charlotte and Cornelis, Jan and Lancieri, Luigi and Desmet, Piet and Depaepe, Fien},
	month = sep,
	year = {2020},
	pages = {1548--1562},
}

@article{147203129,
	title = {Multimodal {Learning} {Analytics} to {Inform} {Learning} {Design}: {Lessons} {Learned} from {Computing} {Education}},
	volume = {7},
	issn = {19297750},
	shorttitle = {Multimodal {Learning} {Analytics} to {Inform} {Learning} {Design}},
	url = {https://www.learning-analytics.info/index.php/JLA/article/view/6816},
	doi = {10.18608/jla.2020.73.7},
	abstract = {Programming is a complex learning activity that involves coordination of cognitive processes and affective states. These aspects are often considered individually in computing education research, demonstrating limited understanding of how and when students learn best. This issue confines researchers to contextualize evidencedriven outcomes when learning behaviour deviates from pedagogical intentions. Multimodal learning analytics (MMLA) captures data essential for measuring constructs (e.g., cognitive load, confusion) that are posited in the learning sciences as important for learning, and cannot effectively be measured solely with the use of programming process data (IDE-log data). Thus, we augmented IDE-log data with physiological data (e.g., gaze data) and participants’ facial expressions, collected during a debugging learning activity. The findings emphasize the need for learning analytics that are consequential for learning, rather than easy and convenient to collect. In that regard, our paper aims to provoke productive reflections and conversations about the potential of MMLA to expand and advance the synergy of learning analytics and learning design among the community of educators from a post-evaluation design-aware process to a permanent monitoring process of adaptation.},
	language = {en},
	number = {3},
	urldate = {2023-08-07},
	journal = {Journal of Learning Analytics},
	author = {Mangaroska, Katerina and Sharma, Kshitij and Gašević, Dragan and Giannakos, Michalis},
	month = dec,
	year = {2020},
	pages = {79--97},
}

@article{123412197,
	title = {Utilizing {Multimodal} {Data} {Through} {fsQCA} to {Explain} {Engagement} in {Adaptive} {Learning}},
	volume = {13},
	issn = {1939-1382, 2372-0050},
	url = {https://ieeexplore.ieee.org/document/9181457/},
	doi = {10.1109/TLT.2020.3020499},
	abstract = {Investigating and explaining the patterns of learners’ engagement in adaptive learning conditions is a core issue towards improving the quality of personalized learning services. This article collects learner data from multiple sources during an adaptive learning activity, and employs a fuzzy set qualitative comparative analysis (fsQCA) approach to shed light to learners’ engagement patterns, with respect to their learning performance. Speciﬁcally, this article measures and codes learners’ engagement by fusing and compiling clickstreams (e.g., response time), physiological data (e.g., eye-tracking, electroencephalography, electrodermal activity), and survey data (e.g., goal-orientation) to determine what conﬁgurations of those data explain when learners can attain high or medium/low learning performance. For the evaluation of the approach, an empirical study with 32 undergraduates was conducted. The analysis revealed six conﬁgurations that explain learners’ high performance and three that explain learners’ medium/low performance, driven by engagement measures coming from the multimodal data. Since fsQCA explains the outcome of interest itself, rather than its variance, these ﬁndings advance our understanding on the combined effect of the multiple indicators of engagement on learners’ performance. Limitations and potential implications of the ﬁndings are also discussed.},
	language = {en},
	number = {4},
	urldate = {2023-08-07},
	journal = {IEEE Transactions on Learning Technologies},
	author = {Papamitsiou, Zacharoula and Pappas, Ilias O. and Sharma, Kshitij and Giannakos, Michail N.},
	month = oct,
	year = {2020},
	pages = {689--703},
}

@article{86191824,
	title = {Examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes},
	volume = {29},
	issn = {1049-4820, 1744-5191},
	url = {https://www.tandfonline.com/doi/full/10.1080/10494820.2019.1612450},
	doi = {10.1080/10494820.2019.1612450},
	abstract = {Previous research illustrates the collaborative nature of adolescents’ multimodal composing processes. However, few studies have speciﬁcally focused on how diﬀerent modes inﬂuence student interactions over time. This study examines how multiple modes (e.g. text, music, visuals, and animations) mediated middle schoolers’ composing processes as they worked in small groups to create multimodal science ﬁctions. Situated in an afterschool program, each student selected the role of writer, scientist, or designer. Data sources included screen capture video, semi-structured interviews, and multimodal products. Qualitative data analysis involved the constant comparative method to establish codes for types of interactions and the mediating modes as a case study small group collaboratively composed. Findings indicate: (1) students were inclined to provide short responses to move on with composing practices; (2) group discussions while multimodal composing followed three stages: mode and story exploration, mode-story integration, and mode-story completion; (3) multimodal comics fostered the most discussion; (4) diﬀerent modes supported self-oriented and grouporiented contributions in unique ways. This study contributes an initial understanding into how diﬀerent modalities mediate students’ interactions and oﬀers implications for scaﬀolding peer interactions during multimodal composing processes.},
	language = {en},
	number = {5},
	urldate = {2023-08-07},
	journal = {Interactive Learning Environments},
	author = {Jiang, Shiyan and Smith, Blaine E. and Shen, Ji},
	month = jul,
	year = {2021},
	pages = {807--820},
}

@incollection{32184286,
	address = {Cham},
	title = {Once {More} with {Feeling}: {Emotions} in {Multimodal} {Learning} {Analytics}},
	isbn = {978-3-031-08075-3 978-3-031-08076-0},
	shorttitle = {Once {More} with {Feeling}},
	url = {https://link.springer.com/10.1007/978-3-031-08076-0_11},
	abstract = {The emotions that students experience when engaging in tasks critically inﬂuence their performance and many models of learning and competence include assumptions about affective variables and respective emotions. However, while researchers agree about the importance of emotions for learning, it remains challenging to connect momentary affect, i.e., emotions, to learning processes. Advances in automated speech recognition and natural language processing (NLP) allow real time detection of emotions in recorded language. We use NLP and machine learning techniques to automatically extract information about students’ motivational states while engaging in the construction of explanations and investigate how this information can help more accurately predict students’ learning over the course of a 10-week energy unit. Our results show how NLP and ML techniques allow the use of different modalities of the same data in order to better understand individual differences in students’ performances. However, in realistic settings, this task remains far from trivial and requires extensive preprocessing of the data and the results need to be interpreted with care and caution. Thus, future research is needed before these methods can be deployed at scale.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {The {Multimodal} {Learning} {Analytics} {Handbook}},
	publisher = {Springer International Publishing},
	author = {Kubsch, Marcus and Caballero, Daniela and Uribe, Pablo},
	editor = {Giannakos, Michail and Spikol, Daniel and Di Mitri, Daniele and Sharma, Kshitij and Ochoa, Xavier and Hammad, Rawad},
	year = {2022},
	pages = {261--285},
}
