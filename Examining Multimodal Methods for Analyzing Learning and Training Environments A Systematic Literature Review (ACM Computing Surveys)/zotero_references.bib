
@inproceedings{chua_technologies_2019,
	address = {Tempe AZ USA},
	title = {Technologies for automated analysis of co-located, real-life, physical learning spaces: {Where} are we now?},
	url = {https://dl-acm-org.proxy.library.vanderbilt.edu/doi/10.1145/3303772.3303811},
	abstract = {The motivation for this paper is derived from the fact that there has been increasing interest among researchers and practitioners in developing technologies that capture, model and analyze learning and teaching experiences that take place beyond computer-based learning environments. In this paper, we review case studies of tools and technologies developed to collect and analyze data in educational settings, quantify learning and teaching processes and support assessment of learning and teaching in an automated fashion. We focus on pipelines that leverage information and data harnessed from physical spaces and/or integrates collected data across physical and digital spaces. Our review reveals a promising field of physical classroom analysis. We describe some trends and suggest potential future directions. Specifically, more research should be geared towards a) deployable and sustainable data collection set-ups in physical learning environments, b) teacher assessment, c) developing feedback and visualization systems and d) promoting inclusivity and generalizability of models across populations.},
	language = {en},
	booktitle = {{LAK19}: 9th {International} {Learning} {Analytics} and {Knowledge} {Conference}},
	publisher = {ACM},
	author = {Chua, Yi Han Victoria and Dauwels, Justin and Tan, Seng Chee},
	year = {2019},
	pages = {10},
}

@inproceedings{chua_technologies_2019-1,
	address = {Tempe AZ USA},
	title = {Technologies for automated analysis of co-located, real-life, physical learning spaces: {Where} are we now?},
	abstract = {The motivation for this paper is derived from the fact that there has been increasing interest among researchers and practitioners in developing technologies that capture, model and analyze learning and teaching experiences that take place beyond computer-based learning environments. In this paper, we review case studies of tools and technologies developed to collect and analyze data in educational settings, quantify learning and teaching processes and support assessment of learning and teaching in an automated fashion. We focus on pipelines that leverage information and data harnessed from physical spaces and/or integrates collected data across physical and digital spaces. Our review reveals a promising field of physical classroom analysis. We describe some trends and suggest potential future directions. Specifically, more research should be geared towards a) deployable and sustainable data collection set-ups in physical learning environments, b) teacher assessment, c) developing feedback and visualization systems and d) promoting inclusivity and generalizability of models across populations.},
	language = {en},
	booktitle = {{LAK19}: 9th {International} {Learning} {Analytics} and {Knowledge} {Conference}},
	publisher = {ACM},
	author = {Chua, Yi Han Victoria and Dauwels, Justin and Tan, Seng Chee},
	year = {2019},
	pages = {10},
}

@phdthesis{qushem_trends_2020,
	title = {Trends of {Multimodal} {Learning} {Analytics}: {A} {Systematic} {Literature} {Review}},
	url = {https://erepo.uef.fi/bitstream/handle/123456789/23508/urn_nbn_fi_uef-20201250.pdf?sequence=1},
	abstract = {Multimodal Learning Analytics (MMLA) has become a promising emerging concept, but higher education practitioners need to familiarize themselves with MMLA issues pertaining to the usage of multimodal approaches in the educational setting. The goal of this review analysis is therefore to identify, analyze and organize ‘MMLA’ literature, concentrating on how this new branch of Learning Analytics (LA) can broaden learning support without technology-mediated tools and the emergence of various educational platforms. This entry seeks to contribute to the knowledge discovery within Education Technology by exploring current methods and approaches to this immersive technique that can enable MMLA to thrive within and beyond the educational boundary, but also by highlighting key challenges currently experienced through the adoption and implementation processes. Thus, the Kitchenham and Charters methodology has been used to conduct this systematic review study on the Multimodal Learning Analytics and its support for educational activities. Analyzing and synthesizing the theoretical underpinnings of numerous studies, a total of 30 high-quality relevant literature meeting the inclusion criteria was considered. The findings of the review analysis presented several interesting methods and potential challenges identified by the different indicators and criteria which will be very useful for education providers or scientific community. The study also recommended a conceptual framework for what first-hand action would be required to develop an MMLA-based system that focuses on student learning and inclusive education.},
	language = {en},
	school = {UNIVERSITY OF EASTERN FINLAND},
	author = {Qushem, Umar Bin},
	month = jun,
	year = {2020},
}

@article{chango_review_2022,
	title = {A review on data fusion in multimodal learning analytics and educational data mining},
	volume = {n/a},
	issn = {1942-4795},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1458},
	doi = {10.1002/widm.1458},
	abstract = {The new educational models such as smart learning environments use of digital and context-aware devices to facilitate the learning process. In this new educational scenario, a huge quantity of multimodal students' data from a variety of different sources can be captured, fused, and analyze. It offers to researchers and educators a unique opportunity of being able to discover new knowledge to better understand the learning process and to intervene if necessary. However, it is necessary to apply correctly data fusion approaches and techniques in order to combine various sources of multimodal learning analytics (MLA). These sources or modalities in MLA include audio, video, electrodermal activity data, eye-tracking, user logs, and click-stream data, but also learning artifacts and more natural human signals such as gestures, gaze, speech, or writing. This survey introduces data fusion in learning analytics (LA) and educational data mining (EDM) and how these data fusion techniques have been applied in smart learning. It shows the current state of the art by reviewing the main publications, the main type of fused educational data, and the data fusion approaches and techniques used in EDM/LA, as well as the main open problems, trends, and challenges in this specific research area. This article is categorized under: Application Areas {\textgreater} Education and Learning},
	language = {en},
	number = {n/a},
	urldate = {2022-06-16},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Chango, Wilson and Lara, Juan A. and Cerezo, Rebeca and Romero, Cristóbal},
	month = feb,
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1458},
	keywords = {data fusion, educational data science, multimodal learning, smart learning},
	pages = {e1458},
}

@article{feng_mapping_2021,
	title = {Mapping {Artificial} {Intelligence} in {Education} {Research}: a {Network}‐based {Keyword} {Analysis}},
	volume = {31},
	issn = {1560-4292, 1560-4306},
	shorttitle = {Mapping {Artificial} {Intelligence} in {Education} {Research}},
	url = {https://link.springer.com/10.1007/s40593-021-00244-4},
	doi = {10.1007/s40593-021-00244-4},
	abstract = {In this study, we review 1830 research articles on artificial intelligence in education (AIED), with the aim of providing a holistic picture of the knowledge evolution in this interdisciplinary research field from 2010 to 2019. A novel three-step approach in the analysis of the keyword co-occurrence networks (KCN) is proposed to identify the knowledge structure, knowledge clusters and trending keywords within AIED over time. The results reveal considerable research diversity in the AIED field, centering around two sustained themes: intelligent tutoring systems (2010-19) and massive open online courses (since 2014). The focal educational concerns reflected in AIED research are: (1) online learning; (2) game-based learning; (3) collaborative learning; (4) assessment; (5) affect; (6) engagement; and (7) learning design. The highly connected keywords relevant to analytic techniques within this field include natural language processing, educational data mining, learning analytics and machine learning. Neural network, deep learning, eye tracking, and personalized learning are trending keywords in this field as they have emerged with key structural roles in the latest two-year period analyzed. This is the first article providing a systematic review of a large body of literature on artificial intelligence in education, and in it we uncover the underlying patterns of knowledge connectivity within the field, as well as provide insight into its future development. The three-step multi-scale (macro, meso, micro) framework proposed in this study can also be applied to map the knowledge development in other scientific research areas.},
	language = {en},
	number = {2},
	urldate = {2023-07-19},
	journal = {International Journal of Artificial Intelligence in Education},
	author = {Feng, Shihui and Law, Nancy},
	month = jun,
	year = {2021},
	pages = {277--303},
}

@incollection{giannakos_once_2022,
	address = {Cham},
	title = {Once {More} with {Feeling}: {Emotions} in {Multimodal} {Learning} {Analytics}},
	isbn = {978-3-031-08075-3 978-3-031-08076-0},
	shorttitle = {Once {More} with {Feeling}},
	url = {https://link.springer.com/10.1007/978-3-031-08076-0_11},
	abstract = {The emotions that students experience when engaging in tasks critically inﬂuence their performance and many models of learning and competence include assumptions about affective variables and respective emotions. However, while researchers agree about the importance of emotions for learning, it remains challenging to connect momentary affect, i.e., emotions, to learning processes. Advances in automated speech recognition and natural language processing (NLP) allow real time detection of emotions in recorded language. We use NLP and machine learning techniques to automatically extract information about students’ motivational states while engaging in the construction of explanations and investigate how this information can help more accurately predict students’ learning over the course of a 10-week energy unit. Our results show how NLP and ML techniques allow the use of different modalities of the same data in order to better understand individual differences in students’ performances. However, in realistic settings, this task remains far from trivial and requires extensive preprocessing of the data and the results need to be interpreted with care and caution. Thus, future research is needed before these methods can be deployed at scale.},
	language = {en},
	urldate = {2023-04-27},
	booktitle = {The {Multimodal} {Learning} {Analytics} {Handbook}},
	publisher = {Springer International Publishing},
	author = {Kubsch, Marcus and Caballero, Daniela and Uribe, Pablo},
	editor = {Giannakos, Michail and Spikol, Daniel and Di Mitri, Daniele and Sharma, Kshitij and Ochoa, Xavier and Hammad, Rawad},
	year = {2022},
	doi = {10.1007/978-3-031-08076-0_11},
	pages = {261--285},
}

@article{larmuseau_multimodal_2020,
	title = {Multimodal learning analytics to investigate cognitive load during online problem solving},
	volume = {51},
	issn = {0007-1013, 1467-8535},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/bjet.12958},
	doi = {10.1111/bjet.12958},
	abstract = {To have insight into cognitive load (CL) during online complex problem solving, this study aimed at measuring CL through physiological data. This study experimentally manipulated intrinsic and extraneous load of exercises in the domain of statistics, resulting in four conditions: high complex with hints, low complex with hints, high complex without hints and low complex without hints. The study had a within-subjectdesign in which 67 students solved the exercises in a randomized order. Self-reported CL was combined with physiological data, namely, galvanic skin response (GSR), skin temperature (ST), heart rate (HR) and heart rate variability (HRV). Multiple imputation was used for handling missing data from resp. 16 and 19 students for GSR/ST and HR/ HRV. First, differences between conditions in view of physiological data were examined. Second, we investigated how much variance of self-reported CL and task performance was explained by physiological data. Finally, we investigated which features can be used to assess (objective) CL. Results revealed no significant differences between the manipulated conditions in terms of physiological data. Nonetheless, HR and ST were significantly related to self-reported CL, whereas ST to task performance. Additionally, this study revealed the potential of ST and HR to assess high CL.},
	language = {en},
	number = {5},
	urldate = {2023-04-27},
	journal = {British Journal of Educational Technology},
	author = {Larmuseau, Charlotte and Cornelis, Jan and Lancieri, Luigi and Desmet, Piet and Depaepe, Fien},
	month = sep,
	year = {2020},
	pages = {1548--1562},
}

@inproceedings{tisza_understanding_2022,
	address = {Braga Portugal},
	title = {Understanding {Fun} in {Learning} to {Code}: {A} {Multi}-{Modal} {Data} approach},
	isbn = {978-1-4503-9197-9},
	shorttitle = {Understanding {Fun} in {Learning} to {Code}},
	url = {https://dl.acm.org/doi/10.1145/3501712.3529716},
	doi = {10.1145/3501712.3529716},
	abstract = {The role of fun in learning, and specifically in learning to code, is critical but not yet fully understood. Fun is typically measured by post session questionnaires, which are coarse-grained, evaluating activities that sometimes last an hour, a day or longer. Here we examine how fun impacts learning during a coding activity, combining continuous physiological response data from wristbands and facial expressions from facial camera videos, along with self-reported measures (i.e. knowledge test and reported fun). Data were collected from primary school students (N = 53) in a single-occasion, two-hours long coding workshop, with the BBC micro:bits. We found that a) sadness, anger and stress are negatively, and arousal is positively related to students’ relative learning gain (RLG), b) experienced fun is positively related to students’ RLG and c) RLG and fun are related to certain physiological markers derived from the physiological response data.},
	language = {en},
	urldate = {2023-04-27},
	booktitle = {Interaction {Design} and {Children}},
	publisher = {ACM},
	author = {Tisza, Gabriella and Sharma, Kshitij and Papavlasopoulou, Sofia and Markopoulos, Panos and Giannakos, Michail},
	month = jun,
	year = {2022},
	pages = {274--287},
}

@article{fwa_investigating_2018,
	title = {Investigating multimodal affect sensing in an affective tutoring system using unobtrusive sensors},
	abstract = {Affect inextricably plays a critical role in the learning process. In this study, we investigate the multimodal fusion of facial, keystrokes, mouse clicks, head posture and contextual features for the detection of student’s frustration in an Affective Tutoring System. The results (AUC=0.64) demonstrated empirically that a multimodal approach offers higher accuracy and better robustness as compared to a unimodal approach. In addition, the inclusion of keystrokes and mouse clicks makes up for the detection gap where video based sensing modes (facial and head postures) are not available. The findings in this paper will dovetail to our end research objective of optimizing the learning of students by adapting empathetically or tailoring to their affective states.},
	language = {en},
	author = {Fwa, Hua Leong and Marshall, Lindsay},
	year = {2018},
}

@incollection{ro_prime_2020,
	address = {Cham},
	title = {{PRIME}: {Block}-{Wise} {Missingness} {Handling} for {Multi}-modalities in {Intelligent} {Tutoring} {Systems}},
	volume = {11962},
	isbn = {978-3-030-37733-5 978-3-030-37734-2},
	shorttitle = {{PRIME}},
	url = {http://link.springer.com/10.1007/978-3-030-37734-2_6},
	abstract = {Block-wise missingness in multimodal data poses a challenging barrier for the analysis over it, which is quite common in practical scenarios such as the multimedia intelligent tutoring systems (ITSs). In this work, we collected data from 194 undergraduates via a biology ITS which involves three modalities: student-system logﬁles, facial expressions, and eye tracking. However, only 32 out of the 194 students had all three modalities and 83\% of them were missing the facial expression data, eye tracking data, or both. To handle such a block-wise missing problem, we propose a Progressively Reﬁned Imputation for Multi-modalities by auto-Encoder (PRIME), which trains the model based on single, pairwise, and entire modalities for imputation in a progressive manner, and therefore enables us to maximally utilize all the available data. We have evaluated PRIME against single-modality log-only (without missingness handling) and ﬁve state-of-the-art missing data handling methods on one important yet challenging student modeling task: to predict students’ learning gains. Our results show that using multimodal data as a result of missing data handling yields better prediction performance than using logﬁles only, and PRIME outperforms other baseline methods for both learning gain prediction and data reconstruction tasks.},
	language = {en},
	urldate = {2023-04-27},
	booktitle = {{MultiMedia} {Modeling}},
	publisher = {Springer International Publishing},
	author = {Yang, Xi and Kim, Yeo-Jin and Taub, Michelle and Azevedo, Roger and Chi, Min},
	editor = {Ro, Yong Man and Cheng, Wen-Huang and Kim, Junmo and Chu, Wei-Ta and Cui, Peng and Choi, Jung-Woo and Hu, Min-Chun and De Neve, Wesley},
	year = {2020},
	doi = {10.1007/978-3-030-37734-2_6},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {63--75},
}

@incollection{eagan_computationally_2019,
	address = {Cham},
	title = {Computationally {Augmented} {Ethnography}: {Emotion} {Tracking} and {Learning} in {Museum} {Games}},
	volume = {1112},
	isbn = {978-3-030-33231-0 978-3-030-33232-7},
	shorttitle = {Computationally {Augmented} {Ethnography}},
	url = {http://link.springer.com/10.1007/978-3-030-33232-7_12},
	abstract = {In this paper, we describe a way of using multi-modal learning analytics to augment qualitative data. We extract facial expressions that may indicate particular emotions from videos of dyads playing an interactive table-top game built for a museum. From this data, we explore the correlation between students’ understanding of the biological and complex systems concepts showcased in the learning environment and their facial expressions. First, we show how information retrieval techniques can be used on facial expression features to investigate emotional variation during key moments of the interaction. Second, we connect these features to moments of learning identiﬁed by traditional qualitative methods. Finally, we present an initial pilot using these methods in concert to identify key moments in multiple modalities. We end with a discussion of our preliminary ﬁndings on interweaving machine and human analytical approaches.},
	language = {en},
	urldate = {2023-04-27},
	booktitle = {Advances in {Quantitative} {Ethnography}},
	publisher = {Springer International Publishing},
	author = {Martin, Kit and Wang, Emily Q. and Bain, Connor and Worsley, Marcelo},
	editor = {Eagan, Brendan and Misfeldt, Morten and Siebert-Evenstone, Amanda},
	year = {2019},
	doi = {10.1007/978-3-030-33232-7_12},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {141--153},
}

@inproceedings{azcona_personalizing_2018,
	address = {San Jose, CA, USA},
	title = {Personalizing {Computer} {Science} {Education} by {Leveraging} {Multimodal} {Learning} {Analytics}},
	isbn = {978-1-5386-1174-6},
	url = {https://ieeexplore.ieee.org/document/8658596/},
	doi = {10.1109/FIE.2018.8658596},
	abstract = {This Research Full Paper implements a framework that harness sources of programming learning analytics on three computer programming courses a Higher Education Institution. The platform, called PredictCS, automatically detects lowerperforming or “at-risk” students in programming courses and automatically and adaptively sends them feedback. This system has been progressively adopted at the classroom level to improve personalized learning. A visual analytics dashboard is developed and accessible to Faculty. This contains information about the models deployed and insights extracted from student’s data. By leveraging historical student data we built predictive models using student characteristics, prior academic history, logged interactions between students and online resources, and students’ progress in programming laboratory work. Predictions were generated every week during the semester’s classes. In addition, during the second half of the semester, students who opted-in received pseudo real-time personalised feedback. Notiﬁcations were personalised based on students’ predicted performance on the course and included a programming suggestion from a topstudent in the class if any programs submitted had failed to meet the speciﬁed criteria. As a result, this helped students who corrected their programs to learn more and reduced the gap between lower and higher-performing students.},
	language = {en},
	urldate = {2023-04-27},
	booktitle = {2018 {IEEE} {Frontiers} in {Education} {Conference} ({FIE})},
	publisher = {IEEE},
	author = {Azcona, David and Hsiao, I-Han and Smeaton, Alan F.},
	month = oct,
	year = {2018},
	pages = {1--9},
}

@article{starr_toward_2018,
	title = {Toward {Using} {Multi}-{Modal} {Learning} {Analytics} to {Support} and {Measure} {Collaboration} in {Co}-{Located} {Dyads}},
	abstract = {This paper describes an empirical study where the productive interactions of small collaborative learning groups in response to two collaboration interventions were evaluated through traditional and multi-modal data collection methods. We asked 42 pairs (N= 84) of participants to program a robot to solve a series of mazes. Participants had no prior programming experience, and we used a block-based environment with pre-made functions as well as video tutorials to scaffold the activity. We explored 2 interventions to support their collaboration: a real-time visualization of their verbal contribution and a short verbal explanation of the benefits of collaboration for learning. This paper describes our experimental design, the effect of the interventions, preliminary results from the Kinect sensor, and our future plans to analyze additional sensor data. We conclude by highlighting the importance of capturing and supporting 21st century skills (i.e., collaboration and effective communication) in small groups of students.},
	language = {en},
	author = {Starr, Emma L and Reilly, Joseph M and Schneider, Bertrand},
	year = {2018},
}

@article{noel_exploring_2018,
	title = {Exploring {Collaborative} {Writing} of {User} {Stories} {With} {Multimodal} {Learning} {Analytics}: {A} {Case} {Study} on a {Software} {Engineering} {Course}},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Exploring {Collaborative} {Writing} of {User} {Stories} {With} {Multimodal} {Learning} {Analytics}},
	url = {https://ieeexplore.ieee.org/document/8496762/},
	doi = {10.1109/ACCESS.2018.2876801},
	abstract = {Software engineering is the application of principles used in engineering design, development, testing, deployment, and management of software systems. One of the software engineering’s approaches, highly used in new industries, is agile development. User stories are a commonly used notation to capture user requirements in agile development. Nevertheless, for the elaboration of user stories, a high level of collaboration with the client is necessary. This professional skill is rarely measured or evaluated in educational contexts. The present work approaches collaboration in software engineering students through multimodal learning analytics, modeling, and evaluating students’ collaboration while they are writing user stories. For that, we used multidirectional microphones in order to derive social network analysis metrics related to collaboration (permanence and prompting) together with human-annotated information (quality of the stories and productivity). Results show that groups with a lower productivity in writing user stories and less professional experience in managing software requirements present a non-collaborative behavior more frequently, and that teams with a fewer number of interventions are more likely to produce a greater number of user stories. Moreover, although low experience subjects produced more user stories, a greater productivity of the most experienced subjects was not statistically veriﬁed. We believe that these types of initiatives will allow the measurement and early development of such skills in university students.},
	language = {en},
	urldate = {2023-04-27},
	journal = {IEEE Access},
	author = {Noel, Rene and Riquelme, Fabian and Lean, Roberto Mac and Merino, Erick and Cechinel, Cristian and Barcelos, Thiago S. and Villarroel, Rodolfo and Munoz, Roberto},
	year = {2018},
	pages = {67783--67798},
}

@article{chango_multi-source_2021,
	title = {Multi-source and multimodal data fusion for predicting academic performance in blended learning university courses},
	volume = {89},
	issn = {00457906},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045790620307606},
	doi = {10.1016/j.compeleceng.2020.106908},
	abstract = {In this paper we apply data fusion approaches for predicting the final academic performance of university students using multiple-source, multimodal data from blended learning environments. We collect and preprocess data about first-year university students from different sources: theory classes, practical sessions, on-line Moodle sessions, and a final exam. Our objective is to discover which data fusion approach produces the best results using our data. We carry out experiments by applying four different data fusion approaches and six classification algorithms. The results show that the best predictions are produced using ensembles and selecting the best attributes approach with discretized data. The best prediction models show us that the level of attention in theory classes, scores in Moodle quizzes, and the level of activity in Moodle forums are the best set of attributes for predicting students’ final performance in our courses.},
	language = {en},
	urldate = {2023-04-27},
	journal = {Computers \& Electrical Engineering},
	author = {Chango, Wilson and Cerezo, Rebeca and Romero, Cristóbal},
	month = jan,
	year = {2021},
	pages = {106908},
}

@article{birt_mobile_2018,
	title = {Mobile {Mixed} {Reality} for {Experiential} {Learning} and {Simulation} in {Medical} and {Health} {Sciences} {Education}},
	volume = {9},
	issn = {2078-2489},
	url = {http://www.mdpi.com/2078-2489/9/2/31},
	doi = {10.3390/info9020031},
	abstract = {New accessible learning methods delivered through mobile mixed reality are becoming possible in education, shifting pedagogy from the use of two dimensional images and videos to facilitating learning via interactive mobile environments. This is especially important in medical and health education, where the required knowledge acquisition is typically much more experiential, self-directed, and hands-on than in many other disciplines. Presented are insights obtained from the implementation and testing of two mobile mixed reality interventions across two Australian higher education classrooms in medicine and health sciences, concentrating on student perceptions of mobile mixed reality for learning physiology and anatomy in a face-to-face medical and health science classroom and skills acquisition in airways management focusing on direct laryngoscopy with foreign body removal in a distance paramedic science classroom. This is unique because most studies focus on a single discipline, focusing on either skills or the learner experience and a single delivery modality rather than linking cross-discipline knowledge acquisition and the development of a student’s tangible skills across multimodal classrooms. Outcomes are presented from post-intervention student interviews and discipline academic observation, which highlight improvements in learner motivation and skills, but also demonstrated pedagogical challenges to overcome with mobile mixed reality learning.},
	language = {en},
	number = {2},
	urldate = {2023-04-27},
	journal = {Information},
	author = {Birt, James and Stromberga, Zane and Cowling, Michael and Moro, Christian},
	month = jan,
	year = {2018},
	pages = {31},
}

@article{reilly_exploring_nodate,
	title = {Exploring {Collaboration} {Using} {Motion} {Sensors} and {Multi}- {Modal} {Learning} {Analytics}},
	abstract = {In this paper, we describe the analysis of multimodal data collected on small collaborative learning groups. In a previous study [1], we asked pairs (N=84) with no programming experience to program a robot to solve a series of mazes. The quality of the dyad’s collaboration was evaluated, and two interventions were implemented to support collaborative learning. In the current study, we present the analysis of KinectTM and speech data gathered on dyads during the programming task. We first show how certain movements and patterns of gestures correlate positively with collaboration and learning gains. We next use clustering algorithms to find prototypical body positions of participants and relate amount of time spent in certain postures with learning gains as in Schneider \& Blikstein’s work [2]. Finally, we examine measures of proxemics and physical orientation within the dyads to explore how to detect good collaboration. We discuss the relevance of these results to designing and assessing collaborative small group activities and outline future work related to other collected sensor data.},
	language = {en},
	author = {Reilly, Joseph M and Ravenell, Milan and Schneider, Bertrand},
}

@article{standen_evaluation_2020,
	title = {An evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities},
	volume = {51},
	issn = {0007-1013, 1467-8535},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/bjet.13010},
	doi = {10.1111/bjet.13010},
	abstract = {Artificial intelligence tools for education (AIEd) have been used to automate the provision of learning support to mainstream learners. One of the most innovative approaches in this field is the use of data and machine learning for the detection of a student’s affective state, to move them out of negative states that inhibit learning, into positive states such as engagement. In spite of their obvious potential to provide the personalisation that would give extra support for learners with intellectual disabilities, little work on AIEd systems that utilise affect recognition currently addresses this group. Our system used multimodal sensor data and machine learning to first identify three affective states linked to learning (engagement, frustration, boredom) and second determine the presentation of learning content so that the learner is maintained in an optimal affective state and rate of learning is maximised. To evaluate this adaptive learning system, 67 participants aged between 6 and 18 years acting as their own control took part in a series of sessions using the system. Sessions alternated between using the system with both affect detection and learning achievement to drive the selection of learning content (intervention) and using learning achievement alone (control) to drive the selection of learning content. Lack of boredom was the state with the strongest link to achievement, with both frustration and engagement positively related to achievement. There was significantly more engagement and less boredom in intervention than control sessions, but no significant difference in achievement. These results suggest that engagement does increase when activities are tailored to the personal needs and emotional state of the learner and that the system was promoting affective states that in turn promote learning. However, longer exposure is necessary to determine the effect on learning.},
	language = {en},
	number = {5},
	urldate = {2023-04-27},
	journal = {British Journal of Educational Technology},
	author = {Standen, Penelope J. and Brown, David J. and Taheri, Mohammad and Galvez Trigo, Maria J. and Boulton, Helen and Burton, Andrew and Hallewell, Madeline J. and Lathe, James G. and Shopland, Nicholas and Blanco Gonzalez, Maria A. and Kwiatkowska, Gosia M. and Milli, Elena and Cobello, Stefano and Mazzucato, Annaleda and Traversi, Marco and Hortal, Enrique},
	month = sep,
	year = {2020},
	pages = {1748--1765},
}

@article{fernandez-nieto_storytelling_2021,
	title = {Storytelling {With} {Learner} {Data}: {Guiding} {Student} {Reflection} on {Multimodal} {Team} {Data}},
	volume = {14},
	issn = {1939-1382, 2372-0050},
	shorttitle = {Storytelling {With} {Learner} {Data}},
	url = {https://ieeexplore.ieee.org/document/9632388/},
	doi = {10.1109/TLT.2021.3131842},
	abstract = {There is growing interest in creating learning analytics feedback interfaces that support students directly. While dashboards and other visualizations are proliferating, the evidence is that many fail to provide meaningful insights that help students reﬂect productively. The contribution of this paper is qualitative and quantitative evidence from two studies evaluating a multimodal teamwork analytics tool in authentic clinical teamwork simulations. Collocated activity data is rendered to help nursing students reﬂect on errors and stress–related incidents during simulations. The user interface explicitly guides student reﬂection using data storytelling principles, tuned to the intended learning outcomes. The results demonstrate the potential of interfaces that “tell one data story at a time”, by helping students to identify misconceptions and errors, think about strategies they might use to address errors, and reﬂect on their arousal levels. The results also illuminate broader issues around automated formative assessment, and the intelligibility and accountability of learning analytics.},
	language = {en},
	number = {5},
	urldate = {2023-04-27},
	journal = {IEEE Transactions on Learning Technologies},
	author = {Fernandez-Nieto, Gloria Milena and Echeverria, Vanessa and Shum, Simon Buckingham and Mangaroska, Katerina and Kitto, Kirsty and Palominos, Evelyn and Axisa, Carmen and Martinez-Maldonado, Roberto},
	month = oct,
	year = {2021},
	pages = {695--708},
}

@inproceedings{martinez-maldonado_data_2020,
	address = {Honolulu HI USA},
	title = {From {Data} to {Insights}: {A} {Layered} {Storytelling} {Approach} for {Multimodal} {Learning} {Analytics}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {From {Data} to {Insights}},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376148},
	doi = {10.1145/3313831.3376148},
	abstract = {Significant progress to integrate and analyse multimodal data has been carried out in the last years. Yet, little research has tackled the challenge of visualising and supporting the sensemaking of multimodal data to inform teaching and learning. It is naïve to expect that simply by rendering multiple data streams visually, a teacher or learner will be able to make sense of them. This paper introduces an approach to unravel the complexity of multimodal data by organising it into meaningful layers that explain critical insights to teachers and students. The approach is illustrated through the design of two data storytelling prototypes in the context of nursing simulation. Two authentic studies with educators and students identified the potential of the approach to create learning analytics interfaces that communicate insights on team performance, as well as concerns in terms of accountability and automated insights discovery.},
	language = {en},
	urldate = {2023-04-27},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Martinez-Maldonado, Roberto and Echeverria, Vanessa and Fernandez Nieto, Gloria and Buckingham Shum, Simon},
	month = apr,
	year = {2020},
	pages = {1--15},
}

@incollection{nkambou_predicting_2018,
	address = {Cham},
	title = {Predicting {Learners}’ {Emotions} in {Mobile} {MOOC} {Learning} via a {Multimodal} {Intelligent} {Tutor}},
	volume = {10858},
	isbn = {978-3-319-91463-3 978-3-319-91464-0},
	url = {http://link.springer.com/10.1007/978-3-319-91464-0_15},
	abstract = {Massive Open Online Courses (MOOCs) are a promising approach for scalable knowledge dissemination. However, they also face major challenges such as low engagement, low retention rate, and lack of personalization. We propose AttentiveLearner2, a multimodal intelligent tutor running on unmodiﬁed smartphones, to supplement today’s clickstream-based learning analytics for MOOCs. AttentiveLearner2 uses both the front and back cameras of a smartphone as two complementary and ﬁne-grained feedback channels in real time: the back camera monitors learners’ photoplethysmography (PPG) signals and the front camera tracks their facial expressions during MOOC learning. AttentiveLearner2 implicitly infers learners’ aﬀective and cognitive states during learning from their PPG signals and facial expressions. Through a 26-participant user study, we found that: (1) AttentiveLearner2 can detect 6 emotions in mobile MOOC learning reliably with high accuracy (average accuracy = 84.4\%); (2) the detected emotions can predict learning outcomes (best R2 = 50.6\%); and (3) it is feasible to track both PPG signals and facial expressions in real time in a scalable manner on today’s unmodiﬁed smartphones.},
	language = {en},
	urldate = {2023-04-27},
	booktitle = {Intelligent {Tutoring} {Systems}},
	publisher = {Springer International Publishing},
	author = {Pham, Phuong and Wang, Jingtao},
	editor = {Nkambou, Roger and Azevedo, Roger and Vassileva, Julita},
	year = {2018},
	doi = {10.1007/978-3-319-91464-0_15},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {150--159},
}

@inproceedings{emerson_early_2020,
	address = {Virtual Event Netherlands},
	title = {Early {Prediction} of {Visitor} {Engagement} in {Science} {Museums} with {Multimodal} {Learning} {Analytics}},
	isbn = {978-1-4503-7581-8},
	url = {https://dl.acm.org/doi/10.1145/3382507.3418890},
	doi = {10.1145/3382507.3418890},
	abstract = {Modeling visitor engagement is a key challenge in informal learning environments, such as museums and science centers. Devising predictive models of visitor engagement that accurately forecast salient features of visitor behavior, such as dwell time, holds significant potential for enabling adaptive learning environments and visitor analytics for museums and science centers. In this paper, we introduce a multimodal early prediction approach to modeling visitor engagement with interactive science museum exhibits. We utilize multimodal sensor data—including eye gaze, facial expression, posture, and interaction log data—captured during visitor interactions with an interactive museum exhibit for environmental science education, to induce predictive models of visitor dwell time. We investigate machine learning techniques (random forest, support vector machine, Lasso regression, gradient boosting trees, and multi-layer perceptron) to induce multimodal predictive models of visitor engagement with data from 85 museum visitors. Results from a series of ablation experiments suggest that incorporating additional modalities into predictive models of visitor engagement improves model accuracy. In addition, the models show improved predictive performance over time, demonstrating that increasingly accurate predictions of visitor dwell time can be achieved as more evidence becomes available from visitor interactions with interactive science museum exhibits. These findings highlight the efficacy of multimodal data for modeling museum exhibit visitor engagement.},
	language = {en},
	urldate = {2023-04-27},
	booktitle = {Proceedings of the 2020 {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {ACM},
	author = {Emerson, Andrew and Henderson, Nathan and Rowe, Jonathan and Min, Wookhee and Lee, Seung and Minogue, James and Lester, James},
	month = oct,
	year = {2020},
	pages = {107--116},
}

@inproceedings{liu_towards_2018,
	address = {Wollongong, NSW},
	title = {Towards {Smart} {Educational} {Recommendations} with {Reinforcement} {Learning} in {Classroom}},
	isbn = {978-1-5386-6522-0},
	url = {https://ieeexplore.ieee.org/document/8615217/},
	doi = {10.1109/TALE.2018.8615217},
	abstract = {In this paper, we propose to construct a cyberphysical-social system that uses multiple sensors such as cameras and a quiz creator to track the learning process of the students and applies reinforcement learning techniques to provide learning guidance based on the multi-modal sensing data in smart classroom. More specifically, the smart learning recommendation system measures the heartbeats, quiz scores, blinks and facial expressions of each student to formulate the learning states and applies reinforcement learning to recommend the effective learning activities for students based on their current learning states. The interactive learning recommendation process in a smart classroom with multiple sensors can be modeled as a Markov decision process. Our simulation results have preliminarily demonstrated the effectiveness of this smart learning recommendation system. This work may provide insights into constructing a future intelligent learning environment for enriched personalized experiences.},
	language = {en},
	urldate = {2023-04-27},
	booktitle = {2018 {IEEE} {International} {Conference} on {Teaching}, {Assessment}, and {Learning} for {Engineering} ({TALE})},
	publisher = {IEEE},
	author = {Liu, Su and Chen, Ye and Huang, Hui and Xiao, Liang and Hei, Xiaojun},
	month = dec,
	year = {2018},
	pages = {1079--1084},
}

@article{liu_learning_2019,
	title = {Learning linkages: {Integrating} data streams of multiple modalities and timescales},
	volume = {35},
	issn = {0266-4909, 1365-2729},
	shorttitle = {Learning linkages},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/jcal.12315},
	doi = {10.1111/jcal.12315},
	abstract = {Increasingly, student work is being conducted on computers and online, producing vast amounts of learning‐related data. The educational analytics fields have produced many insights about learning based solely on tutoring systems' automatically logged data, or “log data.” But log data leave out important contextual information about the learning experience. For example, a student working at a computer might be working independently with few outside influences. Alternatively, he or she might be in a lively classroom, with other students around, talking and offering suggestions. Tools that capture these other experiences have potential to augment and complement log data. However, the collection of rich, multimodal data streams and the increased complexity and heterogeneity in the resulting data pose many challenges to researchers. Here, we present two empirical studies that take advantage of multimodal data sources to enrich our understanding of student learning. We leverage and extend quantitative models of student learning to incorporate insights derived jointly from data collected in multiple modalities (log data, video, and high‐fidelity audio) and contexts (individual vs. collaborative classroom learning). We discuss the unique benefits of multimodal data and present methods that take advantage of such benefits while easing the burden on researchers' time and effort.},
	language = {en},
	number = {1},
	urldate = {2023-04-19},
	journal = {Journal of Computer Assisted Learning},
	author = {Liu, Ran and Stamper, John and Davenport, Jodi and Crossley, Scott and McNamara, Danielle and Nzinga, Kalonji and Sherin, Bruce},
	month = feb,
	year = {2019},
	pages = {99--109},
}

@article{liu_learning_2019-1,
	title = {Learning linkages: {Integrating} data streams of multiple modalities and timescales},
	volume = {35},
	issn = {0266-4909, 1365-2729},
	shorttitle = {Learning linkages},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/jcal.12315},
	doi = {10.1111/jcal.12315},
	language = {en},
	number = {1},
	urldate = {2023-04-19},
	journal = {Journal of Computer Assisted Learning},
	author = {Liu, Ran and Stamper, John and Davenport, Jodi and Crossley, Scott and McNamara, Danielle and Nzinga, Kalonji and Sherin, Bruce},
	month = feb,
	year = {2019},
	pages = {99--109},
}

@article{cukurova_artificial_2019,
	title = {Artificial intelligence and multimodal data in the service of human decision‐making: {A} case study in debate tutoring},
	volume = {50},
	issn = {0007-1013, 1467-8535},
	shorttitle = {Artificial intelligence and multimodal data in the service of human decision‐making},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/bjet.12829},
	doi = {10.1111/bjet.12829},
	abstract = {Abstract: The question: ‘What is an appropriate role for AI?’ is the subject of much discussion and interest. Arguments about whether AI should be a human replacing technology or a human assisting technology frequently take centre stage. Education is no exception when it comes to questions about the role that AI should play, and as with many other professional areas, the exact role of AI in education is not easy to predict. Here, we argue that one potential role for AI in education is to provide opportunities for human intelligence augmentation, with AI supporting us in decision-making processes, rather than replacing us through automation. To provide empirical evidence to support our argument, we present a case study in the context of debate tutoring, in which we use prediction and classification models to increase the transparency of the intuitive decision-making processes of expert tutors for advanced reflections and feedback. Furthermore, we compare the accuracy of unimodal and multimodal classification models of expert human tutors’ decisions about the social and emotional aspects of tutoring while evaluating trainees. Our results show that multimodal data leads to more accurate classification models in the context we studied.},
	language = {en},
	number = {6},
	urldate = {2023-04-19},
	journal = {British Journal of Educational Technology},
	author = {Cukurova, Mutlu and Kent, Carmel and Luckin, Rosemary},
	month = nov,
	year = {2019},
	pages = {3032--3046},
}

@article{tanaka_embodied_2017,
	title = {Embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders},
	volume = {12},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0182151},
	doi = {10.1371/journal.pone.0182151},
	abstract = {Social skills training, performed by human trainers, is a well-established method for obtaining appropriate skills in social interaction. Previous work automated the process of social skills training by developing a dialogue system that teaches social communication skills through interaction with a computer avatar. Even though previous work that simulated social skills training only considered acoustic and linguistic information, human social skills trainers take into account visual and other non-verbal features. In this paper, we create and evaluate a social skills training system that closes this gap by considering the audiovisual features of the smiling ratio and the head pose (yaw and pitch). In addition, the previous system was only tested with graduate students; in this paper, we applied our system to children or young adults with autism spectrum disorders. For our experimental evaluation, we recruited 18 members from the general population and 10 people with autism spectrum disorders and gave them our proposed multimodal system to use. An experienced human social skills trainer rated the social skills of the users. We evaluated the system’s effectiveness by comparing pre- and post-training scores and identified significant improvement in their social skills using our proposed multimodal system. Computer-based social skills training is useful for people who experience social difficulties. Such a system can be used by teachers, therapists, and social skills trainers for rehabilitation and the supplemental use of human-based training anywhere and anytime.},
	language = {en},
	number = {8},
	urldate = {2023-04-19},
	journal = {PLOS ONE},
	author = {Tanaka, Hiroki and Negoro, Hideki and Iwasaka, Hidemi and Nakamura, Satoshi},
	editor = {Sakakibara, Manabu},
	month = aug,
	year = {2017},
	pages = {e0182151},
}

@article{liu_learning_2019-2,
	title = {Learning linkages: {Integrating} data streams of multiple modalities and timescales},
	volume = {35},
	issn = {0266-4909, 1365-2729},
	shorttitle = {Learning linkages},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/jcal.12315},
	doi = {10.1111/jcal.12315},
	language = {en},
	number = {1},
	urldate = {2023-04-19},
	journal = {Journal of Computer Assisted Learning},
	author = {Liu, Ran and Stamper, John and Davenport, Jodi and Crossley, Scott and McNamara, Danielle and Nzinga, Kalonji and Sherin, Bruce},
	month = feb,
	year = {2019},
	pages = {99--109},
}

@article{jarvela_what_2021,
	title = {What multimodal data can tell us about the students’ regulation of their learning process?},
	volume = {72},
	issn = {09594752},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095947521830416X},
	doi = {10.1016/j.learninstruc.2019.04.004},
	language = {en},
	urldate = {2023-04-05},
	journal = {Learning and Instruction},
	author = {Järvelä, Sanna and Malmberg, Jonna and Haataja, Eetu and Sobocinski, Marta and Kirschner, Paul A.},
	month = apr,
	year = {2021},
	pages = {101203},
}

@article{ochoa_chapter_nodate,
	title = {Chapter 6: {Multimodal} {Learning} {Analytics} - {Rationale}, {Process}, {Examples}, and {Direction}},
	abstract = {This chapter is an introduction to the use of multiple modalities of learning trace data to better understand and feedback learning processes that occur both in digital and face-to-face contexts. First, it will explain the rationale behind the emergence of this type of study, followed by a brief explanation of what Multimodal Learning Analytics (MmLA) is based on current conceptual understandings and current state-of-the-art implementations. The majority of this chapter is dedicated to describing the general process of MmLA from the mapping of learning constructs to low-level multimodal learning traces to the reciprocal implementation of multimedia recording, multimodal feature extraction, analysis, and fusion to detect behavioral markers and estimate the studied constructs. This process is illustrated by the detailed dissection of a real-world example. This chapter concludes with a discussion of the current challenges facing the field and the directions in which the field is moving to address them.},
	language = {en},
	author = {Ochoa, Xavier},
}

@article{mangaroska_challenges_2021,
	title = {Challenges and opportunities of multimodal data in human learning: {The} computer science students' perspective},
	volume = {37},
	issn = {0266-4909, 1365-2729},
	shorttitle = {Challenges and opportunities of multimodal data in human learning},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/jcal.12542},
	doi = {10.1111/jcal.12542},
	abstract = {Multimodal data have the potential to explore emerging learning practices that extend human cognitive capacities. A critical issue stretching in many multimodal learning analytics (MLA) systems and studies is the current focus aimed at supporting researchers to model learner behaviours, rather than directly supporting learners. Moreover, many MLA systems are designed and deployed without learners' involvement. We argue that in order to create MLA interfaces that directly support learning, we need to gain an expanded understanding of how multimodal data can support learners' authentic needs. We present a qualitative study in which 40 computer science students were tracked in an authentic learning activity using wearable and static sensors. Our findings outline learners' curated representations about multimodal data and the non-technical challenges in using these data in their learning practice. The paper discusses 10 dimensions that can serve as guidelines for researchers and designers to create effective and ethically aware student-facing MLA innovations.},
	language = {en},
	number = {4},
	urldate = {2023-03-22},
	journal = {Journal of Computer Assisted Learning},
	author = {Mangaroska, Katerina and Martinez‐Maldonado, Roberto and Vesin, Boban and Gašević, Dragan},
	month = aug,
	year = {2021},
	pages = {1030--1047},
}

@inproceedings{di_mitri_learning_2017,
	address = {Vancouver British Columbia Canada},
	title = {Learning pulse: a machine learning approach for predicting performance in self-regulated learning using multimodal data},
	isbn = {978-1-4503-4870-6},
	shorttitle = {Learning pulse},
	url = {https://dl.acm.org/doi/10.1145/3027385.3027447},
	doi = {10.1145/3027385.3027447},
	abstract = {Learning Pulse explores whether using a machine learning approach on multimodal data such as heart rate, step count, weather condition and learning activity can be used to predict learning performance in self-regulated learning settings. An experiment was carried out lasting eight weeks involving PhD students as participants, each of them wearing a Fitbit HR wristband and having their application on their computer recorded during their learning and working activities throughout the day. A software infrastructure for collecting multimodal learning experiences was implemented. As part of this infrastructure a Data Processing Application was developed to pre-process, analyse and generate predictions to provide feedback to the users about their learning performance. Data from diﬀerent sources were stored using the xAPI standard into a cloud-based Learning Record Store. The participants of the experiment were asked to rate their learning experience through an Activity Rating Tool indicating their perceived level of productivity, stress, challenge and abilities. These self-reported performance indicators were used as markers to train a Linear Mixed Eﬀect Model to generate learner-speciﬁc predictions of the learning performance. We discuss the advantages and the limitations of the used approach, highlighting further development points.},
	language = {en},
	urldate = {2023-03-22},
	booktitle = {Proceedings of the {Seventh} {International} {Learning} {Analytics} \& {Knowledge} {Conference}},
	publisher = {ACM},
	author = {Di Mitri, Daniele and Scheffel, Maren and Drachsler, Hendrik and Börner, Dirk and Ternier, Stefaan and Specht, Marcus},
	month = mar,
	year = {2017},
	pages = {188--197},
}

@article{junokas_using_nodate,
	title = {Using {One}-{Shot} {Machine} {Learning} to {Implement} {Real}- {Time} {Multimodal} {Learning} {Analytics}},
	abstract = {Educational research has demonstrated the importance of embodiment in the design of student learning environments, connecting bodily actions to critical concepts. Gestural recognition algorithms have become important tools in leveraging this connection but are limited in their development, focusing primarily on traditional machine-learning paradigms.},
	language = {en},
	author = {Junokas, Michael J and Kohlburn, Greg and Kumar, Sahil and Lane, Benjamin and Fu, Wai-Tat and Lindgren, Robb},
}

@article{noauthor_educational_nodate,
	title = {Educational {Technology} \& {Society}},
	abstract = {This study integrated the multimodal framework of learning analytics (IMFLA) with the concept mapping (Cmap) approach to improve students’ vocabulary and reading abilities. A total of 70 participants were divided into 2 classes, Class 1 (experimental) and Class 2 (control), for a 1-year period. Vocabulary and reading tests were implemented 3 times. Repeated measures were conducted to test the effect of the program. The results indicated that Time had a significant effect on enhancing both outcome measures (p {\textless} .01 for vocabulary; p {\textless} .001 for reading). Moreover, significant interaction effects between Time and the program on both vocabulary (p {\textless} .05) and reading (p {\textless} .001) further suggest that a longer period of time spent on the program would result in a significant effect on both the vocabulary and reading outcome measures. That is, a significant interaction effect occurred between IMFLA and the time factor. Such interaction effects resulted in better vocabulary and reading abilities when students of Class 1 spent more time on the IMFLA procedure. The multimodal and learning analyses of students’ weekly logs confirmed this improvement. We therefore suggest that instructors use digitalized wordlists and Cmaps in language instruction to enhance students’ vocabulary and reading abilities.},
	language = {en},
}

@article{lee-cultura_childrens_2022,
	title = {Children’s play and problem-solving in motion-based learning technologies using a multi-modal mixed methods approach},
	volume = {31},
	issn = {22128689},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2212868921000647},
	doi = {10.1016/j.ijcci.2021.100355},
	abstract = {Motion-Based Learning Technologies (MBLT) offer a promising approach for integrating play and problem-solving behaviour within children’s learning. The proliferation of sensor technology has driven the field of learning technology towards the development of tools and methods that may benefit from the produced Multi-Modal Data (MMD). Such data can be used to uncover cognitive, affective and physiological processes during learning activities. Combining MMD with more traditionally exercised assessment tools, such as video content analysis, provides a more holistic understanding of children’s learning experiences and has the potential to enable the design of educational technologies capable of harmonising children’s cognitive, affective and physiological processes, while promoting appropriately balanced play and problem-solving efforts. However, the use of an MMD mixed methods approach that combines qualitative and MMD data to understand children’s behaviours during engagement with MBLT is rather unexplored. We present an in-situ study where 26 children, ages 10–12, solved a motion-based sorting task for learning geometry. We continuously and unobtrusively monitored children’s learning experiences using MMD collection via eye-trackers, wristbands, Kinect joint tracking, and a web camera. We devised SP3, a novel observational scheme that can be used to understand children’s solo interactions with MBLT, and applied it to identify and extract children’s evoked play and problem-solving behaviour. Collective analysis of the MMD and video codes provided explanations of children’s task performance through consideration of their holistic learning experience. Lastly, we applied predictive modelling to identify the synergies between various MMD measurements and children’s play and problem-solving behaviours. This research sheds light on the opportunities offered in the confluence of video coding (a traditional method in learning sciences) and MMD (an emerging method that leverages sensors proliferation) for investigating children’s behaviour with MBLT.},
	language = {en},
	urldate = {2023-02-10},
	journal = {International Journal of Child-Computer Interaction},
	author = {Lee-Cultura, Serena and Sharma, Kshitij and Giannakos, Michail},
	month = mar,
	year = {2022},
	pages = {100355},
}

@article{liu_novel_2018,
	title = {A {Novel} {Method} for the {In}-{Depth} {Multimodal} {Analysis} of {Student} {Learning} {Trajectories} in {Intelligent} {Tutoring} {Systems}},
	volume = {5},
	issn = {1929-7750},
	url = {https://learning-analytics.info/index.php/JLA/article/view/5423},
	doi = {10.18608/jla.2018.51.4},
	abstract = {Temporal analyses are critical to understanding learning processes, yet understudied in education research. Data from different sources are often collected at different grain sizes, which are difficult to integrate. Making sense of data at many levels of analysis, including the most detailed levels, is highly time-consuming. In this paper, we describe a generalizable approach for more efficient yet rich sensemaking of temporal data during student use of intelligent tutoring systems. This multi-step approach involves using coarse-grain temporality — learning trajectories across knowledge components — to identify and further explore “focal” moments worthy of more finegrain, context-rich analysis. We discuss the application of this approach to data collected from a classroom study in which students engaged in a Chemistry Virtual Lab tutoring system. We show that the application of this multistep approach efficiently led to interpretable and actionable insights while making use of the richness of the available data. This method is generalizable to many types of datasets and can help handle large volumes of rich data at multiple levels of granularity. We argue that it can be a valuable approach to tackling some of the most prohibitive methodological challenges involved in temporal learning analytics.},
	language = {en},
	number = {1},
	urldate = {2023-02-10},
	journal = {Journal of Learning Analytics},
	author = {Liu, Ran and Stamper, John C and Davenport, Jodi},
	month = apr,
	year = {2018},
}

@inproceedings{lee-cultura_childrens_2021,
	address = {Athens Greece},
	title = {Children’s {Play} and {Problem} {Solving} in {Motion}-{Based} {Educational} {Games}: {Synergies} between {Human} {Annotations} and {Multi}-{Modal} {Data}},
	isbn = {978-1-4503-8452-0},
	shorttitle = {Children’s {Play} and {Problem} {Solving} in {Motion}-{Based} {Educational} {Games}},
	url = {https://dl.acm.org/doi/10.1145/3459990.3460702},
	doi = {10.1145/3459990.3460702},
	abstract = {Identifying and supporting children’s play and problem solving behaviour is important for designing educational technologies. This can inform feedback mechanisms to scaffold learning (provide hints or progress information), and assist facilitators (teachers, parents) in supporting children. Traditionally, researchers manually code video to dissect children’s nuanced play and problem solving behaviour. Advancements in sensing technologies and their respective MultiModal Data (MMD), afford observation of invisible states (cognitive, affective, physiological), and provide opportunities to inspect internal processes experienced during learning and play. However, limited research combines traditional video annotations and MMD to understand children’s behaviour as they interact with educational technology. To address this concern, we collected data from webcam, wristband, eye-trackers, and Kinect, as 26 children, aged 10-12, played a Motion-Based Educational Games (MBEG). Results showed significant differences in children’s experience during play and problem solving episodes, and motivate design considerations aimed to facilitate children’s interactions with MBEG.},
	language = {en},
	urldate = {2023-02-10},
	booktitle = {Interaction {Design} and {Children}},
	publisher = {ACM},
	author = {Lee-Cultura, Serena and Sharma, Kshitij and Cosentino, Giulia and Papavlasopoulou, Sofia and Giannakos, Michail},
	month = jun,
	year = {2021},
	pages = {408--420},
}

@inproceedings{aslan_investigating_2019,
	address = {Glasgow Scotland Uk},
	title = {Investigating the {Impact} of a {Real}-time, {Multimodal} {Student} {Engagement} {Analytics} {Technology} in {Authentic} {Classrooms}},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300534},
	doi = {10.1145/3290605.3300534},
	abstract = {We developed a real-time, multimodal Student Engagement Analytics Technology so that teachers can provide just-in-time personalized support to students who risk disengagement. To investigate the impact of the technology, we ran an exploratory semester-long study with a teacher in two classrooms. We used a multi-method approach consisting of a quasi-experimental design to evaluate the impact of the technology and a case study design to understand the environmental and social factors surrounding the classroom setting. The results show that the technology had a significant impact on the teacher’s classroom practices (i.e., increased scaffolding to the students) and student engagement (i.e., less boredom). These results suggest that the technology has the potential to support teachers’ role of being a coach in technology-mediated learning environments.},
	language = {en},
	urldate = {2023-02-10},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Aslan, Sinem and Alyuz, Nese and Tanriover, Cagri and Mete, Sinem E. and Okur, Eda and D'Mello, Sidney K. and Arslan Esme, Asli},
	month = may,
	year = {2019},
	pages = {1--12},
}

@inproceedings{nasir_is_2020,
	address = {Virtual Event Netherlands},
	title = {Is {There} '{ONE} way' of {Learning}? {A} {Data}-driven {Approach}},
	isbn = {978-1-4503-8002-7},
	shorttitle = {Is {There} '{ONE} way' of {Learning}?},
	url = {https://dl.acm.org/doi/10.1145/3395035.3425200},
	doi = {10.1145/3395035.3425200},
	abstract = {Intelligent Tutoring Systems (ITS) are required to intervene in a learning activity while it is unfolding, to support the learner. To do so, they often rely on performance of a learner, as an approximation for engagement in the learning process. However, in learning tasks that are exploratory by design, such as constructivist learning activities, performance in the task can be misleading and may not always hint at an engagement that is conducive to learning. Using the data from a robot mediated collaborative learning task in an out-of-lab setting, tested with around 70 children, we show that datadriven clustering approaches, applied on behavioral features including interaction with the activity, speech, emotional and gaze patterns, not only are capable of discriminating between high and low learners, but can do so better than classical approaches that rely on performance alone. First experiments reveal the existence of at least two distinct multimodal behavioral patterns that are indicative of high learning in constructivist, collaborative activities.},
	language = {en},
	urldate = {2023-02-10},
	booktitle = {Companion {Publication} of the 2020 {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {ACM},
	author = {Nasir, Jauwairia and Bruno, Barbara and Dillenbourg, Pierre},
	month = oct,
	year = {2020},
	pages = {388--391},
}

@article{prieto_multimodal_2018,
	title = {Multimodal teaching analytics: {Automated} extraction of orchestration graphs from wearable sensor data},
	volume = {34},
	issn = {02664909},
	shorttitle = {Multimodal teaching analytics},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/jcal.12232},
	doi = {10.1111/jcal.12232},
	abstract = {The pedagogical modelling of everyday classroom practice is an interesting kind of evidence, both for educational research and teachers’ own professional development. This paper explores the usage of wearable sensors and machine learning techniques to automatically extract orchestration graphs (teaching activities and their social plane over time), on a dataset of 12 classroom sessions enacted by two different teachers in different classroom settings. The dataset included mobile eyetracking as well as audiovisual and accelerometry data from sensors worn by the teacher. We evaluated both time-independent and time-aware models, achieving median F1 scores of about 0.7-0.8 on leave-one-session-out k-fold cross-validation. Although these results show the feasibility of this approach, they also highlight the need for larger datasets, recorded in a wider variety of classroom settings, to provide automated tagging of classroom practice that can be used in everyday practice across multiple teachers.},
	language = {en},
	number = {2},
	urldate = {2023-02-10},
	journal = {Journal of Computer Assisted Learning},
	author = {Prieto, L.P. and Sharma, K. and Kidzinski, Ł. and Rodríguez-Triana, M.J. and Dillenbourg, P.},
	month = apr,
	year = {2018},
	pages = {193--203},
}

@article{worsley_multimodal_2018,
	title = {A {Multimodal} {Analysis} of {Making}},
	volume = {28},
	issn = {1560-4292, 1560-4306},
	url = {http://link.springer.com/10.1007/s40593-017-0160-1},
	doi = {10.1007/s40593-017-0160-1},
	abstract = {This paper presents three multimodal learning analytic approaches from a hands-on learning activity. We use video, audio, gesture and bio-physiology data from a two-condition study (N = 20), to identify correlations between the multimodal data, experimental condition, and two learning outcomes: design quality and learning. The three approaches incorporate: 1) human-annotated coding of video data, 2) automated coding of gesture, audio and bio-physiological data and, 3) concatenated humanannotated and automatically annotated data. Within each analysis we employ the same machine learning and sequence mining techniques. Ultimately we find that each approach provides different affordances depending on the similarity metric and the dependent variable. For example, the analysis based on human-annotated data found strong correlations among multimodal behaviors, experimental condition, success and learning, when we relaxed constraints on temporal similarity. The second approach performed well when comparing students’ multimodal behaviors as a time series, but was less effective using the temporally relaxed similarity metric. The take-away is that there are several strategies for doing multimodal learning analytics, and that many of these approaches can provide a meaningful glimpse into a complex data set, glimpses that may be difficult to identify using traditional approaches.},
	language = {en},
	number = {3},
	urldate = {2023-02-10},
	journal = {International Journal of Artificial Intelligence in Education},
	author = {Worsley, Marcelo and Blikstein, Paulo},
	month = sep,
	year = {2018},
	pages = {385--419},
}

@incollection{bittencourt_real-time_2020,
	address = {Cham},
	title = {Real-{Time} {Multimodal} {Feedback} with the {CPR} {Tutor}},
	volume = {12163},
	isbn = {978-3-030-52236-0 978-3-030-52237-7},
	url = {http://link.springer.com/10.1007/978-3-030-52237-7_12},
	abstract = {We developed the CPR Tutor, a real-time multimodal feedback system for cardiopulmonary resuscitation (CPR) training. The CPR Tutor detects mistakes using recurrent neural networks for real-time time-series classiﬁcation. From a multimodal data stream consisting of kinematic and electromyographic data, the CPR Tutor system automatically detects the chest compressions, which are then classiﬁed and assessed according to ﬁve performance indicators. Based on this assessment, the CPR Tutor provides audio feedback to correct the most critical mistakes and improve the CPR performance. To test the validity of the CPR Tutor, we ﬁrst collected the data corpus from 10 experts used for model training. Hence, to test the impact of the feedback functionality, we ran a user study involving 10 participants. The CPR Tutor pushes forward the current state of the art of real-time multimodal tutors by providing: 1) an architecture design, 2) a methodological approach to design multimodal feedback and 3) a ﬁeld study on real-time feedback for CPR training.},
	language = {en},
	urldate = {2023-02-10},
	booktitle = {Artificial {Intelligence} in {Education}},
	publisher = {Springer International Publishing},
	author = {Di Mitri, Daniele and Schneider, Jan and Trebing, Kevin and Sopka, Sasa and Specht, Marcus and Drachsler, Hendrik},
	editor = {Bittencourt, Ig Ibert and Cukurova, Mutlu and Muldner, Kasia and Luckin, Rose and Millán, Eva},
	year = {2020},
	doi = {10.1007/978-3-030-52237-7_12},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {141--152},
}

@article{martinez-maldonado_designing_2022,
	title = {Designing translucent learning analytics with teachers: an elicitation process},
	volume = {30},
	issn = {1049-4820, 1744-5191},
	shorttitle = {Designing translucent learning analytics with teachers},
	url = {https://www.tandfonline.com/doi/full/10.1080/10494820.2019.1710541},
	doi = {10.1080/10494820.2019.1710541},
	language = {en},
	number = {6},
	urldate = {2023-02-10},
	journal = {Interactive Learning Environments},
	author = {Martinez-Maldonado, Roberto and Elliott, Doug and Axisa, Carmen and Power, Tamara and Echeverria, Vanessa and Buckingham Shum, Simon},
	month = jul,
	year = {2022},
	pages = {1077--1091},
}

@article{prieto_smart_nodate,
	title = {Smart {School} {Multimodal} {Dataset} and {Challenges}},
	abstract = {As part of a research project aiming to explore the notion of ‘smart school’ (especially for STEM education) in Estonia, we are developing classrooms and schools that incorporate data gathering not only from digital traces, but also physical ones (through a variety of sensors). This workshop contribution describes brieﬂy the setting and our initial eﬀorts in setting up a classroom that is able to generate such a multimodal dataset. The paper also describes some of the most important challenges that we are facing as we setup the project and attempt to build up such dataset, focusing on the speciﬁcs of doing it in an everyday, authentic school setting. We believe these challenges provide a nice sample of those that the multimodal learning analytics (MMLA) community will have to face as it transitions from an emergent to a mainstream community of research and practice.},
	language = {en},
	author = {Prieto, Luis P and Rodrıguez-Triana, Marıa Jesus and Kusmin, Marge},
}

@article{noel_visualizing_2022,
	title = {Visualizing {Collaboration} in {Teamwork}: {A} {Multimodal} {Learning} {Analytics} {Platform} for {Non}-{Verbal} {Communication}},
	volume = {12},
	issn = {2076-3417},
	shorttitle = {Visualizing {Collaboration} in {Teamwork}},
	url = {https://www.mdpi.com/2076-3417/12/15/7499},
	doi = {10.3390/app12157499},
	abstract = {Developing communication skills in collaborative contexts is of special interest for educational institutions, since these skills are crucial to forming competent professionals for today’s world. New and accessible technologies open a way to analyze collaborative activities in face-to-face and non-face-to-face situations, where collaboration and student attitudes are difﬁcult to measure using traditional methods. In this context, Multimodal Learning Analytics (MMLA) appear as an alternative to complement the evaluation and feedback of core skills. We present a MMLA platform to support collaboration assessment based on the capture and classiﬁcation of non-verbal communication interactions. The developed platform integrates hardware and software, including machine learning techniques, to detect spoken interactions and body postures from video and audio recordings. The captured data is presented in a set of visualizations, designed to help teachers to obtain insights about the collaboration of a team. We performed a case study to explore if the visualizations were useful to represent different behavioral indicators of collaboration in different teamwork situations: a collaborative situation and a competitive situation. We discussed the results of the case study in a focus group with three teachers, to get insights in the usefulness of our proposal. The results show that the measurements and visualizations are helpful to understand differences in collaboration, conﬁrming the feasibility the MMLA approach for assessing and providing collaboration insights based on non-verbal communication.},
	language = {en},
	number = {15},
	urldate = {2023-02-10},
	journal = {Applied Sciences},
	author = {Noël, René and Miranda, Diego and Cechinel, Cristian and Riquelme, Fabián and Primo, Tiago Thompsen and Munoz, Roberto},
	month = jul,
	year = {2022},
	pages = {7499},
}

@inproceedings{ochoa_rap_2018,
	address = {Sydney New South Wales Australia},
	title = {The {RAP} system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors},
	isbn = {978-1-4503-6400-3},
	shorttitle = {The {RAP} system},
	url = {https://dl.acm.org/doi/10.1145/3170358.3170406},
	doi = {10.1145/3170358.3170406},
	abstract = {Developing communication skills in higher education students could be a challenge to professors due to the time needed to provide formative feedback. This work presents RAP, a scalable system to provide automatic feedback to entry-level students to develop basic oral presentation skills. The system improves the state-ofthe-art by analyzing posture, gaze, volume, filled pauses and the slides of the presenters through data captured by very low-cost sensors. The system also provides an off-line feedback report with multimodal recordings of their performance. An initial evaluation of the system indicates that the system’s feedback highly agrees with human feedback and that students considered that feedback useful to develop their oral presentation skills.},
	language = {en},
	urldate = {2023-02-10},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Learning} {Analytics} and {Knowledge}},
	publisher = {ACM},
	author = {Ochoa, Xavier and Domínguez, Federico and Guamán, Bruno and Maya, Ricardo and Falcones, Gabriel and Castells, Jaime},
	month = mar,
	year = {2018},
	pages = {360--364},
}

@misc{alyuz_unobtrusive_2019,
	title = {Unobtrusive and {Multimodal} {Approach} for {Behavioral} {Engagement} {Detection} of {Students}},
	url = {http://arxiv.org/abs/1901.05835},
	abstract = {We propose a multimodal approach for detection of students’ behavioral engagement states (i.e., On-Task vs. Off-Task), based on three unobtrusive modalities: Appearance, Context-Performance, and Mouse. Final behavioral engagement states are achieved by fusing modality-speciﬁc classiﬁers at the decision level. Various experiments were conducted on a student dataset collected in an authentic classroom.},
	language = {en},
	urldate = {2023-02-10},
	publisher = {arXiv},
	author = {Alyuz, Nese and Okur, Eda and Genc, Utku and Aslan, Sinem and Tanriover, Cagri and Esme, Asli Arslan},
	month = jan,
	year = {2019},
	note = {arXiv:1901.05835 [cs, stat]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{rodriguez-triana_teacher_2018,
	address = {Sydney New South Wales Australia},
	title = {The teacher in the loop: customizing multimodal learning analytics for blended learning},
	isbn = {978-1-4503-6400-3},
	shorttitle = {The teacher in the loop},
	url = {https://dl.acm.org/doi/10.1145/3170358.3170364},
	doi = {10.1145/3170358.3170364},
	abstract = {In blended learning scenarios, evidence needs to be gathered from digital and physical spaces to obtain a more complete view of the teaching and learning processes. However, these scenarios are highly heterogeneous, and the varying data sources available in each particular context can condition the accuracy, relevance, interpretability and actionability of the Learning Analytics (LA) solutions, affecting also the user’s sense of agency and trust in such solutions. To aid stakeholders in making use of learning analytics, we propose a process to involve teachers in customizing multimodal LA (MMLA) solutions, adapting them to their particular blended learning situation (e.g., identifying relevant data sources and metrics). Since measuring the added value of adopting an LA solution is not straightforward, we also propose a concrete method for doing so. The results obtained from two case studies in authentic, blended computer-supported collaborative learning settings show an improvement in the sensitivity and F1 scores of the customized MMLA solution. Aside from these quantitative improvements, participant teachers reported both an increment in the effort involved, but also increased relevance, understanding and actionability of the results.},
	language = {en},
	urldate = {2023-02-10},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Learning} {Analytics} and {Knowledge}},
	publisher = {ACM},
	author = {Rodríguez-Triana, María Jesüs and Prieto, Luis P. and Martínez-Monés, Alejandra and Asensio-Pérez, Juan I. and Dimitriadis, Yannis},
	month = mar,
	year = {2018},
	pages = {417--426},
}

@article{nasir_many_2021,
	title = {Many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities},
	volume = {16},
	issn = {1556-1607, 1556-1615},
	url = {https://link.springer.com/10.1007/s11412-021-09358-2},
	doi = {10.1007/s11412-021-09358-2},
	abstract = {Understanding the way learners engage with learning technologies, and its relation with their learning, is crucial for motivating design of effective learning interventions. Assessing the learners’ state of engagement, however, is non-trivial. Research suggests that performance is not always a good indicator of learning, especially with open-ended constructivist activities. In this paper, we describe a combined multi-modal learning analytics and interaction analysis method that uses video, audio and log data to identify multi-modal collaborative learning behavioral profiles of 32 dyads as they work on an open-ended task around interactive tabletops with a robot mediator. These profiles, which we name Expressive Explorers, Calm Tinkerers, and Silent Wanderers, confirm previous collaborative learning findings. In particular, the amount of speech interaction and the overlap of speech between a pair of learners are behavior patterns that strongly distinguish between learning and non-learning pairs. Delving deeper, findings suggest that overlapping speech between learners can indicate engagement that is conducive to learning. When we more broadly consider learner affect and actions during the task, we are better able to characterize the range of behavioral profiles exhibited among those who learn. Specifically, we discover two behavioral dimensions along which those who learn vary, namely, problem solving strategy (actions) and emotional expressivity (affect). This finding suggests a relation between problem solving strategy and emotional behavior; one strategy leads to more frustration compared to another. These findings have implications for the design of real-time learning interventions that support productive collaborative learning in open-ended tasks.},
	language = {en},
	number = {4},
	urldate = {2023-02-10},
	journal = {International Journal of Computer-Supported Collaborative Learning},
	author = {Nasir, Jauwairia and Kothiyal, Aditi and Bruno, Barbara and Dillenbourg, Pierre},
	month = dec,
	year = {2021},
	pages = {485--523},
}

@inproceedings{birt_piloting_2019,
	address = {Kyoto, Japan},
	title = {Piloting {Multimodal} {Learning} {Analytics} using {Mobile} {Mixed} {Reality} in {Health} {Education}},
	isbn = {978-1-72810-300-6},
	url = {https://ieeexplore.ieee.org/document/8882435/},
	doi = {10.1109/SeGAH.2019.8882435},
	abstract = {Mobile mixed reality has been shown to increase higher achievement and lower cognitive load within spatial disciplines. However, traditional methods of assessment restrict examiners ability to holistically assess spatial understanding. Multimodal learning analytics seeks to investigate how combinations of data types such as spatial data and traditional assessment can be combined to better understand both the learner and learning environment. This paper explores the pedagogical possibilities of a smartphone enabled mixed reality multimodal learning analytics case study for health education, focused on learning the anatomy of the heart. The context for this study is the first loop of a design based research study exploring the acquisition and retention of knowledge by piloting the proposed system with practicing health experts. Outcomes from the pilot study showed engagement and enthusiasm of the method among the experts, but also demonstrated problems to overcome in the pedagogical method before deployment with learners.},
	language = {en},
	urldate = {2023-02-10},
	booktitle = {2019 {IEEE} 7th {International} {Conference} on {Serious} {Games} and {Applications} for {Health} ({SeGAH})},
	publisher = {IEEE},
	author = {Birt, James and Clare, Darryl and Cowling, Michael},
	month = aug,
	year = {2019},
	pages = {1--6},
}

@inproceedings{vrzakova_focused_2020,
	address = {Frankfurt Germany},
	title = {Focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving},
	isbn = {978-1-4503-7712-6},
	shorttitle = {Focused or stuck together},
	url = {https://dl.acm.org/doi/10.1145/3375462.3375467},
	doi = {10.1145/3375462.3375467},
	abstract = {Collaborative problem solving (CPS) in virtual environments is an increasingly important context of 21st century learning. However, our understanding of this complex and dynamic phenomenon is still limited. Here, we examine unimodal primitives (activity on the screen, speech, and body movements), and their multimodal combinations during remote CPS. We analyze two datasets where 116 triads collaboratively engaged in a challenging visual programming task using video conferencing software. We investigate how UI-interactions, behavioral primitives, and multimodal patterns were associated with teams’ subjective and objective performance outcomes. We found that idling with limited speech (i.e., silence or backchannel feedback only) and without movement was negatively correlated with task performance and with participants’ subjective perceptions of the collaboration. However, being silent and focused during solution execution was positively correlated with task performance. Results illustrate that in some cases, multimodal patterns improved the predictions and improved explanatory power over the unimodal primitives. We discuss how the findings can inform the design of real-time interventions for remote CPS.},
	language = {en},
	urldate = {2023-02-10},
	booktitle = {Proceedings of the {Tenth} {International} {Conference} on {Learning} {Analytics} \& {Knowledge}},
	publisher = {ACM},
	author = {Vrzakova, Hana and Amon, Mary Jean and Stewart, Angela and Duran, Nicholas D. and D'Mello, Sidney K.},
	month = mar,
	year = {2020},
	pages = {295--304},
}

@article{di_mitri_keep_2022,
	title = {Keep {Me} in the {Loop}: {Real}-{Time} {Feedback} with {Multimodal} {Data}},
	volume = {32},
	issn = {1560-4292, 1560-4306},
	shorttitle = {Keep {Me} in the {Loop}},
	url = {https://link.springer.com/10.1007/s40593-021-00281-z},
	doi = {10.1007/s40593-021-00281-z},
	abstract = {This paper describes the CPR Tutor, a real-time multimodal feedback system for cardiopulmonary resuscitation (CPR) training. The CPR Tutor detects training mistakes using recurrent neural networks. The CPR Tutor automatically recognises and assesses the quality of the chest compressions according to five CPR performance indicators. It detects training mistakes in real-time by analysing a multimodal data stream consisting of kinematic and electromyographic data. Based on this assessment, the CPR Tutor provides audio feedback to correct the most critical mistakes and improve the CPR performance. The mistake detection models of the CPR Tutor were trained using a dataset from 10 experts. Hence, we tested the validity of the CPR Tutor and the impact of its feedback functionality in a user study involving additional 10 participants. The CPR Tutor pushes forward the current state of the art of real-time multimodal tutors by providing: (1) an architecture design, (2) a methodological approach for delivering real-time feedback using multimodal data and (3) a field study on real-time feedback for CPR training. This paper details the results of a field study by quantitatively measuring the impact of the CPR Tutor feedback on the performance indicators and qualitatively analysing the participants’ questionnaire answers.},
	language = {en},
	number = {4},
	urldate = {2023-02-10},
	journal = {International Journal of Artificial Intelligence in Education},
	author = {Di Mitri, Daniele and Schneider, Jan and Drachsler, Hendrik},
	month = dec,
	year = {2022},
	pages = {1093--1118},
}

@incollection{giannakos_intermodality_2022,
	address = {Cham},
	title = {Intermodality in {Multimodal} {Learning} {Analytics} for {Cognitive} {Theory} {Development}: {A} {Case} from {Embodied} {Design} for {Mathematics} {Learning}},
	isbn = {978-3-031-08075-3 978-3-031-08076-0},
	shorttitle = {Intermodality in {Multimodal} {Learning} {Analytics} for {Cognitive} {Theory} {Development}},
	url = {https://link.springer.com/10.1007/978-3-031-08076-0_6},
	abstract = {Multimodal Learning Analytics (MMLA) grant us insight into learners’ physiological, cognitive, and behavioral activity as it unfolds. In this chapter, we query the relations among modalities, intermodality, in the context of a designbased research program studying the relations between learning to move in new ways and learning to think in new ways. In the ﬁrst part, we reﬂect on how different methods have afforded purchase on the investigation, development, and elaboration of theoretical claims about the multimodal enactment of cognitive events, culminating in the use of Recurrence Quantiﬁcation Analysis (RQA) to quantify the microgenesis of stable new patterns in hand movement and gaze. In the second part, we analyze an RQA case study spanning across hand and gaze modalities to examine the emergence of intermodal coordination at a critical moment in the mathematical task. We conclude with implications and open questions around intermodality in embodied learning.},
	language = {en},
	urldate = {2023-02-10},
	booktitle = {The {Multimodal} {Learning} {Analytics} {Handbook}},
	publisher = {Springer International Publishing},
	author = {Tancredi, Sofia and Abdu, Rotem and Balasubramaniam, Ramesh and Abrahamson, Dor},
	editor = {Giannakos, Michail and Spikol, Daniel and Di Mitri, Daniele and Sharma, Kshitij and Ochoa, Xavier and Hammad, Rawad},
	year = {2022},
	doi = {10.1007/978-3-031-08076-0_6},
	pages = {133--158},
}

@article{azevedo_lessons_2022,
	title = {Lessons {Learned} and {Future} {Directions} of {MetaTutor}: {Leveraging} {Multichannel} {Data} to {Scaffold} {Self}-{Regulated} {Learning} {With} an {Intelligent} {Tutoring} {System}},
	volume = {13},
	issn = {1664-1078},
	shorttitle = {Lessons {Learned} and {Future} {Directions} of {MetaTutor}},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2022.813632/full},
	doi = {10.3389/fpsyg.2022.813632},
	abstract = {Self-regulated learning (SRL) is critical for learning across tasks, domains, and contexts. Despite its importance, research shows that not all learners are equally skilled at accurately and dynamically monitoring and regulating their self-regulatory processes. Therefore, learning technologies, such as intelligent tutoring systems (ITSs), have been designed to measure and foster SRL. This paper presents an overview of over 10 years of research on SRL with MetaTutor, a hypermedia-based ITS designed to scaffold college students’ SRL while they learn about the human circulatory system. MetaTutor’s architecture and instructional features are designed based on models of SRL, empirical evidence on human and computerized tutoring principles of multimedia learning, Artificial Intelligence (AI) in educational systems for metacognition and SRL, and research on SRL from our team and that of other researchers. We present MetaTutor followed by a synthesis of key research findings on the effectiveness of various versions of the system (e.g., adaptive scaffolding vs. no scaffolding of self-regulatory behavior) on learning outcomes. First, we focus on findings from self-reports, learning outcomes, and multimodal data (e.g., log files, eye tracking, facial expressions of emotion, screen recordings) and their contributions to our understanding of SRL with an ITS. Second, we elaborate on the role of embedded pedagogical agents (PAs) as external regulators designed to scaffold learners’ cognitive and metacognitive SRL strategy use. Third, we highlight and elaborate on the contributions of multimodal data in measuring and understanding the role of cognitive, affective, metacognitive, and motivational (CAMM) processes. Additionally, we unpack some of the challenges these data pose for designing real-time instructional interventions that scaffold SRL. Fourth, we present existing theoretical, methodological, and analytical challenges and briefly discuss lessons learned and open challenges.},
	language = {en},
	urldate = {2023-02-10},
	journal = {Frontiers in Psychology},
	author = {Azevedo, Roger and Bouchet, François and Duffy, Melissa and Harley, Jason and Taub, Michelle and Trevors, Gregory and Cloude, Elizabeth and Dever, Daryn and Wiedbusch, Megan and Wortha, Franz and Cerezo, Rebeca},
	month = jun,
	year = {2022},
	pages = {813632},
}

@article{munoz_development_nodate,
	title = {Development of a {Software} that {Supports} {Multimodal} {Learning} {Analytics}: {A} {Case} {Study} on {Oral} {Presentations}},
	abstract = {Learning Analytics is the intelligent use of data generated from students with the objective of understanding and improving the teaching and learning process. Currently, there is a lack of tools to measure the development of complex skills in real classroom environments that are flexible enough to add and process data from different sensors and oriented towards a massive public. Based on this finding, we developed a free software system that permits to capture and to visualize a set of 10 body postures using the Microsoft Kinect sensor, along with the ability to track custom body postures and data from other sensors. The developed tool was validated by means of precision and usability tests. Furthermore, with the goal of demonstrating the potential of incorporating this type of software into the classroom, the software was used as a tool to give feedback to the teacher and to the students at the moment of giving and evaluating oral presentations. Also, a clustering analysis of data gathered from 45 student presentations indicate that presentations on similar topics with also similar complexity levels can be successfully discriminated.},
	language = {en},
	author = {Munoz, Roberto and Villarroel, Rodolfo and Barcelos, Thiago S and Souza, Alexandra and Merino, Erick and Guiñez, Rodolfo and Silva, Leandro A},
}

@inproceedings{petukhova_virtual_2017,
	address = {Glasgow UK},
	title = {Virtual debate coach design: assessing multimodal argumentation performance},
	isbn = {978-1-4503-5543-8},
	shorttitle = {Virtual debate coach design},
	url = {https://dl.acm.org/doi/10.1145/3136755.3136775},
	doi = {10.1145/3136755.3136775},
	abstract = {This paper discusses the design and evaluation of a coaching system used to train young politicians to apply appropriate multimodal rhetoric devices to improve their debate skills. The presented study is carried out to develop debate performance assessment methods and interaction models underlying a Virtual Debate Coach (VDC) application. We identify a number of criteria associated with three questions: (1) how convincing is a debater’s argumentation; (2) how well are debate arguments structured; and (3) how well is an argument delivered. We collected and analysed multimodal data of trainees’ debate behaviour, and contrasted it with that of skilled professional debaters. Observational, correlation and machine learning experiments were performed to identify multimodal correlates of convincing debate performance and link them to experts’ assessments. A rich set of prosodic, motion, linguistic and structural features was considered for the system to operate on. The VDC system was positively evaluated in a trainee-based setting.},
	language = {en},
	urldate = {2023-02-10},
	booktitle = {Proceedings of the 19th {ACM} {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {ACM},
	author = {Petukhova, Volha and Mayer, Tobias and Malchanau, Andrei and Bunt, Harry},
	month = nov,
	year = {2017},
	pages = {41--50},
}

@incollection{fang_multicraft_2021,
	address = {Cham},
	title = {Multicraft: {A} {Multimodal} {Interface} for {Supporting} and {Studying} {Learning} in {Minecraft}},
	volume = {12790},
	isbn = {978-3-030-77413-4 978-3-030-77414-1},
	shorttitle = {Multicraft},
	url = {https://link.springer.com/10.1007/978-3-030-77414-1_10},
	abstract = {In this paper, we present work on bringing multimodal interaction to Minecraft. The platform, Multicraft, incorporates speech-based input, eye tracking, and natural language understanding to facilitate more equitable gameplay in Minecraft. We tested the platform with elementary, middle school students and college students through a collection of studies. Students found each of the provided modalities to be a compelling way to play Minecraft. Additionally, we discuss the ways that these different types of multimodal data can be used to identify the meaningful spatial reasoning practices that students demonstrate while playing Minecraft. Collectively, this paper emphasizes the opportunity to bridge a multimodal interface with a means for collecting rich data that can better support diverse learners in non-traditional learning environments.},
	language = {en},
	urldate = {2023-02-10},
	booktitle = {{HCI} in {Games}: {Serious} and {Immersive} {Games}},
	publisher = {Springer International Publishing},
	author = {Worsley, Marcelo and Mendoza Tudares, Kevin and Mwiti, Timothy and Zhen, Mitchell and Jiang, Marc},
	editor = {Fang, Xiaowen},
	year = {2021},
	doi = {10.1007/978-3-030-77414-1_10},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {113--131},
}

@article{lopez_using_nodate,
	title = {Using {Multimodal} {Learning} {Analytics} to {Explore} {Collaboration} in a {Sustainability} {Co}-located {Tabletop} {Game}},
	abstract = {Serious Games (SGs) are particularly suitable to foster collaboration in complex domains that challenge formal education approaches. However, their effectiveness depends on their features as much as on the ability to assess their impacts on players, and analysing collaboration in games remains by and large an open problem. Research has traditionally used rich unimodal data to examine collaboration processes in games (e.g., video content analysis of verbal exchanges). Despite providing relevant semantic information, this can make data coding and analysis difficult and time-consuming. Furthermore, unimodal approaches can only partially capture complex processes defined by multiple interacting variables, such as collaboration. Recent research highlighted the potentialities offered by multimodal learning analytics (MMLA) to address these issues. MMLA integrates multiple types of data captured both in and out of the game system through different modalities to analyse complex processes. Although it has been highlighted as particularly suitable to investigate collaboration, research on MMLA in SGs is still scarce. This work contributes to the state-of-the-art by leveraging MMLA to explore collaboration indicators in a multiplayer, co-located SG for education in sustainable development. Our results corroborate the MMLA effectiveness in analysing complex collaborative dynamics, and identify key multimodal analytics useful to investigate collaboration in SGs.},
	language = {en},
	author = {López, María Ximena and Strada, Francesco and Bottino, Andrea and Fabricatore, Carlo},
}

@article{eradze_how_nodate,
	title = {How to {Aggregate} {Lesson} {Observation} {Data} into {Learn}- ing {Analytics} {Dataset}?},
	abstract = {The technological environment that supports the learning process tends to be the main data source for Learning Analytics. However, this trend leaves out those parts of the learning process that are not computer-mediated. To overcome this problem, involving additional data gathering techniques such as ambient sensors, audio and video recordings, or even observations could enrich datasets. This paper focuses on how the data extracted from the observations can be integrated with data coming from activity tracking, resulting in a multimodal dataset. The paper identifies the need for theoretical and pedagogical semantics in multimodal learning analytics, and examines the xAPI potential for the multimodal data gathering and aggregation. Finally, we propose an approach for pedagogy-driven observational data identification. As a proof of concept, we have applied the approach in two research works where observations had been used to enrich or triangulate the results obtained for traditional data sources. Through these examples, we illustrate some of the challenges that multimodal dataset may present when including observational data.},
	language = {en},
	author = {Eradze, Maka and Rodríguez-Triana, María Jesús and Laanpere, Mart},
}

@incollection{pammer-schindler_multimodal_2018,
	address = {Cham},
	title = {Multimodal {Learning} {Hub}: {A} {Tool} for {Capturing} {Customizable} {Multimodal} {Learning} {Experiences}},
	volume = {11082},
	isbn = {978-3-319-98571-8 978-3-319-98572-5},
	shorttitle = {Multimodal {Learning} {Hub}},
	url = {http://link.springer.com/10.1007/978-3-319-98572-5_4},
	abstract = {Studies in Learning Analytics provide concrete examples of how the analysis of direct interactions with learning management systems can be used to optimize and understand the learning process. Learning, however, does not necessarily only occur when the learner is directly interacting with such systems. With the use of sensors, it is possible to collect data from learners and their environment ubiquitously, therefore expanding the use cases of Learning Analytics. For this reason, we developed the Multimodal Learning Hub (MLH), a system designed to enhance learning in ubiquitous learning scenarios, by collecting and integrating multimodal data from customizable conﬁgurations of ubiquitous data providers. In this paper, we describe the MLH and report on the results of tests where we explored its reliability to integrate multimodal data.},
	language = {en},
	urldate = {2023-02-10},
	booktitle = {Lifelong {Technology}-{Enhanced} {Learning}},
	publisher = {Springer International Publishing},
	author = {Schneider, Jan and Di Mitri, Daniele and Limbu, Bibeg and Drachsler, Hendrik},
	editor = {Pammer-Schindler, Viktoria and Pérez-Sanagustín, Mar and Drachsler, Hendrik and Elferink, Raymond and Scheffel, Maren},
	year = {2018},
	doi = {10.1007/978-3-319-98572-5_4},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {45--58},
}

@article{chango_improving_2021,
	title = {Improving prediction of students’ performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources},
	volume = {33},
	issn = {1042-1726, 1867-1233},
	url = {https://link.springer.com/10.1007/s12528-021-09298-8},
	doi = {10.1007/s12528-021-09298-8},
	abstract = {The aim of this study was to predict university students’ learning performance using different sources of performance and multimodal data from an Intelligent Tutoring System. We collected and preprocessed data from 40 students from different multimodal sources: learning strategies from system logs, emotions from videos of facial expressions, allocation and fixations of attention from eye tracking, and performance on posttests of domain knowledge. Our objective was to test whether the prediction could be improved by using attribute selection and classification ensembles. We carried out three experiments by applying six classification algorithms to numerical and discretized preprocessed multimodal data. The results show that the best predictions were produced using ensembles and selecting the best attributes approach with numerical data.},
	language = {en},
	number = {3},
	urldate = {2023-02-08},
	journal = {Journal of Computing in Higher Education},
	author = {Chango, Wilson and Cerezo, Rebeca and Sanchez-Santillan, Miguel and Azevedo, Roger and Romero, Cristóbal},
	month = dec,
	year = {2021},
	pages = {614--634},
}

@article{caspari-sadeghi_applying_2022,
	title = {Applying {Learning} {Analytics} in {Online} {Environments}: {Measuring} {Learners}’ {Engagement} {Unobtrusively}},
	volume = {7},
	issn = {2504-284X},
	shorttitle = {Applying {Learning} {Analytics} in {Online} {Environments}},
	url = {https://www.frontiersin.org/articles/10.3389/feduc.2022.840947/full},
	doi = {10.3389/feduc.2022.840947},
	abstract = {Prior to the emergence of Big Data and technologies such as Learning Analytics (LA), classroom research focused mainly on measuring learning outcomes of a small sample through tests. Research on online environments shows that learners’ engagement is a critical precondition for successful learning and lack of engagement is associated with failure and dropout. LA helps instructors to track, measure and visualize students’ online behavior and use such digital traces to improve instruction and provide individualized support, i.e., feedback. This paper examines 1) metrics or indicators of learners’ engagement as extracted and displayed by LA, 2) their relationship with academic achievement and performance, and 3) some freely available LA tools for instructors and their usability. The paper concludes with making recommendations for practice and further research by considering challenges associated with using LA in classrooms.},
	language = {en},
	urldate = {2023-02-08},
	journal = {Frontiers in Education},
	author = {Caspari-Sadeghi, Sima},
	month = jan,
	year = {2022},
	pages = {840947},
}

@article{ochoa_chapter_nodate-1,
	title = {Chapter 6: {Multimodal} {Learning} {Analytics} - {Rationale}, {Process}, {Examples}, and {Direction}},
	abstract = {This chapter is an introduction to the use of multiple modalities of learning trace data to better understand and feedback learning processes that occur both in digital and face-to-face contexts. First, it will explain the rationale behind the emergence of this type of study, followed by a brief explanation of what Multimodal Learning Analytics (MmLA) is based on current conceptual understandings and current state-of-the-art implementations. The majority of this chapter is dedicated to describing the general process of MmLA from the mapping of learning constructs to low-level multimodal learning traces to the reciprocal implementation of multimedia recording, multimodal feature extraction, analysis, and fusion to detect behavioral markers and estimate the studied constructs. This process is illustrated by the detailed dissection of a real-world example. This chapter concludes with a discussion of the current challenges facing the field and the directions in which the field is moving to address them.},
	language = {en},
	author = {Ochoa, Xavier},
}

@article{cornide-reyes_introducing_2019,
	title = {Introducing {Low}-{Cost} {Sensors} into the {Classroom} {Settings}: {Improving} the {Assessment} in {Agile} {Practices} with {Multimodal} {Learning} {Analytics}},
	volume = {19},
	issn = {1424-8220},
	shorttitle = {Introducing {Low}-{Cost} {Sensors} into the {Classroom} {Settings}},
	url = {https://www.mdpi.com/1424-8220/19/15/3291},
	doi = {10.3390/s19153291},
	abstract = {Currently, the improvement of core skills appears as one of the most signiﬁcant educational challenges of this century. However, assessing the development of such skills is still a challenge in real classroom environments. In this context, Multimodal Learning Analysis techniques appear as an attractive alternative to complement the development and evaluation of core skills. This article presents an exploratory study that analyzes the collaboration and communication of students in a Software Engineering course, who perform a learning activity simulating Scrum with Lego R bricks. Data from the Scrum process was captured, and multidirectional microphones were used in the retrospective ceremonies. Social network analysis techniques were applied, and a correlational analysis was carried out with all the registered information. The results obtained allowed the detection of important relationships and characteristics of the collaborative and Non-Collaborative groups, with productivity, effort, and predominant personality styles in the groups. From all the above, we can conclude that the Multimodal Learning Analysis techniques offer considerable feasibilities to support the process of skills development in students.},
	language = {en},
	number = {15},
	urldate = {2023-02-08},
	journal = {Sensors},
	author = {Cornide-Reyes, Hector and Noël, René and Riquelme, Fabián and Gajardo, Matías and Cechinel, Cristian and Mac Lean, Roberto and Becerra, Carlos and Villarroel, Rodolfo and Munoz, Roberto},
	month = jul,
	year = {2019},
	pages = {3291},
}

@article{closser_blending_2022,
	title = {Blending learning analytics and embodied design to model students’ comprehension of measurement using their actions, speech, and gestures},
	volume = {32},
	issn = {22128689},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2212868921000866},
	doi = {10.1016/j.ijcci.2021.100391},
	abstract = {Although interdisciplinary collaborations are becoming increasingly common, researchers typically use data analysis methods specific to their field in order to uncover how students learn. We present affordances of integrating theories of embodied cognition and design with machine-learning methods to study student learning in mathematics and inform the design of embodied learning activities. By increasing such collaborative research efforts, learning scientists can incorporate regularization in computational models and ultimately draw reliable conclusions to further inform theory and practice through the design of technology-augmented learning activities. To illustrate this point, we explored students’ conceptual understanding of measurement since limited research has identified measurement estimation strategies that should be emphasized in classroom instruction. By uniquely applying machine-learning methods to a small, multimodal dataset from a study on student behavior in mathematics, we identified behavioral profiles, patterns in speech, and specific actions and gestures that are predictive of performance. These findings may be used to inform the design of embodied learning activities for measurement. We discuss the contribution of these findings to the field of embodied design, and the affordances and challenges of conducting collaborative research in the learning sciences.},
	language = {en},
	urldate = {2023-02-08},
	journal = {International Journal of Child-Computer Interaction},
	author = {Closser, Avery H. and Erickson, John A. and Smith, Hannah and Varatharaj, Ashvini and Botelho, Anthony F.},
	month = jun,
	year = {2022},
	pages = {100391},
}

@inproceedings{kasepalu_overcoming_2020,
	address = {Tartu, Estonia},
	title = {Overcoming the {Difficulties} for {Teachers} in {Collaborative} {Learning} {Using} {Multimodal} {Learning} {Analytics}},
	isbn = {978-1-72816-090-0},
	url = {https://ieeexplore.ieee.org/document/9156022/},
	doi = {10.1109/ICALT49669.2020.00124},
	abstract = {During collaborative learning (CL) teachers have little to no information about what is happening in the groups, however, they are expected to plan, monitor, support, consolidate and reflect upon student interactions. How is a teacher able to provide support the students when she/he is not fully aware of the situation and progress of the students? Multimodal learning analytics (MMLA) could offer teachers valuable insights into the CL process, however, not many MMLA outputs are used in real practice today, and designing such MMLA dashboards remains a challenge. The aim of the present design-science research is to find out if multimodal learning analytics is able to help teachers help support students in CL. After having gone through two cycles of design based research, it is apparent that the involved teachers are interested in getting an overview of the activities within the groups and help with assessment. The involved teachers were ready to use collaboration analytics if it can assure a certain level of accuracy, more research is needed on how to use MMLA to support teachers in CL assessment.},
	language = {en},
	urldate = {2023-02-08},
	booktitle = {2020 {IEEE} 20th {International} {Conference} on {Advanced} {Learning} {Technologies} ({ICALT})},
	publisher = {IEEE},
	author = {Kasepalu, Reet},
	month = jul,
	year = {2020},
	pages = {393--395},
}

@incollection{russo_multimodal_2021,
	address = {Cham},
	title = {Multimodal {Affective} {Pedagogical} {Agents} for {Different} {Types} of {Learners}},
	volume = {1322},
	isbn = {978-3-030-68016-9 978-3-030-68017-6},
	url = {https://link.springer.com/10.1007/978-3-030-68017-6_33},
	abstract = {The paper reports progress on an NSF-funded project whose goal is to research and develop multimodal affective animated pedagogical agents (APA) for different types of learners. Although the preponderance of research on APA tends to focus on the cognitive aspects of online learning, this project explores the less-studied role of affective features. More speciﬁcally, the objectives of the work are to: (1) research and develop novel algorithms for emotion recognition and for life-like emotion representation in embodied agents, which will be integrated in a new system for creating APA to be embedded in digital lessons; and (2) develop an empirically grounded research base that will guide the design of affective APA that are effective for different types of learners. This involves conducting a series of experiments to determine the effects of the agent’s emotional style and emotional intelligence on a diverse population of students. The paper outlines the work conducted so far, e.g., development of a new system (and underlying algorithms) for producing affective APA. It also reports the ﬁndings from two preliminary studies.},
	language = {en},
	urldate = {2023-02-08},
	booktitle = {Intelligent {Human} {Systems} {Integration} 2021},
	publisher = {Springer International Publishing},
	author = {Adamo, Nicoletta and Benes, Bedrich and Mayer, Richard E. and Lei, Xingyu and Wang, Zhiquan and Meyer, Zachary and Lawson, Alyssa},
	editor = {Russo, Dario and Ahram, Tareq and Karwowski, Waldemar and Di Bucchianico, Giuseppe and Taiar, Redha},
	year = {2021},
	doi = {10.1007/978-3-030-68017-6_33},
	note = {Series Title: Advances in Intelligent Systems and Computing},
	pages = {218--224},
}

@article{psaltis_multimodal_2018,
	title = {Multimodal {Student} {Engagement} {Recognition} in {Prosocial} {Games}},
	volume = {10},
	issn = {2475-1502, 2475-1510},
	url = {https://ieeexplore.ieee.org/document/8015151/},
	doi = {10.1109/TCIAIG.2017.2743341},
	abstract = {In this paper we address the problem of recognizing student engagement in prosocial games by exploiting engagement cues from different input modalities. Since engagement is a multifaceted phenomenon with different dimensions, i.e., behavioral, cognitive and affective, we propose the modeling of student engagement using real-time data from both the students and the game. More specifically, we apply body motion and facial expression analysis to identify the affective state of students, while we extract features related to their cognitive and behavioral engagement based on the analysis of their interaction with the game. For the automatic recognition of engagement, we adopt a machine learning approach based on artificial neural networks, while for the annotation of the engagement data, we introduce a novel approach based on the use of games with different degrees of challenge in conjunction with a retrospective self-reporting method. To evaluate the proposed methodology, we conducted real-life experiments in four classes, in three primary schools, with 72 students and 144 gameplay recordings in total. Experimental results show the great potential of the proposed methodology, which improves the classification accuracy of the three distinct dimensions with a detection rate of 85\%. A detailed analysis of the role of each component of the Game Engagement Questionnaire (GEQ), i.e., immersion, presence, flow and absorption, in the classification process is also presented in this paper.},
	language = {en},
	number = {3},
	urldate = {2023-02-08},
	journal = {IEEE Transactions on Games},
	author = {Psaltis, Athanasios and Apostolakis, Konstantinos C. and Dimitropoulos, Kosmas and Daras, Petros},
	month = sep,
	year = {2018},
	pages = {292--303},
}

@inproceedings{spikol_estimation_2017,
	address = {Timisoara, Romania},
	title = {Estimation of {Success} in {Collaborative} {Learning} {Based} on {Multimodal} {Learning} {Analytics} {Features}},
	isbn = {978-1-5386-3870-5},
	url = {http://ieeexplore.ieee.org/document/8001779/},
	doi = {10.1109/ICALT.2017.122},
	abstract = {Multimodal learning analytics offer researchers new tools and techniques to capture different types of data from complex learning activities in dynamic learning environments. This paper investigates high-ﬁdelity synchronised multimodal recordings of small groups of learners interacting from diverse sensors that include computer vision, user generated content, and data from the learning objects (like physical computing components or laboratory equipment). We processed and extracted different aspects of the students’ interactions to answer the following question: which features of student group work are good predictors of team success in open-ended tasks with physical computing? The answer to the question provides ways to automatically identify the students at risk of not having success during the learning activities and provides means for different types of interventions to support and scaffold the student learning.},
	language = {en},
	urldate = {2023-02-08},
	booktitle = {2017 {IEEE} 17th {International} {Conference} on {Advanced} {Learning} {Technologies} ({ICALT})},
	publisher = {IEEE},
	author = {Spikol, Daniel and Ruffaldi, Emanuele and Landolfi, Lorenzo and Cukurova, Mutlu},
	month = jul,
	year = {2017},
	pages = {269--273},
}

@incollection{pammer-schindler_multimodal_2018-1,
	address = {Cham},
	title = {Multimodal {Analytics} for {Real}-{Time} {Feedback} in {Co}-located {Collaboration}},
	volume = {11082},
	isbn = {978-3-319-98571-8 978-3-319-98572-5},
	url = {http://link.springer.com/10.1007/978-3-319-98572-5_15},
	language = {en},
	urldate = {2023-02-08},
	booktitle = {Lifelong {Technology}-{Enhanced} {Learning}},
	publisher = {Springer International Publishing},
	author = {Praharaj, Sambit and Scheffel, Maren and Drachsler, Hendrik and Specht, Marcus},
	editor = {Pammer-Schindler, Viktoria and Pérez-Sanagustín, Mar and Drachsler, Hendrik and Elferink, Raymond and Scheffel, Maren},
	year = {2018},
	doi = {10.1007/978-3-319-98572-5_15},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {187--201},
}

@article{olsen_temporal_2020,
	title = {Temporal analysis of multimodal data to predict collaborative learning outcomes},
	volume = {51},
	issn = {0007-1013, 1467-8535},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/bjet.12982},
	doi = {10.1111/bjet.12982},
	abstract = {The analysis of multiple data streams is a long-standing practice within educational research. Both multimodal data analysis and temporal analysis have been applied successfully, but in the area of collaborative learning, very few studies have investigated specific advantages of multiple modalities versus a single modality, especially combined with temporal analysis. In this paper, we investigate how both the use of multimodal data and moving from averages and counts to temporal aspects in a collaborative setting provides a better prediction of learning gains. To address these questions, we analyze multimodal data collected from 25 9–11-year-old dyads using a fractions intelligent tutoring system. Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality. Our work contributes to the understanding of how analyzing multimodal data in temporal manner provides additional information around the collaborative learning process.},
	language = {en},
	number = {5},
	urldate = {2023-02-08},
	journal = {British Journal of Educational Technology},
	author = {Olsen, Jennifer K. and Sharma, Kshitij and Rummel, Nikol and Aleven, Vincent},
	month = sep,
	year = {2020},
	pages = {1527--1547},
}

@article{noauthor_educational_nodate-1,
	title = {Educational {Technology} \& {Society}},
	abstract = {This study integrated the multimodal framework of learning analytics (IMFLA) with the concept mapping (Cmap) approach to improve students’ vocabulary and reading abilities. A total of 70 participants were divided into 2 classes, Class 1 (experimental) and Class 2 (control), for a 1-year period. Vocabulary and reading tests were implemented 3 times. Repeated measures were conducted to test the effect of the program. The results indicated that Time had a significant effect on enhancing both outcome measures (p {\textless} .01 for vocabulary; p {\textless} .001 for reading). Moreover, significant interaction effects between Time and the program on both vocabulary (p {\textless} .05) and reading (p {\textless} .001) further suggest that a longer period of time spent on the program would result in a significant effect on both the vocabulary and reading outcome measures. That is, a significant interaction effect occurred between IMFLA and the time factor. Such interaction effects resulted in better vocabulary and reading abilities when students of Class 1 spent more time on the IMFLA procedure. The multimodal and learning analyses of students’ weekly logs confirmed this improvement. We therefore suggest that instructors use digitalized wordlists and Cmaps in language instruction to enhance students’ vocabulary and reading abilities.},
	language = {en},
}

@article{ochoa_controlled_2020,
	title = {Controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting},
	volume = {51},
	issn = {0007-1013, 1467-8535},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/bjet.12987},
	doi = {10.1111/bjet.12987},
	abstract = {Developing oral presentation skills requires both practice and expert feedback. Several systems have been developed during the last 20 years to provide ample practice opportunities and automated feedback for novice presenters. However, a comprehensive literature review discovered that none of those systems have been adequately evaluated in real learning settings. This work is the first randomised controlled evaluation of the impact that one of these systems has in developing oral presentation skills during a real semester-long learning activity with 180 students. The main findings are that (1) the development of different dimensions of the oral presentations are not affected equally by the automated feedback and (2) there is a small but statistically significant effect of the use of the tool when a subsequent presentation is evaluated by a human expert.},
	language = {en},
	number = {5},
	urldate = {2023-02-08},
	journal = {British Journal of Educational Technology},
	author = {Ochoa, Xavier and Dominguez, Federico},
	month = sep,
	year = {2020},
	pages = {1615--1630},
}

@article{blikstein_multimodal_nodate,
	title = {{MULTIMODAL} {LEARNING} {ANALYTICS} {AND} {ASSESSMENT} {OF} {OPEN}- {ENDED} {ARTIFACTS}},
	language = {en},
	author = {Blikstein, Paulo and Worsley, Marcelo},
}

@article{mitri_detecting_nodate,
	title = {Detecting {Medical} {Simulation} {Errors} with {Machine} learning and {Multimodal} {Data}},
	abstract = {In this doctoral consortium paper, we introduce the CPR Tutor, an intelligent tutoring system for cardiopulmonary resuscitation (CPR) training based on the analysis of multimodal data. Using a multisensor setup, the CPR Tutor tracks the CPR execution of the trainee and generates automatic adaptive feedback to improve the trainee’s performance. This research work is part of a PhD project entitled “Multimodal Tutor: adaptive feedback from multimodal experience capturing”, a project which investigates how to use multimodal and multi-sensor data to generate personalised feedback for training psycho-motor skills at the workplace or during medical simulations. In the CPR Tutor, we use Microsoft Kinect and Myo to track trainee’s body position and the ResusciAnne QCPR manikin to get correct CPR performance metrics. We then use a validated approach, the Multimodal Pipeline, for the collection, storage, processing, annotation of multimodal data. This paper describes the preliminary results obtained in the ﬁrst design of the CPR Tutor.},
	language = {en},
	author = {Mitri, Daniele Di},
}

@inproceedings{sharma_predicting_2020,
	address = {Frankfurt Germany},
	title = {Predicting learners' effortful behaviour in adaptive assessment using multimodal data},
	isbn = {978-1-4503-7712-6},
	url = {https://dl.acm.org/doi/10.1145/3375462.3375498},
	doi = {10.1145/3375462.3375498},
	abstract = {Many factors influence learners’ performance on an activity beyond the knowledge required. Learners’ on-task effort has been acknowledged for strongly relating to their educational outcomes, reflecting how actively they are engaged in that activity. However, effort is not directly observable. Multimodal data can provide additional insights into the learning processes and may allow for effort estimation. This paper presents an approach for the classification of effort in an adaptive assessment context. Specifically, the behaviour of 32 students was captured during an adaptive self-assessment activity, using logs and physiological data (i.e., eye-tracking, EEG, wristband and facial expressions). We applied k-means to the multimodal data to cluster students’ behavioural patterns. Next, we predicted students’ effort to complete the upcoming task, based on the discovered behavioural patterns using a combination of Hidden Markov Models (HMMs) and the Viterbi algorithm. We also compared the results with other state-of-the-art classification algorithms (SVM, Random Forest). Our findings provide evidence that HMMs can encode the relationship between effort and behaviour (captured by the multimodal data) in a more efficient way than the other methods. Foremost, a practical implication of the approach is that the derived HMMs also pinpoint the moments to provide preventive/prescriptive feedback to the learners in real-time, by building-upon the relationship between behavioural patterns and the effort the learners are putting in.},
	language = {en},
	urldate = {2023-02-08},
	booktitle = {Proceedings of the {Tenth} {International} {Conference} on {Learning} {Analytics} \& {Knowledge}},
	publisher = {ACM},
	author = {Sharma, Kshitij and Papamitsiou, Zacharoula and Olsen, Jennifer K. and Giannakos, Michail},
	month = mar,
	year = {2020},
	pages = {480--489},
}

@inproceedings{lee-cultura_motion-based_2020,
	address = {Osaka, Japan},
	title = {Motion-{Based} {Educational} {Games}: {Using} {Multi}-{Modal} {Data} to {Predict} {Player}’s {Performance}},
	isbn = {978-1-72814-533-4},
	shorttitle = {Motion-{Based} {Educational} {Games}},
	url = {https://ieeexplore.ieee.org/document/9231892/},
	doi = {10.1109/CoG47356.2020.9231892},
	abstract = {Multi-Modal Data (MMD) can help educational games researchers understand the synergistic relationship between player’s movement and their learning experiences, and consequently uncover insights that may lead to improved design of movement-based game technologies for learning. Predicting player performance fosters opportunities to cultivate heightened educational experiences and outcomes. However, predicting player’s performance utilising player-generated MMD during their interactions with educational Motion-Based Touchless Games (MBTG) is challenging. To bridge this gap, we implemented an in-situ study where 26 users, age 11, played 2 maths MBTGs in a single 20-30 minute session. We collected player’s MMD (i.e., gaze data from eye-tracking glasses, physiological data from wristbands, and skeleton data from Kinect) produced during game-play. To investigate the potential of MMD for predicting player’s academic performance, we used machine learning techniques and MMD derived from player’s game-play. This allowed us to identify the MMD features that drive rapid highly accurate predictions of players’ academic performance in educational MBTGs. This might allow us to provide realtime proactive feedback to the player to support them through their educational gaming experience. Our analysis compared two data lengths corresponding to half and full duration of the player’s question solving time. We showed that all combinations of extracted features associated with gaze, physiological, and skeleton data, predicted student performance more accurately than the majority baseline. Additionally, the most accurate prediction of player’s performance derived from the combination of gaze and physiological data for both full and half data lengths. Our ﬁndings emphasise the signiﬁcance of using MMD for realtime performance prediction in educational MBTG and offer implications for practice.},
	language = {en},
	urldate = {2023-02-08},
	booktitle = {2020 {IEEE} {Conference} on {Games} ({CoG})},
	publisher = {IEEE},
	author = {Lee-Cultura, Serena and Sharma, Kshitij and Papavlasopoulou, Sofia and Giannakos, Michail},
	month = aug,
	year = {2020},
	pages = {17--24},
}

@article{emerson_multimodal_2020,
	title = {Multimodal learning analytics for game‐based learning},
	volume = {51},
	issn = {0007-1013, 1467-8535},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/bjet.12992},
	doi = {10.1111/bjet.12992},
	abstract = {A distinctive feature of game-based learning environments is their capacity to create learning experiences that are both effective and engaging. Recent advances in sensorbased technologies such as facial expression analysis and gaze tracking have introduced the opportunity to leverage multimodal data streams for learning analytics. Learning analytics informed by multimodal data captured during students’ interactions with game-based learning environments hold significant promise for developing a deeper understanding of game-based learning, designing game-based learning environments to detect maladaptive behaviors and informing adaptive scaffolding to support individualized learning. This paper introduces a multimodal learning analytics approach that incorporates student gameplay, eye tracking and facial expression data to predict student posttest performance and interest after interacting with a game-based learning environment, Crystal Island. We investigated the degree to which separate and combined modalities (ie, gameplay, facial expressions of emotions and eye gaze) captured from students (n = 65) were predictive of student posttest performance and interest after interacting with Crystal Island. Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. We discuss the synergistic effects of combining modalities for predicting both student interest and posttest performance. The findings suggest that multimodal learning analytics can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding.},
	language = {en},
	number = {5},
	urldate = {2023-02-08},
	journal = {British Journal of Educational Technology},
	author = {Emerson, Andrew and Cloude, Elizabeth B. and Azevedo, Roger and Lester, James},
	month = sep,
	year = {2020},
	pages = {1505--1526},
}

@article{nguyen_examining_2023,
	title = {Examining socially shared regulation and shared physiological arousal events with multimodal learning analytics},
	volume = {54},
	issn = {0007-1013, 1467-8535},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/bjet.13280},
	doi = {10.1111/bjet.13280},
	language = {en},
	number = {1},
	urldate = {2023-02-08},
	journal = {British Journal of Educational Technology},
	author = {Nguyen, Andy and Järvelä, Sanna and Rosé, Carolyn and Järvenoja, Hanna and Malmberg, Jonna},
	month = jan,
	year = {2023},
	pages = {293--312},
}

@incollection{virvou_multimodal_2020,
	address = {Cham},
	title = {Multimodal {Learning} {Analytics} in a {Laboratory} {Classroom}},
	volume = {158},
	isbn = {978-3-030-13742-7 978-3-030-13743-4},
	url = {http://link.springer.com/10.1007/978-3-030-13743-4_8},
	abstract = {Sophisticated research approaches and tools can help researchers to investigate the complex processes involved in learning in various settings. The use of video technology to record classroom practices, in particular, can be a powerful way for capturing and studying learning and related phenomena within a social setting such as the classroom. This chapter outlines several multimodal techniques to analyze the learning activities in a laboratory classroom. The video and audio recordings were processed automatically to obtain information rather than requiring manual coding. Moreover, these automated techniques are able to extract information with an efﬁciency that is beyond the capabilities of human-coders, providing the means to deal analytically with the multiple modalities that characterize the classroom. Once generated, the information provided by the different modalities is used to explain and predict high-level constructs such as students’ attention and engagement. This chapter not only presents the results of the analysis, but also describes the setting, hardware and software needed to replicate this analytical approach.},
	language = {en},
	urldate = {2023-02-08},
	booktitle = {Machine {Learning} {Paradigms}},
	publisher = {Springer International Publishing},
	author = {Chan, Man Ching Esther and Ochoa, Xavier and Clarke, David},
	editor = {Virvou, Maria and Alepis, Efthimios and Tsihrintzis, George A. and Jain, Lakhmi C.},
	year = {2020},
	doi = {10.1007/978-3-030-13743-4_8},
	note = {Series Title: Intelligent Systems Reference Library},
	pages = {131--156},
}

@article{tan_analysing_2020,
	title = {Analysing student engagement with 360-degree videos through multimodal data analytics and user annotations},
	volume = {29},
	issn = {1475-939X, 1747-5139},
	url = {https://www.tandfonline.com/doi/full/10.1080/1475939X.2020.1835708},
	doi = {10.1080/1475939X.2020.1835708},
	language = {en},
	number = {5},
	urldate = {2023-02-08},
	journal = {Technology, Pedagogy and Education},
	author = {Tan, Sabine and Wiebrands, Michael and O’Halloran, Kay and Wignell, Peter},
	month = oct,
	year = {2020},
	pages = {593--612},
}

@incollection{giannakos_multimodal_2022,
	address = {Cham},
	title = {Multimodal {Learning} {Experience} for {Deliberate} {Practice}},
	isbn = {978-3-031-08075-3 978-3-031-08076-0},
	url = {https://link.springer.com/10.1007/978-3-031-08076-0_8},
	abstract = {While digital education technologies have improved to make educational resources more available, the modes of interaction they implement remain largely unnatural for the learner. Modern sensor-enabled computer systems allow extending human-computer interfaces for multimodal communication. Advances in Artiﬁcial Intelligence allow interpreting the data collected from multimodal and multi-sensor devices. These insights can be used to support deliberate practice with personalised feedback and adaptation through Multimodal Learning Experiences (MLX). This chapter elaborates on the approaches, architectures, and methodologies in ﬁve different use cases that use multimodal learning analytics applications for deliberate practice.},
	language = {en},
	urldate = {2023-02-08},
	booktitle = {The {Multimodal} {Learning} {Analytics} {Handbook}},
	publisher = {Springer International Publishing},
	author = {Di Mitri, Daniele and Schneider, Jan and Limbu, Bibeg and Mat Sanusi, Khaleel Asyraaf and Klemke, Roland},
	editor = {Giannakos, Michail and Spikol, Daniel and Di Mitri, Daniele and Sharma, Kshitij and Ochoa, Xavier and Hammad, Rawad},
	year = {2022},
	doi = {10.1007/978-3-031-08076-0_8},
	pages = {183--204},
}

@incollection{rau_integrating_2019,
	address = {Cham},
	title = {Integrating {Multimodal} {Learning} {Analytics} and {Inclusive} {Learning} {Support} {Systems} for {People} of {All} {Ages}},
	volume = {11577},
	isbn = {978-3-030-22579-7 978-3-030-22580-3},
	url = {http://link.springer.com/10.1007/978-3-030-22580-3_35},
	abstract = {Extended learning environments involving system to collect data for learning analytics and to support learners will be useful for all-age education. As the ﬁrst steps towards to build new learning environments, we developed a system for multimodal learning analytics using eye-tracker and EEG measurement, and inclusive user interface design for elderly learners by dual-tablet system. Multimodal learning analytics system can be supportive to extract where and how learners with varied backgrounds feel difﬁculty in learning process. The eye-tracker can retrieve information where the learners paid attention. EEG signals will provide clues to estimate their mental states during gazes in learning. We developed simultaneous measurement system of these multimodal responses and are trying to integrate the information to explore learning problems. A dual-tablet user interface with simpliﬁed visual layers and more intuitive operations was designed aiming to reduce the physical and mental loads of elderly learners. A prototype was developed based on a cross-platform framework, which is being reﬁned by iterative formative evaluations participated by elderlies, in order to improve the usability of the interface design. We propose a system architecture applying the multimodal learning analytics and the userfriendly design for elderly learners, which couples learning analytics “in the wild” environment and learning analytics in controlled lab environments.},
	language = {en},
	urldate = {2023-02-08},
	booktitle = {Cross-{Cultural} {Design}. {Culture} and {Society}},
	publisher = {Springer International Publishing},
	author = {Tamura, Kaori and Lu, Min and Konomi, Shin’ichi and Hatano, Kohei and Inaba, Miyuki and Oi, Misato and Okamoto, Tsuyoshi and Okubo, Fumiya and Shimada, Atsushi and Wang, Jingyun and Yamada, Masanori and Yamada, Yuki},
	editor = {Rau, Pei-Luen Patrick},
	year = {2019},
	doi = {10.1007/978-3-030-22580-3_35},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {469--481},
}

@article{shankar_multimodal_2020,
	title = {Multimodal {Data} {Value} {Chain} ({M}-{DVC}): {A} {Conceptual} {Tool} to {Support} the {Development} of {Multimodal} {Learning} {Analytics} {Solutions}},
	volume = {15},
	issn = {1932-8540, 2374-0132},
	shorttitle = {Multimodal {Data} {Value} {Chain} ({M}-{DVC})},
	url = {https://ieeexplore.ieee.org/document/9066924/},
	doi = {10.1109/RITA.2020.2987887},
	abstract = {Multimodal Learning Analytics (MMLA) systems, understood as those that exploit multimodal evidence of learning to better model a learning situation, have not yet spread widely in educational practice. Their inherent technical complexity, and the lack of educational stakeholder involvement in their design, are among the hypothesized reasons for the slow uptake of this emergent ﬁeld. To aid in the process of stakeholder communication and systematization leading to the speciﬁcation of MMLA systems, this paper proposes a Multimodal Data Value Chain (M-DVC). This conceptual tool, derived from both the ﬁeld of Big Data and the needs of MMLA scenarios, has been evaluated in terms of its usefulness for stakeholders, in three authentic case studies of MMLA systems currently under development. The results of our mixed-methods evaluation highlight the usefulness of the M-DVC to elicit unspoken assumptions or unclear data processing steps in the initial stages of development. The evaluation also revealed limitations of the M-DVC in terms of the technical terminology employed, and the need for more detailed contextual information to be included. These limitations also prompt potential improvements for the M-DVC, on the path towards clearer speciﬁcation and communication within the multi-disciplinary teams needed to build educationally-meaningful MMLA solutions.},
	language = {en},
	number = {2},
	urldate = {2023-02-08},
	journal = {IEEE Revista Iberoamericana de Tecnologias del Aprendizaje},
	author = {Shankar, Shashi Kant and Rodriguez-Triana, Maria Jesus and Ruiz-Calleja, Adolfo and Prieto, Luis P. and Chejara, Pankaj and Martinez-Mones, Alejandra},
	month = may,
	year = {2020},
	pages = {113--122},
}

@inproceedings{henderson_enhancing_2020,
	address = {Virtual Event Netherlands},
	title = {Enhancing {Affect} {Detection} in {Game}-{Based} {Learning} {Environments} with {Multimodal} {Conditional} {Generative} {Modeling}},
	isbn = {978-1-4503-7581-8},
	url = {https://dl.acm.org/doi/10.1145/3382507.3418892},
	doi = {10.1145/3382507.3418892},
	language = {en},
	urldate = {2023-02-08},
	booktitle = {Proceedings of the 2020 {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {ACM},
	author = {Henderson, Nathan and Min, Wookhee and Rowe, Jonathan and Lester, James},
	month = oct,
	year = {2020},
	pages = {134--143},
}

@article{worsley_situating_2016,
	title = {Situating {Multimodal} {Learning} {Analytics}},
	abstract = {The digital age has introduced a host of new challenges and opportunities for the learning sciences community. These challenges and opportunities are particularly abundant in multimodal learning analytics (MMLA), a research methodology that aims to extend work from Educational Data Mining (EDM) and Learning Analytics (LA) to multimodal learning environments by treating multimodal data. Recognizing the short-term opportunities and longterm challenges will help develop proof cases and identify grand challenges that will help propel the field forward. To support the field’s growth, we use this paper to describe several ways that MMLA can potentially advance learning sciences research and touch upon key challenges that researchers who utilize MMLA have encountered over the past few years.},
	language = {en},
	author = {Worsley, Marcelo and Abrahamson, Dor and Blikstein, Paulo and Grover, Shuchi and Schneider, Bertrand and Tissenbaum, Mike},
	year = {2016},
	pages = {5},
}

@incollection{bittencourt_real-time_2020-1,
	address = {Cham},
	title = {Real-{Time} {Multimodal} {Feedback} with the {CPR} {Tutor}},
	volume = {12163},
	isbn = {978-3-030-52236-0 978-3-030-52237-7},
	url = {http://link.springer.com/10.1007/978-3-030-52237-7_12},
	abstract = {We developed the CPR Tutor, a real-time multimodal feedback system for cardiopulmonary resuscitation (CPR) training. The CPR Tutor detects mistakes using recurrent neural networks for real-time time-series classiﬁcation. From a multimodal data stream consisting of kinematic and electromyographic data, the CPR Tutor system automatically detects the chest compressions, which are then classiﬁed and assessed according to ﬁve performance indicators. Based on this assessment, the CPR Tutor provides audio feedback to correct the most critical mistakes and improve the CPR performance. To test the validity of the CPR Tutor, we ﬁrst collected the data corpus from 10 experts used for model training. Hence, to test the impact of the feedback functionality, we ran a user study involving 10 participants. The CPR Tutor pushes forward the current state of the art of real-time multimodal tutors by providing: 1) an architecture design, 2) a methodological approach to design multimodal feedback and 3) a ﬁeld study on real-time feedback for CPR training.},
	language = {en},
	urldate = {2022-07-15},
	booktitle = {Artificial {Intelligence} in {Education}},
	publisher = {Springer International Publishing},
	author = {Di Mitri, Daniele and Schneider, Jan and Trebing, Kevin and Sopka, Sasa and Specht, Marcus and Drachsler, Hendrik},
	editor = {Bittencourt, Ig Ibert and Cukurova, Mutlu and Muldner, Kasia and Luckin, Rose and Millán, Eva},
	year = {2020},
	doi = {10.1007/978-3-030-52237-7_12},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {141--152},
}

@inproceedings{barmaki_providing_2015,
	address = {Seattle Washington USA},
	title = {Providing {Real}-time {Feedback} for {Student} {Teachers} in a {Virtual} {Rehearsal} {Environment}},
	isbn = {978-1-4503-3912-4},
	url = {https://dl.acm.org/doi/10.1145/2818346.2830604},
	doi = {10.1145/2818346.2830604},
	abstract = {Research in learning analytics and educational data mining has recently become prominent in the ﬁelds of computer science and education. Most scholars in the ﬁeld emphasize student learning and student data analytics; however, it is also important to focus on teaching analytics and teacher preparation because of their key roles in student learning, especially in K-12 learning environments. Nonverbal communication strategies play an important role in successful interpersonal communication of teachers with their students. In order to assist novice or practicing teachers with exhibiting open and aﬃrmative nonverbal cues in their classrooms, we have designed a multimodal teaching platform with provisions for online feedback. We used an interactive teaching rehearsal software, TeachLivETM, as our basic research environment. TeachLivE employs a digital puppetry paradigm as its core technology. Individuals walk into this virtual environment and interact with virtual students displayed on a large screen. They can practice classroom management, pedagogy and content delivery skills with a teaching plan in the TeachLivE environment.},
	language = {en},
	urldate = {2022-07-15},
	booktitle = {Proceedings of the 2015 {ACM} on {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {ACM},
	author = {Barmaki, Roghayeh and Hughes, Charles E.},
	month = nov,
	year = {2015},
	pages = {531--537},
}

@misc{rojat_explainable_2021,
	title = {Explainable {Artificial} {Intelligence} ({XAI}) on {TimeSeries} {Data}: {A} {Survey}},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI}) on {TimeSeries} {Data}},
	url = {http://arxiv.org/abs/2104.00950},
	abstract = {Most of state of the art methods applied on time series consist of deep learning methods that are too complex to be interpreted. This lack of interpretability is a major drawback, as several applications in the real world are critical tasks, such as the medical ﬁeld or the autonomous driving ﬁeld. The explainability of models applied on time series has not gather much attention compared to the computer vision or the natural language processing ﬁelds. In this paper, we present an overview of existing explainable AI (XAI) methods applied on time series and illustrate the type of explanations they produce. We also provide a reﬂection on the impact of these explanation methods to provide conﬁdence and trust in the AI systems.},
	language = {en},
	urldate = {2022-07-15},
	publisher = {arXiv},
	author = {Rojat, Thomas and Puget, Raphaël and Filliat, David and Del Ser, Javier and Gelin, Rodolphe and Díaz-Rodríguez, Natalia},
	month = apr,
	year = {2021},
	note = {arXiv:2104.00950 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{linardatos_explainable_2020,
	title = {Explainable {AI}: {A} {Review} of {Machine} {Learning} {Interpretability} {Methods}},
	volume = {23},
	issn = {1099-4300},
	shorttitle = {Explainable {AI}},
	url = {https://www.mdpi.com/1099-4300/23/1/18},
	doi = {10.3390/e23010018},
	abstract = {Recent advances in artiﬁcial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a signiﬁcant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientiﬁc interest in the ﬁeld of Explainable Artiﬁcial Intelligence (XAI), a ﬁeld that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more speciﬁcally, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.},
	language = {en},
	number = {1},
	urldate = {2022-07-15},
	journal = {Entropy},
	author = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
	month = dec,
	year = {2020},
	pages = {18},
}

@article{giannakos_sensing-based_2021,
	title = {Sensing-{Based} {Analytics} in {Education}: {The} {Rise} of {Multimodal} {Data} {Enabled} {Learning} {Systems}},
	volume = {23},
	issn = {1520-9202, 1941-045X},
	shorttitle = {Sensing-{Based} {Analytics} in {Education}},
	url = {https://ieeexplore.ieee.org/document/9655388/},
	doi = {10.1109/MITP.2021.3089659},
	language = {en},
	number = {6},
	urldate = {2022-07-15},
	journal = {IT Professional},
	author = {Giannakos, Michail N. and Lee-Cultura, Serena and Sharma, Kshitij},
	month = nov,
	year = {2021},
	pages = {31--38},
}

@incollection{rau_design_2019,
	address = {Cham},
	title = {Design of an {Online} {Education} {Evaluation} {System} {Based} on {Multimodal} {Data} of {Learners}},
	volume = {11577},
	isbn = {978-3-030-22579-7 978-3-030-22580-3},
	url = {http://link.springer.com/10.1007/978-3-030-22580-3_34},
	abstract = {Online education breaks the time and space constraints of learning, but it also presents some new challenges for the teachers: less interaction between instructors and learners, and loss of real-time feedback of teaching effects. Our study aims to ﬁll these gaps by designing a tool for instructors that shows how learners’ status change along the lecture video timeline. The study uses multimodal data consist of facial expressions and timeline-anchored comments and labels the data with two learning status dimensions (difﬁculty and interestingness). To acquire training dataset, 20 teaching video clips are selected, and 15 volunteers are invited to watch the videos to collect their facial expressions and subjective learning status ratings. Then we build a fusion model with results from a CNN (Convolutional Neural Network) model and a LSTM (Long Short-Term Memory) model, and design an effective interface to present feedbacks from the model. After evaluation of the model, we put forward some possible improvements and future prospects for this design.},
	language = {en},
	urldate = {2022-07-15},
	booktitle = {Cross-{Cultural} {Design}. {Culture} and {Society}},
	publisher = {Springer International Publishing},
	author = {Peng, Qijia and Qie, Nan and Yuan, Liang and Chen, Yue and Gao, Qin},
	editor = {Rau, Pei-Luen Patrick},
	year = {2019},
	doi = {10.1007/978-3-030-22580-3_34},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {458--468},
}

@inproceedings{di_mitri_read_2019,
	address = {Tempe AZ USA},
	title = {Read {Between} the {Lines}: {An} {Annotation} {Tool} for {Multimodal} {Data} for {Learning}},
	isbn = {978-1-4503-6256-6},
	shorttitle = {Read {Between} the {Lines}},
	url = {https://dl.acm.org/doi/10.1145/3303772.3303776},
	doi = {10.1145/3303772.3303776},
	abstract = {This paper introduces the Visual Inspection Tool (VIT) which supports researchers in the annotation of multimodal data as well as the processing and exploitation for learning purposes. While most of the existing Multimodal Learning Analytics (MMLA) solutions are tailor-made for specific learning tasks and sensors, the VIT addresses the data annotation for different types of learning tasks that can be captured with a customisable set of sensors in a flexible way. The VIT supports MMLA researchers in 1) triangulating multimodal data with video recordings; 2) segmenting the multimodal data into time-intervals and adding annotations to the time-intervals; 3) downloading the annotated dataset and using it for multimodal data analysis. The VIT is a crucial component that was so far missing in the available tools for MMLA research. By filling this gap we also identified an integrated workflow that characterises current MMLA research. We call this workflow the Multimodal Learning Analytics Pipeline, a toolkit for orchestration, the use and application of various MMLA tools.},
	language = {en},
	urldate = {2022-07-15},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Learning} {Analytics} \& {Knowledge}},
	publisher = {ACM},
	author = {Di Mitri, Daniele and Schneider, Jan and Klemke, Roland and Specht, Marcus and Drachsler, Hendrik},
	month = mar,
	year = {2019},
	pages = {51--60},
}

@article{di_mitri_keep_2021,
	title = {Keep {Me} in the {Loop}: {Real}-{Time} {Feedback} with {Multimodal} {Data}},
	issn = {1560-4292, 1560-4306},
	shorttitle = {Keep {Me} in the {Loop}},
	url = {https://link.springer.com/10.1007/s40593-021-00281-z},
	doi = {10.1007/s40593-021-00281-z},
	abstract = {This paper describes the CPR Tutor, a real-time multimodal feedback system for cardiopulmonary resuscitation (CPR) training. The CPR Tutor detects training mistakes using recurrent neural networks. The CPR Tutor automatically recognises and assesses the quality of the chest compressions according to five CPR performance indicators. It detects training mistakes in real-time by analysing a multimodal data stream consisting of kinematic and electromyographic data. Based on this assessment, the CPR Tutor provides audio feedback to correct the most critical mistakes and improve the CPR performance. The mistake detection models of the CPR Tutor were trained using a dataset from 10 experts. Hence, we tested the validity of the CPR Tutor and the impact of its feedback functionality in a user study involving additional 10 participants. The CPR Tutor pushes forward the current state of the art of real-time multimodal tutors by providing: (1) an architecture design, (2) a methodological approach for delivering real-time feedback using multimodal data and (3) a field study on real-time feedback for CPR training. This paper details the results of a field study by quantitatively measuring the impact of the CPR Tutor feedback on the performance indicators and qualitatively analysing the participants’ questionnaire answers.},
	language = {en},
	urldate = {2022-07-15},
	journal = {International Journal of Artificial Intelligence in Education},
	author = {Di Mitri, Daniele and Schneider, Jan and Drachsler, Hendrik},
	month = nov,
	year = {2021},
}

@incollection{bittencourt_real-time_2020-2,
	address = {Cham},
	title = {Real-{Time} {Multimodal} {Feedback} with the {CPR} {Tutor}},
	volume = {12163},
	isbn = {978-3-030-52236-0 978-3-030-52237-7},
	url = {http://link.springer.com/10.1007/978-3-030-52237-7_12},
	abstract = {We developed the CPR Tutor, a real-time multimodal feedback system for cardiopulmonary resuscitation (CPR) training. The CPR Tutor detects mistakes using recurrent neural networks for real-time time-series classiﬁcation. From a multimodal data stream consisting of kinematic and electromyographic data, the CPR Tutor system automatically detects the chest compressions, which are then classiﬁed and assessed according to ﬁve performance indicators. Based on this assessment, the CPR Tutor provides audio feedback to correct the most critical mistakes and improve the CPR performance. To test the validity of the CPR Tutor, we ﬁrst collected the data corpus from 10 experts used for model training. Hence, to test the impact of the feedback functionality, we ran a user study involving 10 participants. The CPR Tutor pushes forward the current state of the art of real-time multimodal tutors by providing: 1) an architecture design, 2) a methodological approach to design multimodal feedback and 3) a ﬁeld study on real-time feedback for CPR training.},
	language = {en},
	urldate = {2022-07-15},
	booktitle = {Artificial {Intelligence} in {Education}},
	publisher = {Springer International Publishing},
	author = {Di Mitri, Daniele and Schneider, Jan and Trebing, Kevin and Sopka, Sasa and Specht, Marcus and Drachsler, Hendrik},
	editor = {Bittencourt, Ig Ibert and Cukurova, Mutlu and Muldner, Kasia and Luckin, Rose and Millán, Eva},
	year = {2020},
	doi = {10.1007/978-3-030-52237-7_12},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {141--152},
}

@inproceedings{van_rosmalen_feedback_2015,
	address = {Lisbon, Portugal},
	title = {Feedback {Design} in {Multimodal} {Dialogue} {Systems}:},
	isbn = {978-989-758-108-3},
	shorttitle = {Feedback {Design} in {Multimodal} {Dialogue} {Systems}},
	url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0005423102090217},
	doi = {10.5220/0005423102090217},
	abstract = {Feedback, Sensors, Instructional Design, Multimodal Dialogue, Reflection Support.},
	language = {en},
	urldate = {2022-07-15},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Computer} {Supported} {Education}},
	publisher = {SCITEPRESS - Science and and Technology Publications},
	author = {Van Rosmalen, Peter and Börner, Dirk and Schneider, Jan and Petukhova, Olga and van Helvert, Joy},
	year = {2015},
	pages = {209--217},
}

@inproceedings{ochoa_rap_2018-1,
	address = {Sydney New South Wales Australia},
	title = {The {RAP} system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors},
	isbn = {978-1-4503-6400-3},
	shorttitle = {The {RAP} system},
	url = {https://dl.acm.org/doi/10.1145/3170358.3170406},
	doi = {10.1145/3170358.3170406},
	abstract = {Developing communication skills in higher education students could be a challenge to professors due to the time needed to provide formative feedback. This work presents RAP, a scalable system to provide automatic feedback to entry-level students to develop basic oral presentation skills. The system improves the state-ofthe-art by analyzing posture, gaze, volume, filled pauses and the slides of the presenters through data captured by very low-cost sensors. The system also provides an off-line feedback report with multimodal recordings of their performance. An initial evaluation of the system indicates that the system’s feedback highly agrees with human feedback and that students considered that feedback useful to develop their oral presentation skills.},
	language = {en},
	urldate = {2022-07-15},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Learning} {Analytics} and {Knowledge}},
	publisher = {ACM},
	author = {Ochoa, Xavier and Domínguez, Federico and Guamán, Bruno and Maya, Ricardo and Falcones, Gabriel and Castells, Jaime},
	month = mar,
	year = {2018},
	pages = {360--364},
}

@article{joshi_review_2021,
	title = {A {Review} on {Explainability} in {Multimodal} {Deep} {Neural} {Nets}},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9391727/},
	doi = {10.1109/ACCESS.2021.3070212},
	abstract = {Artiﬁcial Intelligence techniques powered by deep neural nets have achieved much success in several application domains, most signiﬁcantly and notably in the Computer Vision applications and Natural Language Processing tasks. Surpassing human-level performance propelled the research in the applications where different modalities amongst language, vision, sensory, text play an important role in accurate predictions and identiﬁcation. Several multimodal fusion methods employing deep learning models are proposed in the literature. Despite their outstanding performance, the complex, opaque and black-box nature of the deep neural nets limits their social acceptance and usability. This has given rise to the quest for model interpretability and explainability, more so in the complex tasks involving multimodal AI methods. This paper extensively reviews the present literature to present a comprehensive survey and commentary on the explainability in multimodal deep neural nets, especially for the vision and language tasks. Several topics on multimodal AI and its applications for generic domains have been covered in this paper, including the signiﬁcance, datasets, fundamental building blocks of the methods and techniques, challenges, applications, and future trends in this domain.},
	language = {en},
	urldate = {2022-07-15},
	journal = {IEEE Access},
	author = {Joshi, Gargi and Walambe, Rahee and Kotecha, Ketan},
	year = {2021},
	pages = {59800--59821},
}

@article{singh_explainable_2020,
	title = {Explainable {Deep} {Learning} {Models} in {Medical} {Image} {Analysis}},
	volume = {6},
	issn = {2313-433X},
	url = {https://www.mdpi.com/2313-433X/6/6/52},
	doi = {10.3390/jimaging6060052},
	abstract = {Deep learning methods have been very effective for a variety of medical diagnostic tasks and have even outperformed human experts on some of those. However, the black-box nature of the algorithms has restricted their clinical use. Recent explainability studies aim to show the features that inﬂuence the decision of a model the most. The majority of literature reviews of this area have focused on taxonomy, ethics, and the need for explanations. A review of the current applications of explainable deep learning for different medical imaging tasks is presented here. The various approaches, challenges for clinical deployment, and the areas requiring further research are discussed here from a practical standpoint of a deep learning researcher designing a system for the clinical end-users.},
	language = {en},
	number = {6},
	urldate = {2022-07-15},
	journal = {Journal of Imaging},
	author = {Singh, Amitojdeep and Sengupta, Sourya and Lakshminarayanan, Vasudevan},
	month = jun,
	year = {2020},
	pages = {52},
}

@article{iv_multimodal_2022,
	title = {Multimodal {Classification}: {Current} {Landscape}, {Taxonomy} and {Future} {Directions}},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Multimodal {Classification}},
	url = {https://dl.acm.org/doi/10.1145/3543848},
	doi = {10.1145/3543848},
	abstract = {Multimodal classiication research has been gaining popularity with new datasets in domains such as satellite imagery, biometrics, and medicine. Prior research has shown the beneits of combining data from multiple sources compared to traditional unimodal data which has led to the development of many novel multimodal architectures. However, the lack of consistent terminologies and architectural descriptions makes it diicult to compare diferent solutions. We address these challenges by proposing a new taxonomy for describing multimodal classiication models based on trends found in recent publications. Examples of how this taxonomy could be applied to existing models are presented as well as a checklist to aid in the clear and complete presentation of future models. Many of the most diicult aspects of unimodal classiication have not yet been fully addressed for multimodal datasets including big data, class imbalance, and instance level diiculty. We also provide a discussion of these challenges and future directions of research. CCS Concepts: · Computing methodologies → Classiication and regression trees; Neural networks.},
	language = {en},
	urldate = {2022-06-17},
	journal = {ACM Computing Surveys},
	author = {Iv, William C. Sleeman and Kapoor, Rishabh and Ghosh, Preetam},
	month = jun,
	year = {2022},
	pages = {3543848},
}

@article{iv_multimodal_2022-1,
	title = {Multimodal {Classification}: {Current} {Landscape}, {Taxonomy} and {Future} {Directions}},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Multimodal {Classification}},
	url = {https://dl.acm.org/doi/10.1145/3543848},
	doi = {10.1145/3543848},
	abstract = {Multimodal classiication research has been gaining popularity with new datasets in domains such as satellite imagery, biometrics, and medicine. Prior research has shown the beneits of combining data from multiple sources compared to traditional unimodal data which has led to the development of many novel multimodal architectures. However, the lack of consistent terminologies and architectural descriptions makes it diicult to compare diferent solutions. We address these challenges by proposing a new taxonomy for describing multimodal classiication models based on trends found in recent publications. Examples of how this taxonomy could be applied to existing models are presented as well as a checklist to aid in the clear and complete presentation of future models. Many of the most diicult aspects of unimodal classiication have not yet been fully addressed for multimodal datasets including big data, class imbalance, and instance level diiculty. We also provide a discussion of these challenges and future directions of research. CCS Concepts: · Computing methodologies → Classiication and regression trees; Neural networks.},
	language = {en},
	urldate = {2022-06-17},
	journal = {ACM Computing Surveys},
	author = {Iv, William C. Sleeman and Kapoor, Rishabh and Ghosh, Preetam},
	month = jun,
	year = {2022},
	pages = {3543848},
}

@article{philippe_multimodal_2020,
	title = {Multimodal teaching, learning and training in virtual reality: a review and case study},
	volume = {2},
	issn = {2096-5796},
	shorttitle = {Multimodal teaching, learning and training in virtual reality},
	url = {https://www.sciencedirect.com/science/article/pii/S2096579620300711},
	doi = {10.1016/j.vrih.2020.07.008},
	abstract = {It is becoming increasingly prevalent in digital learning research to encompass an array of different meanings, spaces, processes, and teaching strategies for discerning a global perspective on constructing the student learning experience. Multimodality is an emergent phenomenon that may influence how digital learning is designed, especially when employed in highly interactive and immersive learning environments such as Virtual Reality (VR). VR environments may aid students' efforts to be active learners through consciously attending to, and reflecting on, critique leveraging reflexivity and novel meaning-making most likely to lead to a conceptual change. This paper employs eleven industrial case-studies to highlight the application of multimodal VR-based teaching and training as a pedagogically rich strategy that may be designed, mapped and visualized through distinct VR-design elements and features. The outcomes of the use cases contribute to discern in-VR multimodal teaching as an emerging discourse that couples system design-based paradigms with embodied, situated and reflective praxis in spatial, emotional and temporal VR learning environments.},
	language = {en},
	number = {5},
	urldate = {2022-06-17},
	journal = {Virtual Reality \& Intelligent Hardware},
	author = {Philippe, Stéphanie and Souchet, Alexis D. and Lameras, Petros and Petridis, Panagiotis and Caporal, Julien and Coldeboeuf, Gildas and Duzan, Hadrien},
	month = oct,
	year = {2020},
	keywords = {Multimodality, Semiotic resources, Teaching and learning, Training, Virtual reality},
	pages = {421--442},
}

@article{philippe_multimodal_2020-1,
	title = {Multimodal teaching, learning and training in virtual reality: a review and case study},
	volume = {2},
	issn = {20965796},
	shorttitle = {Multimodal teaching, learning and training in virtual reality},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2096579620300711},
	doi = {10.1016/j.vrih.2020.07.008},
	abstract = {It is becoming increasingly prevalent in digital learning research to encompass an array of different meanings, spaces, processes, and teaching strategies for discerning a global perspective on constructing the student learning experience. Multimodality is an emergent phenomenon that may influence how digital learning is designed, especially when employed in highly interactive and immersive learning environments such as Virtual Reality (VR). VR environments may aid students' efforts to be active learners through consciously attending to, and reflecting on, critique leveraging reflexivity and novel meaning-making most likely to lead to a conceptual change. This paper employs eleven industrial case-studies to highlight the application of multimodal VR-based teaching and training as a pedagogically rich strategy that may be designed, mapped and visualized through distinct VR-design elements and features. The outcomes of the use cases contribute to discern in-VR multimodal teaching as an emerging discourse that couples system design-based paradigms with embodied, situated and reflective praxis in spatial, emotional and temporal VR learning environments.},
	language = {en},
	number = {5},
	urldate = {2022-06-17},
	journal = {Virtual Reality \& Intelligent Hardware},
	author = {Philippe, Stéphanie and Souchet, Alexis D. and Lameras, Petros and Petridis, Panagiotis and Caporal, Julien and Coldeboeuf, Gildas and Duzan, Hadrien},
	month = oct,
	year = {2020},
	pages = {421--442},
}

@article{di_mitri_signals_2018,
	title = {From signals to knowledge: {A} conceptual model for multimodal learning analytics},
	volume = {34},
	issn = {1365-2729},
	shorttitle = {From signals to knowledge},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/jcal.12288},
	doi = {10.1111/jcal.12288},
	abstract = {Multimodality in learning analytics and learning science is under the spotlight. The landscape of sensors and wearable trackers that can be used for learning support is evolving rapidly, as well as data collection and analysis methods. Multimodal data can now be collected and processed in real time at an unprecedented scale. With sensors, it is possible to capture observable events of the learning process such as learner's behaviour and the learning context. The learning process, however, consists also of latent attributes, such as the learner's cognitions or emotions. These attributes are unobservable to sensors and need to be elicited by human-driven interpretations. We conducted a literature survey of experiments using multimodal data to frame the young research field of multimodal learning analytics. The survey explored the multimodal data used in related studies (the input space) and the learning theories selected (the hypothesis space). The survey led to the formulation of the Multimodal Learning Analytics Model whose main objectives are of (O1) mapping the use of multimodal data to enhance the feedback in a learning context; (O2) showing how to combine machine learning with multimodal data; and (O3) aligning the terminology used in the field of machine learning and learning science.},
	language = {en},
	number = {4},
	urldate = {2022-06-17},
	journal = {Journal of Computer Assisted Learning},
	author = {Di Mitri, Daniele and Schneider, Jan and Specht, Marcus and Drachsler, Hendrik},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jcal.12288},
	keywords = {learning analytics, machine learning, multimodal data, multimodality, sensors, social signal processing},
	pages = {338--349},
}

@inproceedings{severin_head_2021,
	title = {Head {Gesture}-{Based} on {IMU} {Sensors}: a {Performance} {Comparison} {Between} the {Unimodal} and {Multimodal} {Approach}},
	shorttitle = {Head {Gesture}-{Based} on {IMU} {Sensors}},
	doi = {10.1109/ISSCS52333.2021.9497434},
	abstract = {In this paper, we further develop systems used in head gesture recognition based on inertial sensors, previously developed and presented in other research papers. In the current research, we investigate the impact of increasing the number of inertial sensors as a condition to read head orientation with high accuracy. A set of eight predefined commands were evaluated with two systems that contain a single inertial sensor, respectively, three inertial sensors. The computational steps were done based on the Classical Machine Learning algorithms and Deep Learning algorithms. Each algorithm used in the study was trained based on raw data form of inertial signals. This study concluded that the increasing number of inertial sensors in the gestural recognition systems could improve the classification performance in the range of 10\%-22\%. Also, the average classification value calculated on ten predictive models is equal to 6.65\%.},
	booktitle = {2021 {International} {Symposium} on {Signals}, {Circuits} and {Systems} ({ISSCS})},
	author = {Severin, Ionut-Cristian},
	month = jul,
	year = {2021},
	keywords = {Deep Neural Networks, Deep learning, Evaluation metrics, Gesture recognition, Head recognition activity, Inertial sensor, Inertial sensors, Machine learning algorithms, Measurement, Predictive models, Sensor placement, Supervised Machine Learning, Unsupervised Machine Learning},
	pages = {1--4},
}

@misc{noauthor_modality_2022,
	title = {Modality (human–computer interaction)},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Modality_(human%E2%80%93computer_interaction)&oldid=1067172341},
	abstract = {In the context of human–computer interaction, a modality is the classification of a single independent channel of sensory input/output between a computer and a human.
A system is designated unimodal if it has only one modality implemented, and multimodal if it has more than one.  When multiple modalities are available for some tasks or aspects of a task, the system is said to have overlapping modalities. If multiple modalities are available for a task, the system is said to have redundant modalities. Multiple modalities can be used in combination to provide complementary methods that may be redundant but convey information more effectively. Modalities can be generally defined in two forms: human-computer and computer-human modalities.},
	language = {en},
	urldate = {2022-06-17},
	journal = {Wikipedia},
	month = jan,
	year = {2022},
	note = {Page Version ID: 1067172341},
}

@article{di_mitri_signals_2018-1,
	title = {From signals to knowledge: {A} conceptual model for multimodal learning analytics},
	volume = {34},
	issn = {02664909},
	shorttitle = {From signals to knowledge},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/jcal.12288},
	doi = {10.1111/jcal.12288},
	abstract = {Multimodality in learning analytics and learning science is under the spotlight. The landscape of sensors and wearable trackers that can be used for learning support is evolving rapidly, as well as data collection and analysis methods. Multimodal data can now be collected and processed in real time at an unprecedented scale. With sensors, it is possible to capture observable events of the learning process such as learner's behaviour and the learning context. The learning process, however, consists also of latent attributes, such as the learner's cognitions or emotions. These attributes are unobservable to sensors and need to be elicited by human‐driven interpretations. We conducted a literature survey of experiments using multimodal data to frame the young research field of multimodal learning analytics. The survey explored the multimodal data used in related studies (the input space) and the learning theories selected (the hypothesis space). The survey led to the formulation of the Multimodal Learning Analytics Model whose main objectives are of (O1) mapping the use of multimodal data to enhance the feedback in a learning context; (O2) showing how to combine machine learning with multimodal data; and (O3) aligning the terminology used in the field of machine learning and learning science.},
	language = {en},
	number = {4},
	urldate = {2022-06-17},
	journal = {Journal of Computer Assisted Learning},
	author = {Di Mitri, Daniele and Schneider, Jan and Specht, Marcus and Drachsler, Hendrik},
	month = aug,
	year = {2018},
	pages = {338--349},
}

@article{sharma_multimodal_2020,
	title = {Multimodal data capabilities for learning: {What} can multimodal data tell us about learning?},
	volume = {51},
	issn = {1467-8535},
	shorttitle = {Multimodal data capabilities for learning},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/bjet.12993},
	doi = {10.1111/bjet.12993},
	abstract = {Most research on learning technology uses clickstreams and questionnaires as their primary source of quantitative data. This study presents the outcomes of a systematic literature review of empirical evidence on the capabilities of multimodal data (MMD) for human learning. This paper provides an overview of what and how MMD have been used to inform learning and in what contexts. A search resulted in 42 papers that were included in the analysis. The results of the review depict the capabilities of MMD for learning and the ongoing advances and implications that emerge from the employment of MMD to capture and improve learning. In particular, we identified the six main objectives (ie, behavioral trajectories, learning outcome, learning-task performance, teacher support, engagement and student feedback) that the MMLA research has been focusing on. We also summarize the implications derived from the reviewed articles and frame them within six thematic areas. Finally, this review stresses that future research should consider developing a framework that would enable MMD capacities to be aligned with the research and learning design (LD). These MMD capacities could also be utilized on furthering theory and practice. Our findings set a baseline to support the adoption and democratization of MMD within future learning technology research and development. Practitioner Notes What is already known about this topic Capturing and measuring learners’ engagement and behavior using MMD has been explored in recent years and exhibits great potential. There are documented challenges and opportunities associated with capturing, processing, analyzing and interpreting MMD to support human learning. MMD can provide insights into predicting learning engagement and performance as well as into supporting the process. What this paper adds Provides a systematic literature review (SLR) of empirical evidence on MMD for human learning. Summarizes the insights MMD can give us about the learning outcomes and process. Identifies challenges and opportunities of MMD to support human learning. Implications for practice and/or policy Learning analytics researchers will be able to use the SLR as a guide for future research. Learning analytics practitioners will be able to use the SLR as a summary of the current state of the field.},
	language = {en},
	number = {5},
	urldate = {2022-06-16},
	journal = {British Journal of Educational Technology},
	author = {Sharma, Kshitij and Giannakos, Michail},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/bjet.12993},
	keywords = {multimodal learning analytics, multimodal(ity), systematic literature review, systematic review},
	pages = {1450--1484},
}

@misc{alwahaby_evidence_2021,
	title = {The evidence of impact and ethical considerations of {Multimodal} {Learning} {Analytics}: {A} {Systematic} {Literature} {Review}},
	shorttitle = {The evidence of impact and ethical considerations of {Multimodal} {Learning} {Analytics}},
	url = {https://edarxiv.org/sd23y/},
	doi = {10.35542/osf.io/sd23y},
	abstract = {There is a growing interest in the research and use of multimodal data in learning analytics. This paper presents a systematic literature review of multimodal learning analytics (MMLA) research to assess i) the available evidence of impact on learning outcomes in real-world contexts and ii) explore the extent to which ethical considerations are addressed. A few recent literature reviews argue for the promising value of multimodal data in learning analytics research. However, our understanding of the challenges associated with MMLA research from real-world teaching and learning environments is limited. To address this gap, this paper provides an overview of the evidence of impact and ethical considerations stemming from an analysis of the relevant MMLA research published in the last decade. The search of the literature resulted in 663 papers, of which 100 were included in the final synthesis. The results show that the evidence of real-world impact on learning outcomes is weak, and ethical aspects of MMLA work are rarely addressed. We discuss our results through the lenses of two theoretical frameworks for evidence of impact types and ethical dimensions of MMLA. We conclude that for MMLA to stay relevant and become part of mainstream education, future research should directly address the gaps identified in this review.},
	language = {en-us},
	urldate = {2022-06-16},
	publisher = {EdArXiv},
	author = {Alwahaby, Haifa and Cukurova, Mutlu and Papamitsiou, Zacharoula and Giannakos, Michail},
	month = aug,
	year = {2021},
	keywords = {Education, Ethics, Impact Evaluations, Multimodal Learning Analytics},
}
