{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Clayton Cohn<br>\n",
        "3 Dec 2023<br>\n",
        "OELE Lab<br>\n",
        "Vanderbilt University <br>\n",
        "\n",
        "# <center> Isolate NLP Consensus Results"
      ],
      "metadata": {
        "id": "9PKxx1Tapha2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction and Attribution\n",
        "\n",
        "This notebook was create by Clayton Cohn for the purpose joining the NLP papers with the results from the consensus doc.\n",
        "\n",
        "The MMLTE survey project is a collaborative effor between Dr. Gautam Biswas, Clayton Cohn, Eduardo Davalos, Joyce Fonteles, Dr. Meiyi Ma, Caleb Vatral, and Hanchen (David) Wang."
      ],
      "metadata": {
        "id": "J01H3CtWqLU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import S18_NLP and Consensus"
      ],
      "metadata": {
        "id": "VmE1ww6NdAIZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "42RVMc57cF0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7340fffd-39b6-449d-dc46-6982d97ecb50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Used for readability\n",
        "pd.set_option('max_colwidth', 400)\n",
        "pd.set_option('display.max_columns', 50)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "S18_NLP_PATH = \"drive/MyDrive/Clayton/20230420_MMLTE/S18_NLP.csv\"\n",
        "S18_NLP = pd.read_csv(S18_NLP_PATH)\n",
        "\n",
        "assert len(S18_NLP) == 35\n",
        "S18_NLP.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "id": "DCNW6Rl4dEO0",
        "outputId": "fa06f25b-1fe0-46c3-dcc5-4d1e84ecb445"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1118315889   \n",
              "1  3339002981   \n",
              "2  3093310941   \n",
              "3  3095923626   \n",
              "4    85990093   \n",
              "\n",
              "                                                                                                                     Title  \\\n",
              "0                       using multimodal learning analytics to identify aspects of collaboration in project-based learning   \n",
              "1                          estimation of success in collaborative learning based on multimodal learning analytics features   \n",
              "2  embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders   \n",
              "3                                                                                          a multimodal analysis of making   \n",
              "4                                               multimodal markers of persuasive speech : designing a virtual debate coach   \n",
              "\n",
              "      First Author  Year Environment Type       Data Collection Mediums  \\\n",
              "0    Daniel Spikol  2017         Learning              VIDEO,AUDIO,LOGS   \n",
              "1    Daniel Spikol  2017         Learning          EYE,LOGS,VIDEO,AUDIO   \n",
              "2    Hiroki Tanaka  2017         Training               AUDIO,VIDEO,PPA   \n",
              "3  Marcelo Worsley  2017         Learning  VIDEO,AUDIO,SENSOR,PPA,INTER   \n",
              "4  Volha Petukhova  2017         Training                   VIDEO,AUDIO   \n",
              "\n",
              "                         Modalities       Analysis Methods Fusion Types  \\\n",
              "0                         POSE,PROS                    REG          MID   \n",
              "1               GAZE,LOGS,PROS,POSE                    CLS          MID   \n",
              "2                  POSE,PROS,AFFECT              REG,STATS          MID   \n",
              "3  GEST,PPA,EDA,ACT,PROS,QUAL,INTER  STATS,CLUST,QUAL,PATT        EARLY   \n",
              "4                         PROS,GEST         CLS,QUAL,STATS          MID   \n",
              "\n",
              "   Publication Environment Setting Environment Subject Participant Structure  \\\n",
              "0         CSCL                PHYS                STEM                 MULTI   \n",
              "1        ICALT                VIRT                STEM                 MULTI   \n",
              "2         PLOS                VIRT                 HUM                   IND   \n",
              "3       IJAIED                PHYS                STEM                 MULTI   \n",
              "4  INTERSPEECH                PHYS                 HUM                 MULTI   \n",
              "\n",
              "  Didactic Nature Level of Instruction or Training Analysis Approach  \n",
              "0           INSTR                              UNI                MB  \n",
              "1           INSTR                              UNI                MB  \n",
              "2           TRAIN                         K12, UNI                MF  \n",
              "3             INF                         K12, UNI                MB  \n",
              "4           TRAIN                              K12                MB  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ec87e7fa-78ee-45c3-aa2d-e9148c57c965\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type</th>\n",
              "      <th>Data Collection Mediums</th>\n",
              "      <th>Modalities</th>\n",
              "      <th>Analysis Methods</th>\n",
              "      <th>Fusion Types</th>\n",
              "      <th>Publication</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1118315889</td>\n",
              "      <td>using multimodal learning analytics to identify aspects of collaboration in project-based learning</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS</td>\n",
              "      <td>POSE,PROS</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>CSCL</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3339002981</td>\n",
              "      <td>estimation of success in collaborative learning based on multimodal learning analytics features</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,LOGS,VIDEO,AUDIO</td>\n",
              "      <td>GAZE,LOGS,PROS,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICALT</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3093310941</td>\n",
              "      <td>embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders</td>\n",
              "      <td>Hiroki Tanaka</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>AUDIO,VIDEO,PPA</td>\n",
              "      <td>POSE,PROS,AFFECT</td>\n",
              "      <td>REG,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>PLOS</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3095923626</td>\n",
              "      <td>a multimodal analysis of making</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR,PPA,INTER</td>\n",
              "      <td>GEST,PPA,EDA,ACT,PROS,QUAL,INTER</td>\n",
              "      <td>STATS,CLUST,QUAL,PATT</td>\n",
              "      <td>EARLY</td>\n",
              "      <td>IJAIED</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>85990093</td>\n",
              "      <td>multimodal markers of persuasive speech : designing a virtual debate coach</td>\n",
              "      <td>Volha Petukhova</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>PROS,GEST</td>\n",
              "      <td>CLS,QUAL,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>INTERSPEECH</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec87e7fa-78ee-45c3-aa2d-e9148c57c965')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ec87e7fa-78ee-45c3-aa2d-e9148c57c965 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ec87e7fa-78ee-45c3-aa2d-e9148c57c965');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3a6a1536-943a-49ad-b842-229650fc874a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3a6a1536-943a-49ad-b842-229650fc874a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3a6a1536-943a-49ad-b842-229650fc874a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "CONSENSUS_PATH = \"drive/MyDrive/Clayton/20230420_MMLTE/S18_IRR_and_Consensus.csv\"\n",
        "S18_CONSENSUS = pd.read_csv(CONSENSUS_PATH)\n",
        "\n",
        "assert len(S18_CONSENSUS) == 219\n",
        "S18_CONSENSUS.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GuFPi-2BlcZq",
        "outputId": "8d3ab62e-21b9-47db-90b5-fb356ef3a1be"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         UUID  \\\n",
              "0  1326191931   \n",
              "1  1326191931   \n",
              "2  1326191931   \n",
              "3  1469065963   \n",
              "4  1469065963   \n",
              "\n",
              "                                                                                                             Title  \\\n",
              "0                                                          multimodal learning analytics in a laboratory classroom   \n",
              "1                                                          multimodal learning analytics in a laboratory classroom   \n",
              "2                                                          multimodal learning analytics in a laboratory classroom   \n",
              "3  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "4  examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "\n",
              "     Mapped First Author  Year Environment Type (learning or training)  \\\n",
              "0  Man Ching Esther Chan  2019                                Learning   \n",
              "1  Man Ching Esther Chan  2019                                Learning   \n",
              "2  Man Ching Esther Chan  2019                                Learning   \n",
              "3            Andy Nguyen  2022                                Learning   \n",
              "4            Andy Nguyen  2022                                Learning   \n",
              "\n",
              "  Mapped Data Collection Mediums Mapped Modalities Mapped Analysis Methods  \\\n",
              "0                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "1                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "2                    VIDEO,AUDIO    POSE,GAZE,PROS               CLS,CLUST   \n",
              "3             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "4             VIDEO,AUDIO,SENSOR          QUAL,EDA          PATT,CLS,CLUST   \n",
              "\n",
              "  Mapped Fusion Types Mapped Publication Acronym  \\\n",
              "0                LATE                     MLPALA   \n",
              "1                LATE                     MLPALA   \n",
              "2                LATE                     MLPALA   \n",
              "3              HYBRID                       BJET   \n",
              "4              HYBRID                       BJET   \n",
              "\n",
              "                                      Mapped Full Publication  Sort Number  \\\n",
              "0  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "1  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "2  Machine Learning Paradigms: Advances in Learning Analytics            3   \n",
              "3                   British Journal of Educational Technology            4   \n",
              "4                   British Journal of Educational Technology            4   \n",
              "\n",
              "  Environment Setting Environment Subject Participant Structure  \\\n",
              "0                PHYS                STEM            IND, MULTI   \n",
              "1                PHYS                STEM            IND, MULTI   \n",
              "2                PHYS                STEM            IND, MULTI   \n",
              "3                PHYS                STEM                 MULTI   \n",
              "4                PHYS                STEM                 MULTI   \n",
              "\n",
              "  Didactic Nature Level of Instruction or Training Analysis Approach  \\\n",
              "0           INSTR                             UNSP                MB   \n",
              "1           INSTR                             UNSP                MB   \n",
              "2           INSTR                             UNSP                MB   \n",
              "3           INSTR                              K12                MB   \n",
              "4           INSTR                              K12                MB   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                       Analysis Results (w/ multimodal advantages)  \\\n",
              "0                                                                                                                                                                           The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings   \n",
              "1                                                                                                                                                                  Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                              NaN   \n",
              "3  Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of con...   \n",
              "4                     The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.   \n",
              "\n",
              "  Full-Read 3 by Researcher Reviewer Reviewer Notes  \n",
              "0                     Joyce        1            NaN  \n",
              "1                   Eduardo        2            NaN  \n",
              "2             Joyce/Eduardo      1&2            NaN  \n",
              "3                     Joyce        1            NaN  \n",
              "4                   Eduardo        2            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d9e2b2c9-5d3f-4847-85e8-602ec83c5546\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Mapped First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type (learning or training)</th>\n",
              "      <th>Mapped Data Collection Mediums</th>\n",
              "      <th>Mapped Modalities</th>\n",
              "      <th>Mapped Analysis Methods</th>\n",
              "      <th>Mapped Fusion Types</th>\n",
              "      <th>Mapped Publication Acronym</th>\n",
              "      <th>Mapped Full Publication</th>\n",
              "      <th>Sort Number</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Analysis Results (w/ multimodal advantages)</th>\n",
              "      <th>Full-Read 3 by Researcher</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Reviewer Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; model) to compute student's engagement in individual, pair, and group structures.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>Machine Learning Paradigms: Advances in Learning Analytics</td>\n",
              "      <td>3</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joyce/Eduardo</td>\n",
              "      <td>1&amp;2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of con...</td>\n",
              "      <td>Joyce</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>British Journal of Educational Technology</td>\n",
              "      <td>4</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.</td>\n",
              "      <td>Eduardo</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d9e2b2c9-5d3f-4847-85e8-602ec83c5546')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d9e2b2c9-5d3f-4847-85e8-602ec83c5546 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d9e2b2c9-5d3f-4847-85e8-602ec83c5546');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7ed28653-55bb-4867-a276-7b814fcc7276\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7ed28653-55bb-4867-a276-7b814fcc7276')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7ed28653-55bb-4867-a276-7b814fcc7276 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Results"
      ],
      "metadata": {
        "id": "Gu8oAnWFm5Fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NLP_UUIDS = set(S18_NLP[\"UUID\"])\n",
        "\n",
        "mp = {}\n",
        "for _, row in S18_CONSENSUS.iterrows():\n",
        "  uuid = row[\"UUID\"]\n",
        "  if row[\"UUID\"] in NLP_UUIDS:\n",
        "    if type(row[\"Analysis Results (w/ multimodal advantages)\"])!=float:\n",
        "      if not mp.get(uuid):\n",
        "        mp[uuid] = []\n",
        "      mp[uuid].append(row[\"Analysis Results (w/ multimodal advantages)\"])\n",
        "mp = {k: \"\\n\\n\".join(v) for k, v in mp.items()}\n",
        "mp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W5pIsfJm7yI",
        "outputId": "a8ba207f-59da-4741-e8df-f0ef56233a13"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1326191931: \"The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings\\n\\nWithin heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.\",\n",
              " 2634033325: 'Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoidance of filled pauses)\\n\\nAuthors showcase that the training tool improved manually defined scores between an initial and second use of the tool.',\n",
              " 3051560548: \"Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.\\n\\nEvaluating how multimodal features contribute to a model's performance to predict learning gains. Audio features introduce noise that negatively impacted the error of the model.\",\n",
              " 3339002981: \"Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, authors found that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.\\n\\nPredicting the learning gains via classification (bad, ok, good) through gaze, logs, audio, and dialog. Determined that distance measures between hands and gaze fixations was the key features to predict students' performance.\",\n",
              " 32184286: \"Results demonstrate how NLP and ML techniques allow us to use different modalities of the same data, voice and transcript, and different modalities of different data sources, voice data from interviews, answers to a goal orientation questionnaire, and answers to open-ended questions about energy, in order to better understand individual differences in students’ performances\\n\\nUsed text and audio to predict student's affect. With the affect, the authors' explored its statistical relation to student's knowledge. Results point that they need more data to improve performance in affect prediction but promising direction.\",\n",
              " 1847468084: 'Authors present an initial pilot using these methods in concert to identify key moments in multiple modalities. While the use of constructivist dialogue mapping showed that the users learned during their interaction with Ant Adaption, emotional logging identified alternative moments of learning outside of their analytic framework.\\n\\nThis paper presented a preliminary approach to augment qualitative analysis of an\\ninformal learning environment. Using techniques from multimodal learning analytics,\\nwe were able to expand our analysis of learning while participants interacted with a\\nmultitouch environment. Our methodological approach required us to extract emotions\\nfrom the low-level logs of facial action units using FACET and then revisit video\\ncorresponding to particular FACET values to identify moments of high emotional stimulation theoretically implicated in learning.',\n",
              " 2345021698: '``There was no difference between the proportion of the session spent being bored or frustrated or in the achievement scores for the two conditions. In order to determine whether the effect on achievement was different depending on length of exposure, participants with a duration less than 60 minutes were excluded from the analysis. However, there was still no significant difference between achievement scores from intervention and control sessions for the group as a whole or for each subgroup.``\\n\\nMain findings of the case study are the relationships between prior experience in software requirements and the way the team members collaborate, and the lower productivity of low experienced groups. No evidence was found that performance of domain experts was superior from non-experts during collaborative problem-solving sessions. Although it was stated that low experience subjects produced more user stories, a greater productivity of top experience subjects was not statistically verified.',\n",
              " 3796643912: 'This is the first study to evaluate an adaptive learning system for learners with ID based on multi-modal affect recognition. Three separate states were automatically identified, with lower levels of the state labelled “boredom” having the strongest link to learning achievement. Both those labelled “frustration” and “engagement” were positively related to achievement. Our results are in line with other studies showing that engagement increases when activities are tailored to the personal needs\\nand emotional states of learners (Athanasiadis et al., 2017), but no significant difference in learn-ing achievement was found (at least for the period of our study) when adaption was based on both the affective state and achievement of the learner, compared with achievement alone.\\n\\nResults suggest that engagement does increase when activities are tailored to the personal needs and emotional state of the learner and that the system was promoting affective states that in turn promote learning. However, longer exposure is necessary to determine the effect on learning.',\n",
              " 2181637610: 'While this study was not able to show a clear effect of providing a real-time visualization to support\\ncollaboration, it made many other valuable contributions. First, it showed that simple verbal interventions can\\nhelp participants pay attention to particular aspects of their collaborative behavior (i.e., how much they are\\ntalking and how much space they are providing to their partner). Second, it suggested that awareness tools such\\nas the one developed for this study have to be designed differently to impact social interactions (e.g., by being\\nmore salient or be used in a setting where users have the mental bandwidth to reflect on their collaborative\\nstyle). Third, we collected a rich multi-modal dataset that can be used to build proxies for measuring effective\\ncollaborations.\\n\\nThe purpose of this paper was to explore the effect of two collaboration interventions and the relationship between collaboration quality, task performance and learning gains, however this study was not able to show a clear effect of providing a real-time visualization to support collaboration. It did show that simple verbal interventions can help participants pay attention to particular aspects of their collaborative behavior, and suggested that awareness tools such as the one developed for this study have to be designed differently to impact social interactions. Authors built a rich multi-modal dataset that can be used to build proxies for measuring effective collaborations. As a preliminary analysis, they found that various indicators captured by the Kinect sensor were correlated with participants’ quality of collaboration.',\n",
              " 3146393211: 'Finally, it was clear from the two intervention studies that the simulation provided by mobile devices, although not as powerful as the desktop-computer-powered Oculus Rift mixed reality or physical simulators, has no negative impacts on the learners. In fact, the simulation improves learners’ competence and skills with the skills intervention showing statistically significant improvements in the learners that received the mobile mixed reality simulation tools prior to residential school.\\n\\nThis study validates the use of mobile devices in university undergraduate health sciences curricula, and shows that not only are these modes (game engines, free AR/VR SDKs and mobile-based devices with GPU-enabled processors and high-quality screens) useful for enhancing the development of physical skills in students, but they are also received favorably.',\n",
              " 4019205162: 'RQ1: Better communication, better collaboration\\nRQ2: Collaborative teams showed lower variability in the estimates of story points (same page)\\nRQ3: Democratic leadership in collaborative groups\\n\\nThe results obtained allowed the detection of important relationships and characteristics of the collaborative and Non-Collaborative groups, with productivity, effort, and predominant personality styles in the groups. Authors conclude that the Multimodal Learning Analysis techniques offer considerable feasibilities to support the process of skills development in students.',\n",
              " 3398902089: 'Our major claim is that multichannel data can be potential for understanding regulatory processes in collaboration. With our five empirical case examples, we illustrate how triangulating multiple sources of data has potential to advance the theoretical and conceptual progress in social aspects of SRL theory.\\n\\nAuthors show with five empirical cases that multichannel data can be potential for understanding regulatory processes in collaboration, illustrating how triangulating multiple sources of data has potential to advance the theoretical and conceptual progress in social aspects of SRL theory: (1) understanding how interactions between different facets of regulation, such as cognition, motivation and emotion interact with cognitive strategic action by using video and EDA data; (2) visualizing how physiological synchrony measured from the heart rate can reveal or backup the interpretation of socially shared regulation of learning or co-regulation of learning located from the video; (3) visualizing temporality and cyclical processes (i.e., planning, enacting strategies, reflecting, adapting) of regulation by using video, EDA and facial expression recognition data; (5)) illustrating how combining not only physiological measures, but also facial expression data can lead even more accurate interpretations of the situations where regulation of learning is needed.',\n",
              " 3093310941: 'We identified a positive correlation between overall narrative skills and other non-verbal behaviors. This result indicates that the overall narrative skills were subjectively decided based on other non-verbal behavior skills [22]. Although the automated social skills trainer did not provide feedback regarding narrative structure or eye gaze, these elements were also improved after the training.\\n\\nThe focus of this study assessed the effectiveness of an automated social skills trainer with multimodal information that adheres to the basic human-based SST as closely as possible. Authors extended a previous method for automatic social skills training by adding audiovisual information regarding smiling ratio and head pose that improved the training effect. \\nMultimodal feedback is also useful for both members of the general population with social difficulties and people with ASD because it helps such people understand and improve their narrative skills, as was previously reported in human-based SST [2, 3].',\n",
              " 1576545447: 'In this study, we used non-transparent prediction models of openSMILE to predict the emotional traits of tutor candidates based on their audio data, and we used transparent logistic regression models to identify exactly what personality, emotion, and experience traits lead to effective debate tutoring skills. Predictive models were very powerful to make sense of complex and nonlinear audio data, whereas the transparent regression models were valuable to identify key aspects for tutors to reflect upon their own decisions and provide tutor candidates with feedback on their performance.\\n\\nAuthors combined predictive and transparent models to support the human decision-making processes involved in tutor trainee evaluations and results showed that models with multimodal data can accurately classify tutors and have the potential to support the intuitive decision-making of expert tutors in the context of evaluating trainee applicants.',\n",
              " 3796180663: \"We presented two empirical studies, collected in classroom studies with two distinct learning technology systems in different contexts (individual and collaborative). Our analyses and findings showcase a few different ways, in which multimodal data sources can enrich our understanding of student learning and provide a more holistic picture.\\n\\nAuthors collected student‐focused screen and webcam video which were useful for understanding students' learning processes and approaches based on detailed analyses of their interactions with the tutor interface, mouse movements, and out‐of‐tutor (in person) help‐seeking. High‐fidelity audio of students' collaborative dialogue was collected to generate high‐quality transcriptions of students' dialogue and apply an NLP approach to make use of the large quantity of audio dialogue. The verbal data allowed authors to identify linguistic features in students' collaborative dialogue that were highly predictive of math performance on pretest and posttest assessments, above and beyond any nonlinguistic variables.\",\n",
              " 1770989706: 'We found that idling with limited speech (i.e., silence or backchannel feedback only) and without movement was negatively correlated with task performance and with participants’ subjective perceptions of the collaboration. However, being silent and focused during solution execution was positively correlated with task performance. Results illustrate that in some cases, multimodal patterns improved the predictions and improved explanatory power over the unimodal primitives.\\n\\n\"Here, we examine unimodal primitives (activity on the screen, speech, and body movements), and their multimodal combinations during remote CPS.\"\\n\\nMixed findings for uni- versus multi-modal:\\n\\n\"These results lead us to question: are the multimodal patterns better than the unimodal primitives? As illustrated above, we found evidence for both sides of the argument. In the case of code execution, the answer is no, but it is a yes in the case of idling. However, it is important to go beyond the significant correlations as there is an informative signal in the non-significant ones as well. For example, consider idling once again. By itself, this pattern is negatively correlated with the task score (r = -.21) and the correlation is even more negative when idling is accompanied by silence/back channeling and little movement (r = -.35). However, there are many other configurations where idling is weak or negligible predictor of task score. For example, idling occurring in the context of the contributors speaking with some movement is more weakly correlated with task score (r = -.11) and the correlation is essentially null when idling is accompanied with the controller speaking and some movement (r = -.06). Thus, even when they do not improve predictive power, multimodal patterns help contextualize and reveal nuances in the unimodal primitives. This supports the overall idea of multimodal learning analytics in which the additional modalities (speech and body movement in our case) help to understand unclear patterns such as idling. This finding is interesting from two perspectives.\"',\n",
              " 518268671: 'Overall, our study suggests that coupling style can be operationalised as a multimodal indicator suitable to investigate complex collaboration dynamics in games, thus confirming what has been found in other domains\\n\\n\"Firstly, we found that coupling styles can be used to characterise collaboration in a co-located SG. Consistently with previous research (Isenberg et al, 2010; Niu et al, 2018), our results showed that teams displayed both close and loose coupling styles while performing individual actions to accomplish shared goals. Interestingly, we found a positive association between the time spent working closely coupled and the individual interactions with the technology. This suggests that the pursuit of collective goals requires players to continuously alternate and integrate individual planning and action with closely-coupled, likely to verify and synchronise their own actions with others. Secondly, we found that the perceived quality of collaboration does not appear to be an effective indicator of collaboration quality by itself. However, its small association with close-coupling style suggests a conscious, continuous, and proactive approach to collaboration, since players who appreciate the value of collaboration also seem to actively engage in closely-coupled interactions with others. Thirdly, our findings suggest that better-performing teams do work more closely-coupled and alternate their interactions with individual work. This result indicates that freely alternating individual work with closely-coupled interaction is an effective collaboration strategy, and that collaborative SGs should afford this opportunity. Overall, our study suggests that coupling style can be operationalised as a multimodal indicator suitable to investigate complex collaboration dynamics in games, thus confirming what has been found in other domains (Isenberg et al, 2010; Niu et al, 2018).\"',\n",
              " 957160695: '\"We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain corpora and classification procedures resulted in the best performance on an argument structure mining task.\"\\n\\n\"We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain corpora and classification procedures resulted in the best performance on an argument structure mining task. Results of the argument quality experiments showed that argument com- prehensibility is affected by the number of referring expressions, information complexity, and presentation fluency. Presence of intensification and segmentation markers, position and movements of hands/ams and certain postures may affect the perception of the clarity, persuasiveness, and credibility of debaters.\"',\n",
              " 3448122334: \"Significant impact on the teacher's scaffolding behavior and student engagement (less bordem)\\n\\nSEAT had positive impact on student engagement and was also helpful to teachers.\",\n",
              " 2497456347: 'Feedback generated by RAP is similar to human feedback across several dimensions and similar to other more complex systems\\n\\n\"It revealed an overwhelmingly positive perception of the system especially in the dimensions of perceived usefulness and feedback which were rated as excellent by 65% and 58% of the students respectively. The qualitative analysis helped discover specific issues, on the positive side, students commented on the potential of the system to quickly learn some basic presentation skills: \"I would like to see this system used in our Communications class\". On the negative side, students commented that they sometimes were aware that they were being recorded and that the environment was too small. Also, some students felt uncomfortable with a pre-recorded audience because it didn’t seem to react to their presentation: \"the audience had always the same expressions\". Overall, the students agreed that the system was useful and that they learned about their own presentation skills while using it.\"',\n",
              " 3783339081: 'Corse-grained learning trajectories were able to identify key moments during learning that warrented futher analysis with other MMD\\n\\n\"Our results from this analysis showed how students’ early experiences struggling with a novel concept could significantly affect both their entire learning trajectories within an activity and pre-test–post-test measurements of learning gains related to that concept. It provided evidence of an important “moment” for early instructional intervention.\\n\\nThe analysis we conducted on the Concentration knowledge component was an example of how a knowledge component-centred analysis can benefit from this multi-step approach as well. Our results led to a modification in the knowledge component assignment to problem steps within a ChemVLab+ activity as well as instructional implications for promoting better learning of a previously hidden conceptual difficulty.\"',\n",
              " 3095923626: '\"each approach provides different affordances depending on the similarity metric and the dependent variable.\"\\n \"The take-away is that there are several strategies for doing multimodal learning analytics, and that many of these approaches can provide a meaningful glimpse into a complex data set, glimpses that may be difficult to identify using traditional approaches.\"\\n\\n\"Looking across analyses, there are clear instances where each provided some novel insights. In this sense, the overall algorithm appears to have relevance for studying learning, success and experimental condition; but honing in on these correlations requires different modes of analysis.\\n\\nAs a whole this article has shown that success, learning and process are not equivalent, though they may occasionally overlap. Thus, when thinking about measuring the effectiveness of a given learning environment it is important to be clear about which metrics one hopes to optimize. At the same time, this article has provided additional evidence that experimental condition can have an impact on learning, success and process. Because of this, one has to be cognizant about how to develop learning and reasoning approaches that allow the environment to realize the desired outcomes.\"',\n",
              " 85990093: '\"Experiments of different types supported fairly reliable identification of markers from multimodal data, and linking these to assessments of debater confidence level and intensification behaviour.\"\\n\\n\"In line with previous empirical findings, we acknowledge that persuasive speech is rather difficult to characterize. Neverthe- less, based on theoretical and empirical frameworks set up by Grice (1975), Gussenhoven (2002) and Hirschberg (2002), we were able to define a set of criteria which help us to explain observed regularities and define rules, strategies and constraints for the generation, assessment and correction of trainees’ debate performance. Experiments of different types supported fairly reliable identification of markers from multimodal data, and linking these to assessments of debater confidence level and intensification behaviour.\"',\n",
              " 86191824: '\"This study contributes an initial understanding into how different modalities mediate students’ interactions and offers implications for scaffolding peer interactions during multimodal composing processes.\"\\n\\n\"Overall, students were more likely to share ideas and ask questions, and they tended to use quick- response strategies while multimodal composing.\"\\n\\n\"Students’ interview responses also suggested that providing short responses was a typical strategy during multimodal composing.\"\\n\\n\"When examining interactions across sessions, the group was more engaged in discussions at the beginning and the end of the project while fewer interactions occurred during the middle of their composing process.\"\\n\\n\"Giving commands occurred much less frequently than other interaction types (Figure 3).\"\\n\\n\"Students discussed more often about comics that combined visuals and text than other modal elements.\"\\n\\n\"Making learning visible in different modes was critical to foster peer interaction (Jahnke, Norqvist, &\\nOlsson, 2013).\"\\n\\n\"Results showed that there were interactional differences based on different modes.\"\\n\\n\"While comparing discussions on static visual modes, namely images and multimodal comics, we found that images involved more self-oriented and less group-oriented contributions.\"\\n\\n\"Discussions on animations included more elaborated feedback.\"\\n\\n\"Written narrative provided the least opportunity for group-oriented contributions.\"',\n",
              " 1118315889: '\"physical aspect of collaborative is an important part of this type of learning and that learning analytics systems can identify features that are relevant for helping researchers, teachers, and learners unpack what is happening.\"\\n\\n\"In this research study, we presented that where the students are looking, the distance between them, the motion of their hands our key features for a learning analytics system to be effectively used to identify collaboration in small groups of Engineering students.\"',\n",
              " 1426267857: '\"this research contributes to the relatively sparse literature in multimodal learning analytics by providing a balanced view of the teacher and student interactions with a data set collected in naturalistic home environments.\"\\n\\n\"Firstly, we note a clear causal pathway between the group of variables describing parents’ support and another group representing the child’s cognitive-affective experience.\"\\n\\n\"Secondly, we note the causal pathway from Profile to Affect and, indirectly, to Support.\"',\n",
              " 2273914836: '\"Our combined multi-modal learning analytics and interaction analysis methodology enabled us to identify two multi-modal profles of learners who have learning gains and one multi-modal profle of learners who do not have learning gains.\"\\n\\n\"Using this approach, we are able to build the multimodal behavioral profles for each group of learners.\"\\n\\n\"Our combined multi-modal learning analytics and interaction analysis methodology enabled us to identify two multi-modal profles of learners who have learning gains and one multi-modal profle of learners who do not have learning gains.\"',\n",
              " 1345598079: '\"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Exploration, increasing confluence during Discovery, and high confluence during Fluency.\"\\n\\n\"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Exploration, increasing confluence during Discovery, and high confluence during Fluency. Towards the end of the Discovery stage, a coordination of coordinations (Piaget, 1970) developed wherein the coordination between the left- and right hands became coordinated with newly developed gaze structures spanning different screen locations.\"\\n\\n\"Our findings point to the importance of MMLA work that attunes to intermodal dynamics of learning, both as a pragmatic resource for identifying key moments in learning and as a resource for refining theoretical understandings of learning processes.\"',\n",
              " 3135645357: '\"In summary, the results from our evaluation of personalized and general models to automatically extract orchestration graphs highlight the fact that machine learning models can be successfully trained with such multimodal sensor data, using relatively low-level features.\"\\n\\n\"In summary, the results from our evaluation of personalized and general models to automatically extract orchestration graphs highlight the fact that machine learning models can be successfully trained with such multimodal sensor data, using relatively low-level features.\"',\n",
              " 2609260641: '\"We conducted a case study to compare the visualizations provided by the system in two different situations: collaborative and competitive activities. The results suggest that the provided visualizations help to identify issues on cognitive contribution, assimilation, self-regulation, and integration of the team members. They could also support teachers to decide whether they must assist a team in fostering collaboration.\\n\\nWhile the results are naturally constrained to the characteristics of the activities in which we tested the platform, they provide initial evidence about the technical fea-\\nsibility of extracting behavioral indicators and traces using MMLA to give insights onteam collaboration.\"\\n\\n\"The results suggest that the provided visualizations help to identify issues on cognitive contribution, assimilation, self-regulation, and integration of the team members. They could also support teachers to decide whether they must assist a team in fostering collaboration.\"',\n",
              " 666050348: '\"Through our user studies, we found that the platform helps fulfill some of those goals by providing capabilities that can spur on amazement and excitement among traditional Minecraft users and novices. We also find that many of the multimodal components, while not immediately intuitive for users, proved to be preferred modes of game play. In this sense, we feel that this tool is moving in the right direction in terms of the system capabilities that it provides. Our analyses also point to the meaningful ways that multimodal data can be used to study student learning in these game-based environments, and free students from standardized testing and learning experiences.\"\\n\\n\"Through our user studies, we found that the platform helps fulfill some of those goals by providing capabilities that can spur on amazement and excitement among traditional Minecraft users and novices. We also find that many of the multimodal components, while not immediately intuitive for users, proved to be preferred modes of game play.\"',\n",
              " 1637690235: '\"In this paper, we show that MMLA and the state-of-the-art computational techniques can be used to generate insights into the \"black box\" of learning in students’ project-based activities. These insights generated from multimodal data can be used to inform teachers about the key features of project-based learning and help them support students appropriately in similar pedagogical approaches.\"\\n\\n\"In this paper, we show that MMLA and the state-of-the-art computational techniques can be used to generate insights into the \"black box\" of learning in students’ project-based activities. These insights generated from multimodal data can be used to inform teachers about the key features of project-based learning and help them support students appropriately in similar pedagogical approaches. Towards achieving this ultimate aim, this paper has three main contributions to the field. First, we show that the distances between students’ hands and faces while they are working on projects is a strong predictor of students’ artefact quality which indicates the value of student collaboration in these pedagogical approaches. Second, we show that both, new and promising approaches such as neural networks and more traditional regression approaches, can be used to classify MMLA data and both have advantages and disadvantages depending on the research questions and contexts being investigated. At last but not least, although, it is traditionally notoriously challenging to provide evidence about the robust and objective evaluations of project-based learning activities, techniques and types of data we presented here can be the first step towards effective implementation and evaluation of project-based learning at a scale.\"',\n",
              " 2155422499: '\"In this study, we have given special emphasis to the interpersonal metafunction (see Fig. 1), which is crucial to enhance EMI students’ active participation and engagement. The outcomes of this study point to the need to address multimodal interactional competence in professional development programs, as described in Morell et al. (2022) to promote effective lecturing in the diverse EMI scenarios.\"\\n\\n\"The exploration of how EMI lecturers use semiotic resources to construct meaning and to create engagement paves the way to a unified multimodal interactional competence. In general, the mastery of this competence enables lecturers to convert students from passive listeners/observers to active participants, giving them opportunities to engage in active learning, language usage and critical thinking.\"',\n",
              " 3754172825: '\"We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements in the video modality are the most significant unimodal indicators of impasse.\"\\n\\n\"We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements in the video modality are the most significant unimodal indicators of impasse. We also trained several multimodal models and found that combining indicators from these three modalities provided the best impasse detection performance.\"',\n",
              " 1296637108: '\"We illustrate the approach from the automated analysis of healthcare simulations to train nurses, generating four visual proxies that fuse multimodal data into higher order patterns.\"\\n\\n\"we introduce the concept of collaboration translucence, an approach to make visible selected features of group activity. This is grounded both theoretically (in the physical, epistemic, social and affective dimensions of group activity), and contextually (using domain-specific concepts). We illustrate the approach from the automated analysis of healthcare simulations to train nurses, generating four visual proxies that fuse multimodal data into higher order patterns.\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify extraction\n",
        "\n",
        "assert set(mp.keys()) == NLP_UUIDS\n",
        "for uuid in NLP_UUIDS:\n",
        "  assert uuid in mp"
      ],
      "metadata": {
        "id": "RO8OnvGMplgc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_arr = []\n",
        "for _,row in S18_NLP.iterrows():\n",
        "  uuid = row[\"UUID\"]\n",
        "  results_arr.append(mp[uuid])"
      ],
      "metadata": {
        "id": "74quZgDllDCc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S18_NLP[\"Results\"] = results_arr\n",
        "S18_NLP.head(50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NyUyT_1UlDE5",
        "outputId": "246f0724-3b2b-4eff-a633-d22a83f9ab82"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          UUID  \\\n",
              "0   1118315889   \n",
              "1   3339002981   \n",
              "2   3093310941   \n",
              "3   3095923626   \n",
              "4     85990093   \n",
              "5    957160695   \n",
              "6   1637690235   \n",
              "7   2181637610   \n",
              "8   3146393211   \n",
              "9   3135645357   \n",
              "10  3783339081   \n",
              "11  3796180663   \n",
              "12  2345021698   \n",
              "13  2497456347   \n",
              "14  4019205162   \n",
              "15  1847468084   \n",
              "16  1326191931   \n",
              "17  1576545447   \n",
              "18  3398902089   \n",
              "19    86191824   \n",
              "20  3448122334   \n",
              "21  1296637108   \n",
              "22  1770989706   \n",
              "23  3051560548   \n",
              "24  3796643912   \n",
              "25  2634033325   \n",
              "26  1426267857   \n",
              "27   666050348   \n",
              "28   518268671   \n",
              "29  2273914836   \n",
              "30    32184286   \n",
              "31  2609260641   \n",
              "32  1345598079   \n",
              "33  2155422499   \n",
              "34  3754172825   \n",
              "\n",
              "                                                                                                                                       Title  \\\n",
              "0                                         using multimodal learning analytics to identify aspects of collaboration in project-based learning   \n",
              "1                                            estimation of success in collaborative learning based on multimodal learning analytics features   \n",
              "2                    embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders   \n",
              "3                                                                                                            a multimodal analysis of making   \n",
              "4                                                                 multimodal markers of persuasive speech : designing a virtual debate coach   \n",
              "5                                                                virtual debate coach design: assessing multimodal argumentation performance   \n",
              "6                              supervised machine learning in multimodal learning analytics for estimating success in project-based learning   \n",
              "7                                       toward using multi-modal learning analytics to support and measure collaboration in co-located dyads   \n",
              "8                                     mobile mixed reality for experiential learning and simulation in medical and health sciences education   \n",
              "9                                      multimodal teaching analytics: automated extraction of orchestration graphs from wearable sensor data   \n",
              "10                      a novel method for the in-depth multimodal analysis of student learning trajectories in intelligent tutoring systems   \n",
              "11                                                         learning linkages: integrating data streams of multiple modalities and timescales   \n",
              "12         exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course   \n",
              "13                             the rap system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors   \n",
              "14  introducing low-cost sensors into the classroom settings: improving the assessment in agile practices with multimodal learning analytics   \n",
              "15                                                      computationally augmented ethnography: emotion tracking and learning in museum games   \n",
              "16                                                                                   multimodal learning analytics in a laboratory classroom   \n",
              "17                      artificial intelligence and multimodal data in the service of human decision-making: a case study in debate tutoring   \n",
              "18                                                what multimodal data can tell us about the students’ regulation of their learning process?   \n",
              "19                 examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes   \n",
              "20                       investigating the impact of a real-time, multimodal student engagement analytics technology in authentic classrooms   \n",
              "21                                                               towards collaboration translucence: giving meaning to multimodal group data   \n",
              "22                                focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving   \n",
              "23                                                           temporal analysis of multimodal data to predict collaborative learning outcomes   \n",
              "24           an evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities   \n",
              "25                               controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "26                                                    affect, support, and personal factors: multimodal causal models of one-on-one coaching   \n",
              "27                                                      multicraft: a multimodal interface for supporting and studying learning in minecraft   \n",
              "28                                 using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game   \n",
              "29             many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities   \n",
              "30                                                                         once more with feeling: emotions in multimodal learning analytics   \n",
              "31                              visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication   \n",
              "32     intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning   \n",
              "33                                            a multimodal analysis of pair work engagement episodes: implications for emi lecturer training   \n",
              "34                                                 detecting impasse during collaborative problem solving with multimodal learning analytics   \n",
              "\n",
              "             First Author  Year Environment Type  \\\n",
              "0           Daniel Spikol  2017         Learning   \n",
              "1           Daniel Spikol  2017         Learning   \n",
              "2           Hiroki Tanaka  2017         Training   \n",
              "3         Marcelo Worsley  2017         Learning   \n",
              "4         Volha Petukhova  2017         Training   \n",
              "5         Volha Petukhova  2017         Training   \n",
              "6           Daniel Spikol  2018         Learning   \n",
              "7           Emma L. Starr  2018         Learning   \n",
              "8              James Birt  2018         Learning   \n",
              "9          Luis P. Prieto  2018         Learning   \n",
              "10                Ran Liu  2018         Learning   \n",
              "11                Ran Liu  2018         Learning   \n",
              "12              René Noël  2018         Learning   \n",
              "13           Xavier Ochoa  2018         Training   \n",
              "14   Hector Cornide-Reyes  2019         Learning   \n",
              "15             Kit Martin  2019         Learning   \n",
              "16  Man Ching Esther Chan  2019         Learning   \n",
              "17         Mutlu Cukurova  2019         Learning   \n",
              "18          Sanna Järvelä  2019         Learning   \n",
              "19           Shiyan Jiang  2019         Learning   \n",
              "20            Sinem Aslan  2019         Learning   \n",
              "21     Vanessa Echeverria  2019         Training   \n",
              "22          Hana Vrzakova  2020         Learning   \n",
              "23      Jennifer K. Olsen  2020         Learning   \n",
              "24    Penelope J. Standen  2020         Learning   \n",
              "25           Xavier Ochoa  2020         Training   \n",
              "26       Lujie Karen Chen  2021         Learning   \n",
              "27        Marcelo Worsley  2021         Learning   \n",
              "28     María Ximena López  2021         Learning   \n",
              "29        Jauwairia Nasir  2022         Learning   \n",
              "30          Marcus Kubsch  2022         Learning   \n",
              "31              René Noël  2022         Learning   \n",
              "32         Sofia Tancredi  2022         Learning   \n",
              "33          Teresa Morell  2022         Training   \n",
              "34              Yingbo Ma  2022         Learning   \n",
              "\n",
              "                          Data Collection Mediums  \\\n",
              "0                                VIDEO,AUDIO,LOGS   \n",
              "1                            EYE,LOGS,VIDEO,AUDIO   \n",
              "2                                 AUDIO,VIDEO,PPA   \n",
              "3                    VIDEO,AUDIO,SENSOR,PPA,INTER   \n",
              "4                                     VIDEO,AUDIO   \n",
              "5                                     VIDEO,AUDIO   \n",
              "6                        VIDEO,AUDIO,LOGS,PPA,RPA   \n",
              "7                        VIDEO,AUDIO,PPA,RPA,LOGS   \n",
              "8                                       PPA,INTER   \n",
              "9                          EYE,VIDEO,AUDIO,MOTION   \n",
              "10                          LOGS,AUDIO,SCREEN,PPA   \n",
              "11                    VIDEO,AUDIO,LOGS,SCREEN,PPA   \n",
              "12                                  AUDIO,PPA,RPA   \n",
              "13                         AUDIO,VIDEO,PPA,SURVEY   \n",
              "14                           AUDIO,SURVEY,PPA,RPA   \n",
              "15                            VIDEO,AUDIO,PPA,RPA   \n",
              "16                                    VIDEO,AUDIO   \n",
              "17                                   AUDIO,SURVEY   \n",
              "18                             SENSOR,VIDEO,AUDIO   \n",
              "19                         SCREEN,INTER,PPA,AUDIO   \n",
              "20   VIDEO,LOGS,SCREEN,AUDIO,SURVEY,RPA,PPA,INTER   \n",
              "21           VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER   \n",
              "22                  AUDIO,VIDEO,SCREEN,SURVEY,PPA   \n",
              "23                                 LOGS,AUDIO,EYE   \n",
              "24                           VIDEO,AUDIO,LOGS,RPA   \n",
              "25                                VIDEO,AUDIO,PPA   \n",
              "26                             AUDIO,VIDEO,SURVEY   \n",
              "27  AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS   \n",
              "28                        SURVEY,LOGS,AUDIO,VIDEO   \n",
              "29                    VIDEO,AUDIO,LOGS,PPA,SURVEY   \n",
              "30                               SURVEY,PPA,AUDIO   \n",
              "31                          AUDIO,VIDEO,RPA,INTER   \n",
              "32                          EYE,VIDEO,AUDIO,INTER   \n",
              "33                                VIDEO,AUDIO,PPA   \n",
              "34                                    VIDEO,AUDIO   \n",
              "\n",
              "                                Modalities         Analysis Methods  \\\n",
              "0                                POSE,PROS                      REG   \n",
              "1                      GAZE,LOGS,PROS,POSE                      CLS   \n",
              "2                         POSE,PROS,AFFECT                REG,STATS   \n",
              "3         GEST,PPA,EDA,ACT,PROS,QUAL,INTER    STATS,CLUST,QUAL,PATT   \n",
              "4                                PROS,GEST           CLS,QUAL,STATS   \n",
              "5              GEST,TRANS,PROS,SURVEY,GAZE           STATS,CLS,QUAL   \n",
              "6              POSE,GEST,PROS,LOGS,PPA,RPA                  REG,CLS   \n",
              "7                   POSE,PROS,PPA,RPA,LOGS               STATS,QUAL   \n",
              "8                                PPA,TRANS               QUAL,STATS   \n",
              "9                      GAZE,PROS,ACT,PIXEL  NET,CLS,STATS,PATT,QUAL   \n",
              "10                 LOGS,TRANS,ACT,QUAL,PPA           STATS,REG,QUAL   \n",
              "11                          TRANS,QUAL,PPA                CLS,STATS   \n",
              "12                                RPA,PROS           QUAL,NET,STATS   \n",
              "13                      PPA,GAZE,POSE,PROS           CLS,STATS,QUAL   \n",
              "14                    SURVEY,TRANS,PPA,RPA                NET,STATS   \n",
              "15                       TRANS,AFFECT,QUAL                     QUAL   \n",
              "16                          POSE,GAZE,PROS                CLS,CLUST   \n",
              "17                             AFFECT,LOGS                      CLS   \n",
              "18                         EDA,AFFECT,QUAL                     QUAL   \n",
              "19                        INTER,QUAL,TRANS                     QUAL   \n",
              "20  AFFECT,LOGS,POSE,QUAL,INTER,SURVEY,RPA           QUAL,STATS,CLS   \n",
              "21      POSE,LOGS,TRANS,EDA,ACT,PROS,INTER                     QUAL   \n",
              "22                       PROS,ACT,GEST,PPA               STATS,PATT   \n",
              "23               GAZE,LOGS,PROS,TRANS,QUAL                      REG   \n",
              "24     AFFECT,POSE,LOGS,RPA,GAZE,PROS,GEST                CLS,STATS   \n",
              "25                           POSE,PROS,PPA                    STATS   \n",
              "26           PROS,GAZE,TRANS,AFFECT,SURVEY                STATS,NET   \n",
              "27  PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS                     QUAL   \n",
              "28                   LOGS,SURVEY,GAZE,PROS                    STATS   \n",
              "29             PROS,AFFECT,GAZE,TRANS,LOGS     STATS,QUAL,CLUST,CLS   \n",
              "30          INTER,SURVEY,TRANS,PROS,AFFECT            CLS,REG,STATS   \n",
              "31                PROS,POSE,RPA,INTER,QUAL                     QUAL   \n",
              "32              GAZE,GEST,TRANS,POSE,INTER          PATT,QUAL,STATS   \n",
              "33                 TRANS,PPA,QUAL,POSE,ACT                     QUAL   \n",
              "34              TRANS,PROS,SPECT,GAZE,POSE                      CLS   \n",
              "\n",
              "   Fusion Types    Publication Environment Setting Environment Subject  \\\n",
              "0           MID           CSCL                PHYS                STEM   \n",
              "1           MID          ICALT                VIRT                STEM   \n",
              "2           MID           PLOS                VIRT                 HUM   \n",
              "3         EARLY         IJAIED                PHYS                STEM   \n",
              "4           MID    INTERSPEECH                PHYS                 HUM   \n",
              "5           MID           ICMI                PHYS                 HUM   \n",
              "6           MID           JCAL                BLND                STEM   \n",
              "7           OTH           ICLS                BLND                STEM   \n",
              "8           OTH    Information                BLND                STEM   \n",
              "9        HYBRID           JCAL                PHYS                STEM   \n",
              "10          MID            JLA                VIRT                STEM   \n",
              "11          MID           JCAL                VIRT                STEM   \n",
              "12          OTH         Access                PHYS                STEM   \n",
              "13         LATE            LAK                BLND                 HUM   \n",
              "14      MID,OTH        Sensors                PHYS                STEM   \n",
              "15          OTH           ICQE                VIRT                STEM   \n",
              "16         LATE         MLPALA                PHYS                STEM   \n",
              "17          MID           BJET                UNSP                 HUM   \n",
              "18          OTH            LAI                BLND                STEM   \n",
              "19          OTH            ILE                VIRT                STEM   \n",
              "20         LATE            CHI                VIRT                STEM   \n",
              "21          OTH            CHI                PHYS                STEM   \n",
              "22          MID            LAK                VIRT                STEM   \n",
              "23          MID           BJET                VIRT                STEM   \n",
              "24       HYBRID           BJET                VIRT      HUM, OTH, STEM   \n",
              "25          OTH           BJET                BLND                 HUM   \n",
              "26       HYBRID           JEDM                PHYS                STEM   \n",
              "27          OTH           HCII                VIRT                STEM   \n",
              "28          OTH          ECGBL                BLND                STEM   \n",
              "29       HYBRID         IJCSCL                BLND                STEM   \n",
              "30          OTH  MMLA Handbook                PHYS                STEM   \n",
              "31          OTH          DAMLE                PHYS                 HUM   \n",
              "32          OTH  MMLA Handbook                VIRT                STEM   \n",
              "33          OTH           JEAP                PHYS                 OTH   \n",
              "34       HYBRID            LAK                VIRT                STEM   \n",
              "\n",
              "   Participant Structure Didactic Nature Level of Instruction or Training  \\\n",
              "0                  MULTI           INSTR                              UNI   \n",
              "1                  MULTI           INSTR                              UNI   \n",
              "2                    IND           TRAIN                         K12, UNI   \n",
              "3                  MULTI             INF                         K12, UNI   \n",
              "4                  MULTI           TRAIN                              K12   \n",
              "5                  MULTI           TRAIN                              K12   \n",
              "6                  MULTI           INSTR                              UNI   \n",
              "7                  MULTI             INF                              UNI   \n",
              "8                    IND           INSTR                              UNI   \n",
              "9                  MULTI           INSTR                             PROF   \n",
              "10                   IND           INSTR                              K12   \n",
              "11            IND, MULTI           INSTR                              K12   \n",
              "12                 MULTI             INF                              UNI   \n",
              "13                   IND           TRAIN                              UNI   \n",
              "14                 MULTI           INSTR                              UNI   \n",
              "15                 MULTI             INF                             UNSP   \n",
              "16            IND, MULTI           INSTR                             UNSP   \n",
              "17                   IND           TRAIN                             UNSP   \n",
              "18                 MULTI           INSTR                              K12   \n",
              "19                 MULTI           INSTR                              K12   \n",
              "20                   IND           INSTR                              K12   \n",
              "21                 MULTI           TRAIN                              UNI   \n",
              "22                 MULTI           INSTR                              UNI   \n",
              "23                 MULTI           INSTR                              K12   \n",
              "24                   IND           INSTR                              K12   \n",
              "25                   IND           TRAIN                             UNSP   \n",
              "26                   IND           INSTR                              K12   \n",
              "27                   IND             INF                              K12   \n",
              "28                 MULTI           INSTR                              UNI   \n",
              "29                 MULTI           INSTR                              K12   \n",
              "30                   IND           INSTR                              K12   \n",
              "31                 MULTI             INF                        PROF, UNI   \n",
              "32                   IND           INSTR                              K12   \n",
              "33                 MULTI           TRAIN                             PROF   \n",
              "34                 MULTI           INSTR                              K12   \n",
              "\n",
              "   Analysis Approach  \\\n",
              "0                 MB   \n",
              "1                 MB   \n",
              "2                 MF   \n",
              "3                 MB   \n",
              "4                 MB   \n",
              "5                 MB   \n",
              "6                 MB   \n",
              "7                 MF   \n",
              "8                 MF   \n",
              "9                 MB   \n",
              "10                MB   \n",
              "11            MB, MF   \n",
              "12            MB, MF   \n",
              "13                MB   \n",
              "14            MB, MF   \n",
              "15            MB, MF   \n",
              "16                MB   \n",
              "17            MB, MF   \n",
              "18                MF   \n",
              "19                MB   \n",
              "20                MB   \n",
              "21                MB   \n",
              "22                MF   \n",
              "23                MB   \n",
              "24                MB   \n",
              "25                MF   \n",
              "26                MB   \n",
              "27                MF   \n",
              "28                MF   \n",
              "29                MB   \n",
              "30            MB, MF   \n",
              "31                MF   \n",
              "32                MF   \n",
              "33                MF   \n",
              "34                MB   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                            Results  \n",
              "0   \"physical aspect of collaborative is an important part of this type of learning and that learning analytics systems can identify features that are relevant for helping researchers, teachers, and learners unpack what is happening.\"\\n\\n\"In this research study, we presented that where the students are looking, the distance between them, the motion of their hands our key features for a learning an...  \n",
              "1   Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, authors found that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.\\n\\nPredicting the learning gains via classification (bad, ok, good) through gaze, logs, audio...  \n",
              "2   We identified a positive correlation between overall narrative skills and other non-verbal behaviors. This result indicates that the overall narrative skills were subjectively decided based on other non-verbal behavior skills [22]. Although the automated social skills trainer did not provide feedback regarding narrative structure or eye gaze, these elements were also improved after the trainin...  \n",
              "3   \"each approach provides different affordances depending on the similarity metric and the dependent variable.\"\\n \"The take-away is that there are several strategies for doing multimodal learning analytics, and that many of these approaches can provide a meaningful glimpse into a complex data set, glimpses that may be difficult to identify using traditional approaches.\"\\n\\n\"Looking across analys...  \n",
              "4   \"Experiments of different types supported fairly reliable identification of markers from multimodal data, and linking these to assessments of debater confidence level and intensification behaviour.\"\\n\\n\"In line with previous empirical findings, we acknowledge that persuasive speech is rather difficult to characterize. Neverthe- less, based on theoretical and empirical frameworks set up by Gric...  \n",
              "5   \"We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain corpora and classification procedures resulted in the best performance on an argument structure mining task.\"\\n\\n\"We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain c...  \n",
              "6   \"In this paper, we show that MMLA and the state-of-the-art computational techniques can be used to generate insights into the \"black box\" of learning in students’ project-based activities. These insights generated from multimodal data can be used to inform teachers about the key features of project-based learning and help them support students appropriately in similar pedagogical approaches.\"\\...  \n",
              "7   While this study was not able to show a clear effect of providing a real-time visualization to support\\ncollaboration, it made many other valuable contributions. First, it showed that simple verbal interventions can\\nhelp participants pay attention to particular aspects of their collaborative behavior (i.e., how much they are\\ntalking and how much space they are providing to their partner). Se...  \n",
              "8   Finally, it was clear from the two intervention studies that the simulation provided by mobile devices, although not as powerful as the desktop-computer-powered Oculus Rift mixed reality or physical simulators, has no negative impacts on the learners. In fact, the simulation improves learners’ competence and skills with the skills intervention showing statistically significant improvements in ...  \n",
              "9   \"In summary, the results from our evaluation of personalized and general models to automatically extract orchestration graphs highlight the fact that machine learning models can be successfully trained with such multimodal sensor data, using relatively low-level features.\"\\n\\n\"In summary, the results from our evaluation of personalized and general models to automatically extract orchestration ...  \n",
              "10  Corse-grained learning trajectories were able to identify key moments during learning that warrented futher analysis with other MMD\\n\\n\"Our results from this analysis showed how students’ early experiences struggling with a novel concept could significantly affect both their entire learning trajectories within an activity and pre-test–post-test measurements of learning gains related to that co...  \n",
              "11  We presented two empirical studies, collected in classroom studies with two distinct learning technology systems in different contexts (individual and collaborative). Our analyses and findings showcase a few different ways, in which multimodal data sources can enrich our understanding of student learning and provide a more holistic picture.\\n\\nAuthors collected student‐focused screen and webca...  \n",
              "12  ``There was no difference between the proportion of the session spent being bored or frustrated or in the achievement scores for the two conditions. In order to determine whether the effect on achievement was different depending on length of exposure, participants with a duration less than 60 minutes were excluded from the analysis. However, there was still no significant difference between ac...  \n",
              "13  Feedback generated by RAP is similar to human feedback across several dimensions and similar to other more complex systems\\n\\n\"It revealed an overwhelmingly positive perception of the system especially in the dimensions of perceived usefulness and feedback which were rated as excellent by 65% and 58% of the students respectively. The qualitative analysis helped discover specific issues, on the...  \n",
              "14  RQ1: Better communication, better collaboration\\nRQ2: Collaborative teams showed lower variability in the estimates of story points (same page)\\nRQ3: Democratic leadership in collaborative groups\\n\\nThe results obtained allowed the detection of important relationships and characteristics of the collaborative and Non-Collaborative groups, with productivity, effort, and predominant personality s...  \n",
              "15  Authors present an initial pilot using these methods in concert to identify key moments in multiple modalities. While the use of constructivist dialogue mapping showed that the users learned during their interaction with Ant Adaption, emotional logging identified alternative moments of learning outside of their analytic framework.\\n\\nThis paper presented a preliminary approach to augment quali...  \n",
              "16  The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings\\n\\nWithin heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> mode...  \n",
              "17  In this study, we used non-transparent prediction models of openSMILE to predict the emotional traits of tutor candidates based on their audio data, and we used transparent logistic regression models to identify exactly what personality, emotion, and experience traits lead to effective debate tutoring skills. Predictive models were very powerful to make sense of complex and nonlinear audio dat...  \n",
              "18  Our major claim is that multichannel data can be potential for understanding regulatory processes in collaboration. With our five empirical case examples, we illustrate how triangulating multiple sources of data has potential to advance the theoretical and conceptual progress in social aspects of SRL theory.\\n\\nAuthors show with five empirical cases that multichannel data can be potential for ...  \n",
              "19  \"This study contributes an initial understanding into how different modalities mediate students’ interactions and offers implications for scaffolding peer interactions during multimodal composing processes.\"\\n\\n\"Overall, students were more likely to share ideas and ask questions, and they tended to use quick- response strategies while multimodal composing.\"\\n\\n\"Students’ interview responses al...  \n",
              "20                                                                                                                                                                                                                                Significant impact on the teacher's scaffolding behavior and student engagement (less bordem)\\n\\nSEAT had positive impact on student engagement and was also helpful to teachers.  \n",
              "21  \"We illustrate the approach from the automated analysis of healthcare simulations to train nurses, generating four visual proxies that fuse multimodal data into higher order patterns.\"\\n\\n\"we introduce the concept of collaboration translucence, an approach to make visible selected features of group activity. This is grounded both theoretically (in the physical, epistemic, social and affective ...  \n",
              "22  We found that idling with limited speech (i.e., silence or backchannel feedback only) and without movement was negatively correlated with task performance and with participants’ subjective perceptions of the collaboration. However, being silent and focused during solution execution was positively correlated with task performance. Results illustrate that in some cases, multimodal patterns impro...  \n",
              "23  Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.\\n\\nEvaluating how multimodal features contribute to a model's performance to predict learning gains...  \n",
              "24  This is the first study to evaluate an adaptive learning system for learners with ID based on multi-modal affect recognition. Three separate states were automatically identified, with lower levels of the state labelled “boredom” having the strongest link to learning achievement. Both those labelled “frustration” and “engagement” were positively related to achievement. Our results are in line w...  \n",
              "25  Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoid...  \n",
              "26  \"this research contributes to the relatively sparse literature in multimodal learning analytics by providing a balanced view of the teacher and student interactions with a data set collected in naturalistic home environments.\"\\n\\n\"Firstly, we note a clear causal pathway between the group of variables describing parents’ support and another group representing the child’s cognitive-affective exp...  \n",
              "27  \"Through our user studies, we found that the platform helps fulfill some of those goals by providing capabilities that can spur on amazement and excitement among traditional Minecraft users and novices. We also find that many of the multimodal components, while not immediately intuitive for users, proved to be preferred modes of game play. In this sense, we feel that this tool is moving in the...  \n",
              "28  Overall, our study suggests that coupling style can be operationalised as a multimodal indicator suitable to investigate complex collaboration dynamics in games, thus confirming what has been found in other domains\\n\\n\"Firstly, we found that coupling styles can be used to characterise collaboration in a co-located SG. Consistently with previous research (Isenberg et al, 2010; Niu et al, 2018),...  \n",
              "29  \"Our combined multi-modal learning analytics and interaction analysis methodology enabled us to identify two multi-modal profles of learners who have learning gains and one multi-modal profle of learners who do not have learning gains.\"\\n\\n\"Using this approach, we are able to build the multimodal behavioral profles for each group of learners.\"\\n\\n\"Our combined multi-modal learning analytics an...  \n",
              "30  Results demonstrate how NLP and ML techniques allow us to use different modalities of the same data, voice and transcript, and different modalities of different data sources, voice data from interviews, answers to a goal orientation questionnaire, and answers to open-ended questions about energy, in order to better understand individual differences in students’ performances\\n\\nUsed text and au...  \n",
              "31  \"We conducted a case study to compare the visualizations provided by the system in two different situations: collaborative and competitive activities. The results suggest that the provided visualizations help to identify issues on cognitive contribution, assimilation, self-regulation, and integration of the team members. They could also support teachers to decide whether they must assist a tea...  \n",
              "32  \"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Exploration, increasing confluence during Discovery, and high confluence during Fluency.\"\\n\\n\"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Explorati...  \n",
              "33  \"In this study, we have given special emphasis to the interpersonal metafunction (see Fig. 1), which is crucial to enhance EMI students’ active participation and engagement. The outcomes of this study point to the need to address multimodal interactional competence in professional development programs, as described in Morell et al. (2022) to promote effective lecturing in the diverse EMI scena...  \n",
              "34  \"We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements in the video modality are the most significant unimodal indicators of impasse.\"\\n\\n\"We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements i...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-695a0bad-94dc-4173-bf21-5db04bb0ac47\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type</th>\n",
              "      <th>Data Collection Mediums</th>\n",
              "      <th>Modalities</th>\n",
              "      <th>Analysis Methods</th>\n",
              "      <th>Fusion Types</th>\n",
              "      <th>Publication</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Results</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1118315889</td>\n",
              "      <td>using multimodal learning analytics to identify aspects of collaboration in project-based learning</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS</td>\n",
              "      <td>POSE,PROS</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>CSCL</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"physical aspect of collaborative is an important part of this type of learning and that learning analytics systems can identify features that are relevant for helping researchers, teachers, and learners unpack what is happening.\"\\n\\n\"In this research study, we presented that where the students are looking, the distance between them, the motion of their hands our key features for a learning an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3339002981</td>\n",
              "      <td>estimation of success in collaborative learning based on multimodal learning analytics features</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,LOGS,VIDEO,AUDIO</td>\n",
              "      <td>GAZE,LOGS,PROS,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICALT</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, authors found that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.\\n\\nPredicting the learning gains via classification (bad, ok, good) through gaze, logs, audio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3093310941</td>\n",
              "      <td>embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders</td>\n",
              "      <td>Hiroki Tanaka</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>AUDIO,VIDEO,PPA</td>\n",
              "      <td>POSE,PROS,AFFECT</td>\n",
              "      <td>REG,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>PLOS</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>We identified a positive correlation between overall narrative skills and other non-verbal behaviors. This result indicates that the overall narrative skills were subjectively decided based on other non-verbal behavior skills [22]. Although the automated social skills trainer did not provide feedback regarding narrative structure or eye gaze, these elements were also improved after the trainin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3095923626</td>\n",
              "      <td>a multimodal analysis of making</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR,PPA,INTER</td>\n",
              "      <td>GEST,PPA,EDA,ACT,PROS,QUAL,INTER</td>\n",
              "      <td>STATS,CLUST,QUAL,PATT</td>\n",
              "      <td>EARLY</td>\n",
              "      <td>IJAIED</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"each approach provides different affordances depending on the similarity metric and the dependent variable.\"\\n \"The take-away is that there are several strategies for doing multimodal learning analytics, and that many of these approaches can provide a meaningful glimpse into a complex data set, glimpses that may be difficult to identify using traditional approaches.\"\\n\\n\"Looking across analys...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>85990093</td>\n",
              "      <td>multimodal markers of persuasive speech : designing a virtual debate coach</td>\n",
              "      <td>Volha Petukhova</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>PROS,GEST</td>\n",
              "      <td>CLS,QUAL,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>INTERSPEECH</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"Experiments of different types supported fairly reliable identification of markers from multimodal data, and linking these to assessments of debater confidence level and intensification behaviour.\"\\n\\n\"In line with previous empirical findings, we acknowledge that persuasive speech is rather difficult to characterize. Neverthe- less, based on theoretical and empirical frameworks set up by Gric...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>957160695</td>\n",
              "      <td>virtual debate coach design: assessing multimodal argumentation performance</td>\n",
              "      <td>Volha Petukhova</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>GEST,TRANS,PROS,SURVEY,GAZE</td>\n",
              "      <td>STATS,CLS,QUAL</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICMI</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain corpora and classification procedures resulted in the best performance on an argument structure mining task.\"\\n\\n\"We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1637690235</td>\n",
              "      <td>supervised machine learning in multimodal learning analytics for estimating success in project-based learning</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,PPA,RPA</td>\n",
              "      <td>POSE,GEST,PROS,LOGS,PPA,RPA</td>\n",
              "      <td>REG,CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"In this paper, we show that MMLA and the state-of-the-art computational techniques can be used to generate insights into the \"black box\" of learning in students’ project-based activities. These insights generated from multimodal data can be used to inform teachers about the key features of project-based learning and help them support students appropriately in similar pedagogical approaches.\"\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2181637610</td>\n",
              "      <td>toward using multi-modal learning analytics to support and measure collaboration in co-located dyads</td>\n",
              "      <td>Emma L. Starr</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA,RPA,LOGS</td>\n",
              "      <td>POSE,PROS,PPA,RPA,LOGS</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ICLS</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>While this study was not able to show a clear effect of providing a real-time visualization to support\\ncollaboration, it made many other valuable contributions. First, it showed that simple verbal interventions can\\nhelp participants pay attention to particular aspects of their collaborative behavior (i.e., how much they are\\ntalking and how much space they are providing to their partner). Se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3146393211</td>\n",
              "      <td>mobile mixed reality for experiential learning and simulation in medical and health sciences education</td>\n",
              "      <td>James Birt</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>PPA,INTER</td>\n",
              "      <td>PPA,TRANS</td>\n",
              "      <td>QUAL,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>Information</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>Finally, it was clear from the two intervention studies that the simulation provided by mobile devices, although not as powerful as the desktop-computer-powered Oculus Rift mixed reality or physical simulators, has no negative impacts on the learners. In fact, the simulation improves learners’ competence and skills with the skills intervention showing statistically significant improvements in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3135645357</td>\n",
              "      <td>multimodal teaching analytics: automated extraction of orchestration graphs from wearable sensor data</td>\n",
              "      <td>Luis P. Prieto</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,VIDEO,AUDIO,MOTION</td>\n",
              "      <td>GAZE,PROS,ACT,PIXEL</td>\n",
              "      <td>NET,CLS,STATS,PATT,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"In summary, the results from our evaluation of personalized and general models to automatically extract orchestration graphs highlight the fact that machine learning models can be successfully trained with such multimodal sensor data, using relatively low-level features.\"\\n\\n\"In summary, the results from our evaluation of personalized and general models to automatically extract orchestration ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3783339081</td>\n",
              "      <td>a novel method for the in-depth multimodal analysis of student learning trajectories in intelligent tutoring systems</td>\n",
              "      <td>Ran Liu</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,SCREEN,PPA</td>\n",
              "      <td>LOGS,TRANS,ACT,QUAL,PPA</td>\n",
              "      <td>STATS,REG,QUAL</td>\n",
              "      <td>MID</td>\n",
              "      <td>JLA</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Corse-grained learning trajectories were able to identify key moments during learning that warrented futher analysis with other MMD\\n\\n\"Our results from this analysis showed how students’ early experiences struggling with a novel concept could significantly affect both their entire learning trajectories within an activity and pre-test–post-test measurements of learning gains related to that co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>3796180663</td>\n",
              "      <td>learning linkages: integrating data streams of multiple modalities and timescales</td>\n",
              "      <td>Ran Liu</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,SCREEN,PPA</td>\n",
              "      <td>TRANS,QUAL,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>We presented two empirical studies, collected in classroom studies with two distinct learning technology systems in different contexts (individual and collaborative). Our analyses and findings showcase a few different ways, in which multimodal data sources can enrich our understanding of student learning and provide a more holistic picture.\\n\\nAuthors collected student‐focused screen and webca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2345021698</td>\n",
              "      <td>exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course</td>\n",
              "      <td>René Noël</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,PPA,RPA</td>\n",
              "      <td>RPA,PROS</td>\n",
              "      <td>QUAL,NET,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>Access</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>``There was no difference between the proportion of the session spent being bored or frustrated or in the achievement scores for the two conditions. In order to determine whether the effect on achievement was different depending on length of exposure, participants with a duration less than 60 minutes were excluded from the analysis. However, there was still no significant difference between ac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2497456347</td>\n",
              "      <td>the rap system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2018</td>\n",
              "      <td>Training</td>\n",
              "      <td>AUDIO,VIDEO,PPA,SURVEY</td>\n",
              "      <td>PPA,GAZE,POSE,PROS</td>\n",
              "      <td>CLS,STATS,QUAL</td>\n",
              "      <td>LATE</td>\n",
              "      <td>LAK</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>Feedback generated by RAP is similar to human feedback across several dimensions and similar to other more complex systems\\n\\n\"It revealed an overwhelmingly positive perception of the system especially in the dimensions of perceived usefulness and feedback which were rated as excellent by 65% and 58% of the students respectively. The qualitative analysis helped discover specific issues, on the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>4019205162</td>\n",
              "      <td>introducing low-cost sensors into the classroom settings: improving the assessment in agile practices with multimodal learning analytics</td>\n",
              "      <td>Hector Cornide-Reyes</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,SURVEY,PPA,RPA</td>\n",
              "      <td>SURVEY,TRANS,PPA,RPA</td>\n",
              "      <td>NET,STATS</td>\n",
              "      <td>MID,OTH</td>\n",
              "      <td>Sensors</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>RQ1: Better communication, better collaboration\\nRQ2: Collaborative teams showed lower variability in the estimates of story points (same page)\\nRQ3: Democratic leadership in collaborative groups\\n\\nThe results obtained allowed the detection of important relationships and characteristics of the collaborative and Non-Collaborative groups, with productivity, effort, and predominant personality s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1847468084</td>\n",
              "      <td>computationally augmented ethnography: emotion tracking and learning in museum games</td>\n",
              "      <td>Kit Martin</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA,RPA</td>\n",
              "      <td>TRANS,AFFECT,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ICQE</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>Authors present an initial pilot using these methods in concert to identify key moments in multiple modalities. While the use of constructivist dialogue mapping showed that the users learned during their interaction with Ant Adaption, emotional logging identified alternative moments of learning outside of their analytic framework.\\n\\nThis paper presented a preliminary approach to augment quali...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings\\n\\nWithin heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; mode...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1576545447</td>\n",
              "      <td>artificial intelligence and multimodal data in the service of human decision-making: a case study in debate tutoring</td>\n",
              "      <td>Mutlu Cukurova</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,SURVEY</td>\n",
              "      <td>AFFECT,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>In this study, we used non-transparent prediction models of openSMILE to predict the emotional traits of tutor candidates based on their audio data, and we used transparent logistic regression models to identify exactly what personality, emotion, and experience traits lead to effective debate tutoring skills. Predictive models were very powerful to make sense of complex and nonlinear audio dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3398902089</td>\n",
              "      <td>what multimodal data can tell us about the students’ regulation of their learning process?</td>\n",
              "      <td>Sanna Järvelä</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SENSOR,VIDEO,AUDIO</td>\n",
              "      <td>EDA,AFFECT,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>LAI</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>Our major claim is that multichannel data can be potential for understanding regulatory processes in collaboration. With our five empirical case examples, we illustrate how triangulating multiple sources of data has potential to advance the theoretical and conceptual progress in social aspects of SRL theory.\\n\\nAuthors show with five empirical cases that multichannel data can be potential for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>86191824</td>\n",
              "      <td>examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes</td>\n",
              "      <td>Shiyan Jiang</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SCREEN,INTER,PPA,AUDIO</td>\n",
              "      <td>INTER,QUAL,TRANS</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ILE</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"This study contributes an initial understanding into how different modalities mediate students’ interactions and offers implications for scaffolding peer interactions during multimodal composing processes.\"\\n\\n\"Overall, students were more likely to share ideas and ask questions, and they tended to use quick- response strategies while multimodal composing.\"\\n\\n\"Students’ interview responses al...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>3448122334</td>\n",
              "      <td>investigating the impact of a real-time, multimodal student engagement analytics technology in authentic classrooms</td>\n",
              "      <td>Sinem Aslan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,SCREEN,AUDIO,SURVEY,RPA,PPA,INTER</td>\n",
              "      <td>AFFECT,LOGS,POSE,QUAL,INTER,SURVEY,RPA</td>\n",
              "      <td>QUAL,STATS,CLS</td>\n",
              "      <td>LATE</td>\n",
              "      <td>CHI</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Significant impact on the teacher's scaffolding behavior and student engagement (less bordem)\\n\\nSEAT had positive impact on student engagement and was also helpful to teachers.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1296637108</td>\n",
              "      <td>towards collaboration translucence: giving meaning to multimodal group data</td>\n",
              "      <td>Vanessa Echeverria</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER</td>\n",
              "      <td>POSE,LOGS,TRANS,EDA,ACT,PROS,INTER</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>CHI</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"We illustrate the approach from the automated analysis of healthcare simulations to train nurses, generating four visual proxies that fuse multimodal data into higher order patterns.\"\\n\\n\"we introduce the concept of collaboration translucence, an approach to make visible selected features of group activity. This is grounded both theoretically (in the physical, epistemic, social and affective ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1770989706</td>\n",
              "      <td>focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving</td>\n",
              "      <td>Hana Vrzakova</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,SCREEN,SURVEY,PPA</td>\n",
              "      <td>PROS,ACT,GEST,PPA</td>\n",
              "      <td>STATS,PATT</td>\n",
              "      <td>MID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>We found that idling with limited speech (i.e., silence or backchannel feedback only) and without movement was negatively correlated with task performance and with participants’ subjective perceptions of the collaboration. However, being silent and focused during solution execution was positively correlated with task performance. Results illustrate that in some cases, multimodal patterns impro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3051560548</td>\n",
              "      <td>temporal analysis of multimodal data to predict collaborative learning outcomes</td>\n",
              "      <td>Jennifer K. Olsen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,EYE</td>\n",
              "      <td>GAZE,LOGS,PROS,TRANS,QUAL</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.\\n\\nEvaluating how multimodal features contribute to a model's performance to predict learning gains...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>3796643912</td>\n",
              "      <td>an evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities</td>\n",
              "      <td>Penelope J. Standen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,RPA</td>\n",
              "      <td>AFFECT,POSE,LOGS,RPA,GAZE,PROS,GEST</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>HUM, OTH, STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>This is the first study to evaluate an adaptive learning system for learners with ID based on multi-modal affect recognition. Three separate states were automatically identified, with lower levels of the state labelled “boredom” having the strongest link to learning achievement. Both those labelled “frustration” and “engagement” were positively related to achievement. Our results are in line w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MF</td>\n",
              "      <td>Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1426267857</td>\n",
              "      <td>affect, support, and personal factors: multimodal causal models of one-on-one coaching</td>\n",
              "      <td>Lujie Karen Chen</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,SURVEY</td>\n",
              "      <td>PROS,GAZE,TRANS,AFFECT,SURVEY</td>\n",
              "      <td>STATS,NET</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JEDM</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"this research contributes to the relatively sparse literature in multimodal learning analytics by providing a balanced view of the teacher and student interactions with a data set collected in naturalistic home environments.\"\\n\\n\"Firstly, we note a clear causal pathway between the group of variables describing parents’ support and another group representing the child’s cognitive-affective exp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>666050348</td>\n",
              "      <td>multicraft: a multimodal interface for supporting and studying learning in minecraft</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS</td>\n",
              "      <td>PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>HCII</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"Through our user studies, we found that the platform helps fulfill some of those goals by providing capabilities that can spur on amazement and excitement among traditional Minecraft users and novices. We also find that many of the multimodal components, while not immediately intuitive for users, proved to be preferred modes of game play. In this sense, we feel that this tool is moving in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>518268671</td>\n",
              "      <td>using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game</td>\n",
              "      <td>María Ximena López</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,LOGS,AUDIO,VIDEO</td>\n",
              "      <td>LOGS,SURVEY,GAZE,PROS</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ECGBL</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>Overall, our study suggests that coupling style can be operationalised as a multimodal indicator suitable to investigate complex collaboration dynamics in games, thus confirming what has been found in other domains\\n\\n\"Firstly, we found that coupling styles can be used to characterise collaboration in a co-located SG. Consistently with previous research (Isenberg et al, 2010; Niu et al, 2018),...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2273914836</td>\n",
              "      <td>many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities</td>\n",
              "      <td>Jauwairia Nasir</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,PPA,SURVEY</td>\n",
              "      <td>PROS,AFFECT,GAZE,TRANS,LOGS</td>\n",
              "      <td>STATS,QUAL,CLUST,CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJCSCL</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"Our combined multi-modal learning analytics and interaction analysis methodology enabled us to identify two multi-modal profles of learners who have learning gains and one multi-modal profle of learners who do not have learning gains.\"\\n\\n\"Using this approach, we are able to build the multimodal behavioral profles for each group of learners.\"\\n\\n\"Our combined multi-modal learning analytics an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>32184286</td>\n",
              "      <td>once more with feeling: emotions in multimodal learning analytics</td>\n",
              "      <td>Marcus Kubsch</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,PPA,AUDIO</td>\n",
              "      <td>INTER,SURVEY,TRANS,PROS,AFFECT</td>\n",
              "      <td>CLS,REG,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MMLA Handbook</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>Results demonstrate how NLP and ML techniques allow us to use different modalities of the same data, voice and transcript, and different modalities of different data sources, voice data from interviews, answers to a goal orientation questionnaire, and answers to open-ended questions about energy, in order to better understand individual differences in students’ performances\\n\\nUsed text and au...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>2609260641</td>\n",
              "      <td>visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication</td>\n",
              "      <td>René Noël</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,RPA,INTER</td>\n",
              "      <td>PROS,POSE,RPA,INTER,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>DAMLE</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>PROF, UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"We conducted a case study to compare the visualizations provided by the system in two different situations: collaborative and competitive activities. The results suggest that the provided visualizations help to identify issues on cognitive contribution, assimilation, self-regulation, and integration of the team members. They could also support teachers to decide whether they must assist a tea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1345598079</td>\n",
              "      <td>intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning</td>\n",
              "      <td>Sofia Tancredi</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,VIDEO,AUDIO,INTER</td>\n",
              "      <td>GAZE,GEST,TRANS,POSE,INTER</td>\n",
              "      <td>PATT,QUAL,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MMLA Handbook</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Exploration, increasing confluence during Discovery, and high confluence during Fluency.\"\\n\\n\"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Explorati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>2155422499</td>\n",
              "      <td>a multimodal analysis of pair work engagement episodes: implications for emi lecturer training</td>\n",
              "      <td>Teresa Morell</td>\n",
              "      <td>2022</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>TRANS,PPA,QUAL,POSE,ACT</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>JEAP</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"In this study, we have given special emphasis to the interpersonal metafunction (see Fig. 1), which is crucial to enhance EMI students’ active participation and engagement. The outcomes of this study point to the need to address multimodal interactional competence in professional development programs, as described in Morell et al. (2022) to promote effective lecturing in the diverse EMI scena...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>3754172825</td>\n",
              "      <td>detecting impasse during collaborative problem solving with multimodal learning analytics</td>\n",
              "      <td>Yingbo Ma</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>TRANS,PROS,SPECT,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements in the video modality are the most significant unimodal indicators of impasse.\"\\n\\n\"We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-695a0bad-94dc-4173-bf21-5db04bb0ac47')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-695a0bad-94dc-4173-bf21-5db04bb0ac47 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-695a0bad-94dc-4173-bf21-5db04bb0ac47');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7a6b874f-4d1d-41bb-9f4b-194bda9b98dc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7a6b874f-4d1d-41bb-9f4b-194bda9b98dc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7a6b874f-4d1d-41bb-9f4b-194bda9b98dc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save File"
      ],
      "metadata": {
        "id": "ZxquWaXbiNNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUT_PATH = \"drive/MyDrive/Clayton/20230420_MMLTE/S18_NLP_With_Results.csv\"\n",
        "# S18_NLP.to_csv(OUT_PATH, index=False)"
      ],
      "metadata": {
        "id": "N9751WbkiOYg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify Saved File"
      ],
      "metadata": {
        "id": "2Uas2FVEiu23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_nlp = pd.read_csv(OUT_PATH)\n",
        "assert df_test_nlp.equals(S18_NLP)"
      ],
      "metadata": {
        "id": "OjYxpN2aiw_9"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}