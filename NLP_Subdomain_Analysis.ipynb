{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Clayton Cohn<br>\n",
        "5 Dec 2023<br>\n",
        "OELE Lab<br>\n",
        "Vanderbilt University <br>\n",
        "# <center>\n",
        "# <center> NLP Subdomain Analysis"
      ],
      "metadata": {
        "id": "9PKxx1Tapha2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction and Attribution\n",
        "\n",
        "This notebook was create by Clayton Cohn for the purpose of analyzing the NLP subdomain in the MMLTE corups.\n",
        "\n",
        "The MMLTE survey project is a collaborative effor between Dr. Gautam Biswas, Clayton Cohn, Eduardo Davalos, Joyce Fonteles, Dr. Meiyi Ma, Caleb Vatral, and Hanchen (David) Wang."
      ],
      "metadata": {
        "id": "J01H3CtWqLU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import NLP Domain Papers"
      ],
      "metadata": {
        "id": "VmE1ww6NdAIZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "42RVMc57cF0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "932dd879-9bda-44ca-cc6b-1716cf98bb34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Used for readability\n",
        "pd.set_option('max_colwidth', 400)\n",
        "pd.set_option('display.max_columns', 50)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "NLP_PATH = \"drive/MyDrive/Clayton/20230420_MMLTE/S18_NLP_With_Results.csv\"\n",
        "df_nlp = pd.read_csv(NLP_PATH)\n",
        "\n",
        "print(len(df_nlp))\n",
        "df_nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DCNW6Rl4dEO0",
        "outputId": "3af8cd68-b55e-4e81-9abe-e8bc6f325fd7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          UUID  \\\n",
              "0   1118315889   \n",
              "1   3339002981   \n",
              "2   3093310941   \n",
              "3   3095923626   \n",
              "4     85990093   \n",
              "5    957160695   \n",
              "6   1637690235   \n",
              "7   2181637610   \n",
              "8   3146393211   \n",
              "9   3135645357   \n",
              "10  3783339081   \n",
              "11  3796180663   \n",
              "12  2345021698   \n",
              "13  2497456347   \n",
              "14  4019205162   \n",
              "15  1847468084   \n",
              "16  1326191931   \n",
              "17  1576545447   \n",
              "18  3398902089   \n",
              "19    86191824   \n",
              "20  3448122334   \n",
              "21  1296637108   \n",
              "22  1770989706   \n",
              "23  3051560548   \n",
              "24  3796643912   \n",
              "25  2634033325   \n",
              "26  1426267857   \n",
              "27   666050348   \n",
              "28   518268671   \n",
              "29  2273914836   \n",
              "30    32184286   \n",
              "31  2609260641   \n",
              "32  1345598079   \n",
              "33  2155422499   \n",
              "34  3754172825   \n",
              "\n",
              "                                                                                                                                       Title  \\\n",
              "0                                         using multimodal learning analytics to identify aspects of collaboration in project-based learning   \n",
              "1                                            estimation of success in collaborative learning based on multimodal learning analytics features   \n",
              "2                    embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders   \n",
              "3                                                                                                            a multimodal analysis of making   \n",
              "4                                                                 multimodal markers of persuasive speech : designing a virtual debate coach   \n",
              "5                                                                virtual debate coach design: assessing multimodal argumentation performance   \n",
              "6                              supervised machine learning in multimodal learning analytics for estimating success in project-based learning   \n",
              "7                                       toward using multi-modal learning analytics to support and measure collaboration in co-located dyads   \n",
              "8                                     mobile mixed reality for experiential learning and simulation in medical and health sciences education   \n",
              "9                                      multimodal teaching analytics: automated extraction of orchestration graphs from wearable sensor data   \n",
              "10                      a novel method for the in-depth multimodal analysis of student learning trajectories in intelligent tutoring systems   \n",
              "11                                                         learning linkages: integrating data streams of multiple modalities and timescales   \n",
              "12         exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course   \n",
              "13                             the rap system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors   \n",
              "14  introducing low-cost sensors into the classroom settings: improving the assessment in agile practices with multimodal learning analytics   \n",
              "15                                                      computationally augmented ethnography: emotion tracking and learning in museum games   \n",
              "16                                                                                   multimodal learning analytics in a laboratory classroom   \n",
              "17                      artificial intelligence and multimodal data in the service of human decision-making: a case study in debate tutoring   \n",
              "18                                                what multimodal data can tell us about the students’ regulation of their learning process?   \n",
              "19                 examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes   \n",
              "20                       investigating the impact of a real-time, multimodal student engagement analytics technology in authentic classrooms   \n",
              "21                                                               towards collaboration translucence: giving meaning to multimodal group data   \n",
              "22                                focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving   \n",
              "23                                                           temporal analysis of multimodal data to predict collaborative learning outcomes   \n",
              "24           an evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities   \n",
              "25                               controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "26                                                    affect, support, and personal factors: multimodal causal models of one-on-one coaching   \n",
              "27                                                      multicraft: a multimodal interface for supporting and studying learning in minecraft   \n",
              "28                                 using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game   \n",
              "29             many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities   \n",
              "30                                                                         once more with feeling: emotions in multimodal learning analytics   \n",
              "31                              visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication   \n",
              "32     intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning   \n",
              "33                                            a multimodal analysis of pair work engagement episodes: implications for emi lecturer training   \n",
              "34                                                 detecting impasse during collaborative problem solving with multimodal learning analytics   \n",
              "\n",
              "             First Author  Year Environment Type  \\\n",
              "0           Daniel Spikol  2017         Learning   \n",
              "1           Daniel Spikol  2017         Learning   \n",
              "2           Hiroki Tanaka  2017         Training   \n",
              "3         Marcelo Worsley  2017         Learning   \n",
              "4         Volha Petukhova  2017         Training   \n",
              "5         Volha Petukhova  2017         Training   \n",
              "6           Daniel Spikol  2018         Learning   \n",
              "7           Emma L. Starr  2018         Learning   \n",
              "8              James Birt  2018         Learning   \n",
              "9          Luis P. Prieto  2018         Learning   \n",
              "10                Ran Liu  2018         Learning   \n",
              "11                Ran Liu  2018         Learning   \n",
              "12              René Noël  2018         Learning   \n",
              "13           Xavier Ochoa  2018         Training   \n",
              "14   Hector Cornide-Reyes  2019         Learning   \n",
              "15             Kit Martin  2019         Learning   \n",
              "16  Man Ching Esther Chan  2019         Learning   \n",
              "17         Mutlu Cukurova  2019         Learning   \n",
              "18          Sanna Järvelä  2019         Learning   \n",
              "19           Shiyan Jiang  2019         Learning   \n",
              "20            Sinem Aslan  2019         Learning   \n",
              "21     Vanessa Echeverria  2019         Training   \n",
              "22          Hana Vrzakova  2020         Learning   \n",
              "23      Jennifer K. Olsen  2020         Learning   \n",
              "24    Penelope J. Standen  2020         Learning   \n",
              "25           Xavier Ochoa  2020         Training   \n",
              "26       Lujie Karen Chen  2021         Learning   \n",
              "27        Marcelo Worsley  2021         Learning   \n",
              "28     María Ximena López  2021         Learning   \n",
              "29        Jauwairia Nasir  2022         Learning   \n",
              "30          Marcus Kubsch  2022         Learning   \n",
              "31              René Noël  2022         Learning   \n",
              "32         Sofia Tancredi  2022         Learning   \n",
              "33          Teresa Morell  2022         Training   \n",
              "34              Yingbo Ma  2022         Learning   \n",
              "\n",
              "                          Data Collection Mediums  \\\n",
              "0                                VIDEO,AUDIO,LOGS   \n",
              "1                            EYE,LOGS,VIDEO,AUDIO   \n",
              "2                                 AUDIO,VIDEO,PPA   \n",
              "3                    VIDEO,AUDIO,SENSOR,PPA,INTER   \n",
              "4                                     VIDEO,AUDIO   \n",
              "5                                     VIDEO,AUDIO   \n",
              "6                        VIDEO,AUDIO,LOGS,PPA,RPA   \n",
              "7                        VIDEO,AUDIO,PPA,RPA,LOGS   \n",
              "8                                       PPA,INTER   \n",
              "9                          EYE,VIDEO,AUDIO,MOTION   \n",
              "10                          LOGS,AUDIO,SCREEN,PPA   \n",
              "11                    VIDEO,AUDIO,LOGS,SCREEN,PPA   \n",
              "12                                  AUDIO,PPA,RPA   \n",
              "13                         AUDIO,VIDEO,PPA,SURVEY   \n",
              "14                           AUDIO,SURVEY,PPA,RPA   \n",
              "15                            VIDEO,AUDIO,PPA,RPA   \n",
              "16                                    VIDEO,AUDIO   \n",
              "17                                   AUDIO,SURVEY   \n",
              "18                             SENSOR,VIDEO,AUDIO   \n",
              "19                         SCREEN,INTER,PPA,AUDIO   \n",
              "20   VIDEO,LOGS,SCREEN,AUDIO,SURVEY,RPA,PPA,INTER   \n",
              "21           VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER   \n",
              "22                  AUDIO,VIDEO,SCREEN,SURVEY,PPA   \n",
              "23                                 LOGS,AUDIO,EYE   \n",
              "24                           VIDEO,AUDIO,LOGS,RPA   \n",
              "25                                VIDEO,AUDIO,PPA   \n",
              "26                             AUDIO,VIDEO,SURVEY   \n",
              "27  AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS   \n",
              "28                        SURVEY,LOGS,AUDIO,VIDEO   \n",
              "29                    VIDEO,AUDIO,LOGS,PPA,SURVEY   \n",
              "30                               SURVEY,PPA,AUDIO   \n",
              "31                          AUDIO,VIDEO,RPA,INTER   \n",
              "32                          EYE,VIDEO,AUDIO,INTER   \n",
              "33                                VIDEO,AUDIO,PPA   \n",
              "34                                    VIDEO,AUDIO   \n",
              "\n",
              "                                Modalities         Analysis Methods  \\\n",
              "0                                POSE,PROS                      REG   \n",
              "1                      GAZE,LOGS,PROS,POSE                      CLS   \n",
              "2                         POSE,PROS,AFFECT                REG,STATS   \n",
              "3         GEST,PPA,EDA,ACT,PROS,QUAL,INTER    STATS,CLUST,QUAL,PATT   \n",
              "4                                PROS,GEST           CLS,QUAL,STATS   \n",
              "5              GEST,TRANS,PROS,SURVEY,GAZE           STATS,CLS,QUAL   \n",
              "6              POSE,GEST,PROS,LOGS,PPA,RPA                  REG,CLS   \n",
              "7                   POSE,PROS,PPA,RPA,LOGS               STATS,QUAL   \n",
              "8                                PPA,TRANS               QUAL,STATS   \n",
              "9                      GAZE,PROS,ACT,PIXEL  NET,CLS,STATS,PATT,QUAL   \n",
              "10                 LOGS,TRANS,ACT,QUAL,PPA           STATS,REG,QUAL   \n",
              "11                          TRANS,QUAL,PPA                CLS,STATS   \n",
              "12                                RPA,PROS           QUAL,NET,STATS   \n",
              "13                      PPA,GAZE,POSE,PROS           CLS,STATS,QUAL   \n",
              "14                    SURVEY,TRANS,PPA,RPA                NET,STATS   \n",
              "15                       TRANS,AFFECT,QUAL                     QUAL   \n",
              "16                          POSE,GAZE,PROS                CLS,CLUST   \n",
              "17                             AFFECT,LOGS                      CLS   \n",
              "18                         EDA,AFFECT,QUAL                     QUAL   \n",
              "19                        INTER,QUAL,TRANS                     QUAL   \n",
              "20  AFFECT,LOGS,POSE,QUAL,INTER,SURVEY,RPA           QUAL,STATS,CLS   \n",
              "21      POSE,LOGS,TRANS,EDA,ACT,PROS,INTER                     QUAL   \n",
              "22                       PROS,ACT,GEST,PPA               STATS,PATT   \n",
              "23               GAZE,LOGS,PROS,TRANS,QUAL                      REG   \n",
              "24     AFFECT,POSE,LOGS,RPA,GAZE,PROS,GEST                CLS,STATS   \n",
              "25                           POSE,PROS,PPA                    STATS   \n",
              "26           PROS,GAZE,TRANS,AFFECT,SURVEY                STATS,NET   \n",
              "27  PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS                     QUAL   \n",
              "28                   LOGS,SURVEY,GAZE,PROS                    STATS   \n",
              "29             PROS,AFFECT,GAZE,TRANS,LOGS     STATS,QUAL,CLUST,CLS   \n",
              "30          INTER,SURVEY,TRANS,PROS,AFFECT            CLS,REG,STATS   \n",
              "31                PROS,POSE,RPA,INTER,QUAL                     QUAL   \n",
              "32              GAZE,GEST,TRANS,POSE,INTER          PATT,QUAL,STATS   \n",
              "33                 TRANS,PPA,QUAL,POSE,ACT                     QUAL   \n",
              "34              TRANS,PROS,SPECT,GAZE,POSE                      CLS   \n",
              "\n",
              "   Fusion Types    Publication Environment Setting Environment Subject  \\\n",
              "0           MID           CSCL                PHYS                STEM   \n",
              "1           MID          ICALT                VIRT                STEM   \n",
              "2           MID           PLOS                VIRT                 HUM   \n",
              "3         EARLY         IJAIED                PHYS                STEM   \n",
              "4           MID    INTERSPEECH                PHYS                 HUM   \n",
              "5           MID           ICMI                PHYS                 HUM   \n",
              "6           MID           JCAL                BLND                STEM   \n",
              "7           OTH           ICLS                BLND                STEM   \n",
              "8           OTH    Information                BLND                STEM   \n",
              "9        HYBRID           JCAL                PHYS                STEM   \n",
              "10          MID            JLA                VIRT                STEM   \n",
              "11          MID           JCAL                VIRT                STEM   \n",
              "12          OTH         Access                PHYS                STEM   \n",
              "13         LATE            LAK                BLND                 HUM   \n",
              "14      MID,OTH        Sensors                PHYS                STEM   \n",
              "15          OTH           ICQE                VIRT                STEM   \n",
              "16         LATE         MLPALA                PHYS                STEM   \n",
              "17          MID           BJET                UNSP                 HUM   \n",
              "18          OTH            LAI                BLND                STEM   \n",
              "19          OTH            ILE                VIRT                STEM   \n",
              "20         LATE            CHI                VIRT                STEM   \n",
              "21          OTH            CHI                PHYS                STEM   \n",
              "22          MID            LAK                VIRT                STEM   \n",
              "23          MID           BJET                VIRT                STEM   \n",
              "24       HYBRID           BJET                VIRT      HUM, OTH, STEM   \n",
              "25          OTH           BJET                BLND                 HUM   \n",
              "26       HYBRID           JEDM                PHYS                STEM   \n",
              "27          OTH           HCII                VIRT                STEM   \n",
              "28          OTH          ECGBL                BLND                STEM   \n",
              "29       HYBRID         IJCSCL                BLND                STEM   \n",
              "30          OTH  MMLA Handbook                PHYS                STEM   \n",
              "31          OTH          DAMLE                PHYS                 HUM   \n",
              "32          OTH  MMLA Handbook                VIRT                STEM   \n",
              "33          OTH           JEAP                PHYS                 OTH   \n",
              "34       HYBRID            LAK                VIRT                STEM   \n",
              "\n",
              "   Participant Structure Didactic Nature Level of Instruction or Training  \\\n",
              "0                  MULTI           INSTR                              UNI   \n",
              "1                  MULTI           INSTR                              UNI   \n",
              "2                    IND           TRAIN                         K12, UNI   \n",
              "3                  MULTI             INF                         K12, UNI   \n",
              "4                  MULTI           TRAIN                              K12   \n",
              "5                  MULTI           TRAIN                              K12   \n",
              "6                  MULTI           INSTR                              UNI   \n",
              "7                  MULTI             INF                              UNI   \n",
              "8                    IND           INSTR                              UNI   \n",
              "9                  MULTI           INSTR                             PROF   \n",
              "10                   IND           INSTR                              K12   \n",
              "11            IND, MULTI           INSTR                              K12   \n",
              "12                 MULTI             INF                              UNI   \n",
              "13                   IND           TRAIN                              UNI   \n",
              "14                 MULTI           INSTR                              UNI   \n",
              "15                 MULTI             INF                             UNSP   \n",
              "16            IND, MULTI           INSTR                             UNSP   \n",
              "17                   IND           TRAIN                             UNSP   \n",
              "18                 MULTI           INSTR                              K12   \n",
              "19                 MULTI           INSTR                              K12   \n",
              "20                   IND           INSTR                              K12   \n",
              "21                 MULTI           TRAIN                              UNI   \n",
              "22                 MULTI           INSTR                              UNI   \n",
              "23                 MULTI           INSTR                              K12   \n",
              "24                   IND           INSTR                              K12   \n",
              "25                   IND           TRAIN                             UNSP   \n",
              "26                   IND           INSTR                              K12   \n",
              "27                   IND             INF                              K12   \n",
              "28                 MULTI           INSTR                              UNI   \n",
              "29                 MULTI           INSTR                              K12   \n",
              "30                   IND           INSTR                              K12   \n",
              "31                 MULTI             INF                        PROF, UNI   \n",
              "32                   IND           INSTR                              K12   \n",
              "33                 MULTI           TRAIN                             PROF   \n",
              "34                 MULTI           INSTR                              K12   \n",
              "\n",
              "   Analysis Approach  \\\n",
              "0                 MB   \n",
              "1                 MB   \n",
              "2                 MF   \n",
              "3                 MB   \n",
              "4                 MB   \n",
              "5                 MB   \n",
              "6                 MB   \n",
              "7                 MF   \n",
              "8                 MF   \n",
              "9                 MB   \n",
              "10                MB   \n",
              "11            MB, MF   \n",
              "12            MB, MF   \n",
              "13                MB   \n",
              "14            MB, MF   \n",
              "15            MB, MF   \n",
              "16                MB   \n",
              "17            MB, MF   \n",
              "18                MF   \n",
              "19                MB   \n",
              "20                MB   \n",
              "21                MB   \n",
              "22                MF   \n",
              "23                MB   \n",
              "24                MB   \n",
              "25                MF   \n",
              "26                MB   \n",
              "27                MF   \n",
              "28                MF   \n",
              "29                MB   \n",
              "30            MB, MF   \n",
              "31                MF   \n",
              "32                MF   \n",
              "33                MF   \n",
              "34                MB   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                            Results  \n",
              "0   \"physical aspect of collaborative is an important part of this type of learning and that learning analytics systems can identify features that are relevant for helping researchers, teachers, and learners unpack what is happening.\"\\n\\n\"In this research study, we presented that where the students are looking, the distance between them, the motion of their hands our key features for a learning an...  \n",
              "1   Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, authors found that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.\\n\\nPredicting the learning gains via classification (bad, ok, good) through gaze, logs, audio...  \n",
              "2   We identified a positive correlation between overall narrative skills and other non-verbal behaviors. This result indicates that the overall narrative skills were subjectively decided based on other non-verbal behavior skills [22]. Although the automated social skills trainer did not provide feedback regarding narrative structure or eye gaze, these elements were also improved after the trainin...  \n",
              "3   \"each approach provides different affordances depending on the similarity metric and the dependent variable.\"\\n \"The take-away is that there are several strategies for doing multimodal learning analytics, and that many of these approaches can provide a meaningful glimpse into a complex data set, glimpses that may be difficult to identify using traditional approaches.\"\\n\\n\"Looking across analys...  \n",
              "4   \"Experiments of different types supported fairly reliable identification of markers from multimodal data, and linking these to assessments of debater confidence level and intensification behaviour.\"\\n\\n\"In line with previous empirical findings, we acknowledge that persuasive speech is rather difficult to characterize. Neverthe- less, based on theoretical and empirical frameworks set up by Gric...  \n",
              "5   \"We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain corpora and classification procedures resulted in the best performance on an argument structure mining task.\"\\n\\n\"We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain c...  \n",
              "6   \"In this paper, we show that MMLA and the state-of-the-art computational techniques can be used to generate insights into the \"black box\" of learning in students’ project-based activities. These insights generated from multimodal data can be used to inform teachers about the key features of project-based learning and help them support students appropriately in similar pedagogical approaches.\"\\...  \n",
              "7   While this study was not able to show a clear effect of providing a real-time visualization to support\\ncollaboration, it made many other valuable contributions. First, it showed that simple verbal interventions can\\nhelp participants pay attention to particular aspects of their collaborative behavior (i.e., how much they are\\ntalking and how much space they are providing to their partner). Se...  \n",
              "8   Finally, it was clear from the two intervention studies that the simulation provided by mobile devices, although not as powerful as the desktop-computer-powered Oculus Rift mixed reality or physical simulators, has no negative impacts on the learners. In fact, the simulation improves learners’ competence and skills with the skills intervention showing statistically significant improvements in ...  \n",
              "9   \"In summary, the results from our evaluation of personalized and general models to automatically extract orchestration graphs highlight the fact that machine learning models can be successfully trained with such multimodal sensor data, using relatively low-level features.\"\\n\\n\"In summary, the results from our evaluation of personalized and general models to automatically extract orchestration ...  \n",
              "10  Corse-grained learning trajectories were able to identify key moments during learning that warrented futher analysis with other MMD\\n\\n\"Our results from this analysis showed how students’ early experiences struggling with a novel concept could significantly affect both their entire learning trajectories within an activity and pre-test–post-test measurements of learning gains related to that co...  \n",
              "11  We presented two empirical studies, collected in classroom studies with two distinct learning technology systems in different contexts (individual and collaborative). Our analyses and findings showcase a few different ways, in which multimodal data sources can enrich our understanding of student learning and provide a more holistic picture.\\n\\nAuthors collected student‐focused screen and webca...  \n",
              "12  ``There was no difference between the proportion of the session spent being bored or frustrated or in the achievement scores for the two conditions. In order to determine whether the effect on achievement was different depending on length of exposure, participants with a duration less than 60 minutes were excluded from the analysis. However, there was still no significant difference between ac...  \n",
              "13  Feedback generated by RAP is similar to human feedback across several dimensions and similar to other more complex systems\\n\\n\"It revealed an overwhelmingly positive perception of the system especially in the dimensions of perceived usefulness and feedback which were rated as excellent by 65% and 58% of the students respectively. The qualitative analysis helped discover specific issues, on the...  \n",
              "14  RQ1: Better communication, better collaboration\\nRQ2: Collaborative teams showed lower variability in the estimates of story points (same page)\\nRQ3: Democratic leadership in collaborative groups\\n\\nThe results obtained allowed the detection of important relationships and characteristics of the collaborative and Non-Collaborative groups, with productivity, effort, and predominant personality s...  \n",
              "15  Authors present an initial pilot using these methods in concert to identify key moments in multiple modalities. While the use of constructivist dialogue mapping showed that the users learned during their interaction with Ant Adaption, emotional logging identified alternative moments of learning outside of their analytic framework.\\n\\nThis paper presented a preliminary approach to augment quali...  \n",
              "16  The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings\\n\\nWithin heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> mode...  \n",
              "17  In this study, we used non-transparent prediction models of openSMILE to predict the emotional traits of tutor candidates based on their audio data, and we used transparent logistic regression models to identify exactly what personality, emotion, and experience traits lead to effective debate tutoring skills. Predictive models were very powerful to make sense of complex and nonlinear audio dat...  \n",
              "18  Our major claim is that multichannel data can be potential for understanding regulatory processes in collaboration. With our five empirical case examples, we illustrate how triangulating multiple sources of data has potential to advance the theoretical and conceptual progress in social aspects of SRL theory.\\n\\nAuthors show with five empirical cases that multichannel data can be potential for ...  \n",
              "19  \"This study contributes an initial understanding into how different modalities mediate students’ interactions and offers implications for scaffolding peer interactions during multimodal composing processes.\"\\n\\n\"Overall, students were more likely to share ideas and ask questions, and they tended to use quick- response strategies while multimodal composing.\"\\n\\n\"Students’ interview responses al...  \n",
              "20                                                                                                                                                                                                                                Significant impact on the teacher's scaffolding behavior and student engagement (less bordem)\\n\\nSEAT had positive impact on student engagement and was also helpful to teachers.  \n",
              "21  \"We illustrate the approach from the automated analysis of healthcare simulations to train nurses, generating four visual proxies that fuse multimodal data into higher order patterns.\"\\n\\n\"we introduce the concept of collaboration translucence, an approach to make visible selected features of group activity. This is grounded both theoretically (in the physical, epistemic, social and affective ...  \n",
              "22  We found that idling with limited speech (i.e., silence or backchannel feedback only) and without movement was negatively correlated with task performance and with participants’ subjective perceptions of the collaboration. However, being silent and focused during solution execution was positively correlated with task performance. Results illustrate that in some cases, multimodal patterns impro...  \n",
              "23  Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.\\n\\nEvaluating how multimodal features contribute to a model's performance to predict learning gains...  \n",
              "24  This is the first study to evaluate an adaptive learning system for learners with ID based on multi-modal affect recognition. Three separate states were automatically identified, with lower levels of the state labelled “boredom” having the strongest link to learning achievement. Both those labelled “frustration” and “engagement” were positively related to achievement. Our results are in line w...  \n",
              "25  Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoid...  \n",
              "26  \"this research contributes to the relatively sparse literature in multimodal learning analytics by providing a balanced view of the teacher and student interactions with a data set collected in naturalistic home environments.\"\\n\\n\"Firstly, we note a clear causal pathway between the group of variables describing parents’ support and another group representing the child’s cognitive-affective exp...  \n",
              "27  \"Through our user studies, we found that the platform helps fulfill some of those goals by providing capabilities that can spur on amazement and excitement among traditional Minecraft users and novices. We also find that many of the multimodal components, while not immediately intuitive for users, proved to be preferred modes of game play. In this sense, we feel that this tool is moving in the...  \n",
              "28  Overall, our study suggests that coupling style can be operationalised as a multimodal indicator suitable to investigate complex collaboration dynamics in games, thus confirming what has been found in other domains\\n\\n\"Firstly, we found that coupling styles can be used to characterise collaboration in a co-located SG. Consistently with previous research (Isenberg et al, 2010; Niu et al, 2018),...  \n",
              "29  \"Our combined multi-modal learning analytics and interaction analysis methodology enabled us to identify two multi-modal profles of learners who have learning gains and one multi-modal profle of learners who do not have learning gains.\"\\n\\n\"Using this approach, we are able to build the multimodal behavioral profles for each group of learners.\"\\n\\n\"Our combined multi-modal learning analytics an...  \n",
              "30  Results demonstrate how NLP and ML techniques allow us to use different modalities of the same data, voice and transcript, and different modalities of different data sources, voice data from interviews, answers to a goal orientation questionnaire, and answers to open-ended questions about energy, in order to better understand individual differences in students’ performances\\n\\nUsed text and au...  \n",
              "31  \"We conducted a case study to compare the visualizations provided by the system in two different situations: collaborative and competitive activities. The results suggest that the provided visualizations help to identify issues on cognitive contribution, assimilation, self-regulation, and integration of the team members. They could also support teachers to decide whether they must assist a tea...  \n",
              "32  \"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Exploration, increasing confluence during Discovery, and high confluence during Fluency.\"\\n\\n\"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Explorati...  \n",
              "33  \"In this study, we have given special emphasis to the interpersonal metafunction (see Fig. 1), which is crucial to enhance EMI students’ active participation and engagement. The outcomes of this study point to the need to address multimodal interactional competence in professional development programs, as described in Morell et al. (2022) to promote effective lecturing in the diverse EMI scena...  \n",
              "34  \"We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements in the video modality are the most significant unimodal indicators of impasse.\"\\n\\n\"We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements i...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a0afee19-c6b8-4443-a304-dfa90dc6ff43\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type</th>\n",
              "      <th>Data Collection Mediums</th>\n",
              "      <th>Modalities</th>\n",
              "      <th>Analysis Methods</th>\n",
              "      <th>Fusion Types</th>\n",
              "      <th>Publication</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Results</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1118315889</td>\n",
              "      <td>using multimodal learning analytics to identify aspects of collaboration in project-based learning</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS</td>\n",
              "      <td>POSE,PROS</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>CSCL</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"physical aspect of collaborative is an important part of this type of learning and that learning analytics systems can identify features that are relevant for helping researchers, teachers, and learners unpack what is happening.\"\\n\\n\"In this research study, we presented that where the students are looking, the distance between them, the motion of their hands our key features for a learning an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3339002981</td>\n",
              "      <td>estimation of success in collaborative learning based on multimodal learning analytics features</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,LOGS,VIDEO,AUDIO</td>\n",
              "      <td>GAZE,LOGS,PROS,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICALT</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, authors found that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.\\n\\nPredicting the learning gains via classification (bad, ok, good) through gaze, logs, audio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3093310941</td>\n",
              "      <td>embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders</td>\n",
              "      <td>Hiroki Tanaka</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>AUDIO,VIDEO,PPA</td>\n",
              "      <td>POSE,PROS,AFFECT</td>\n",
              "      <td>REG,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>PLOS</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>We identified a positive correlation between overall narrative skills and other non-verbal behaviors. This result indicates that the overall narrative skills were subjectively decided based on other non-verbal behavior skills [22]. Although the automated social skills trainer did not provide feedback regarding narrative structure or eye gaze, these elements were also improved after the trainin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3095923626</td>\n",
              "      <td>a multimodal analysis of making</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR,PPA,INTER</td>\n",
              "      <td>GEST,PPA,EDA,ACT,PROS,QUAL,INTER</td>\n",
              "      <td>STATS,CLUST,QUAL,PATT</td>\n",
              "      <td>EARLY</td>\n",
              "      <td>IJAIED</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"each approach provides different affordances depending on the similarity metric and the dependent variable.\"\\n \"The take-away is that there are several strategies for doing multimodal learning analytics, and that many of these approaches can provide a meaningful glimpse into a complex data set, glimpses that may be difficult to identify using traditional approaches.\"\\n\\n\"Looking across analys...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>85990093</td>\n",
              "      <td>multimodal markers of persuasive speech : designing a virtual debate coach</td>\n",
              "      <td>Volha Petukhova</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>PROS,GEST</td>\n",
              "      <td>CLS,QUAL,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>INTERSPEECH</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"Experiments of different types supported fairly reliable identification of markers from multimodal data, and linking these to assessments of debater confidence level and intensification behaviour.\"\\n\\n\"In line with previous empirical findings, we acknowledge that persuasive speech is rather difficult to characterize. Neverthe- less, based on theoretical and empirical frameworks set up by Gric...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>957160695</td>\n",
              "      <td>virtual debate coach design: assessing multimodal argumentation performance</td>\n",
              "      <td>Volha Petukhova</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>GEST,TRANS,PROS,SURVEY,GAZE</td>\n",
              "      <td>STATS,CLS,QUAL</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICMI</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain corpora and classification procedures resulted in the best performance on an argument structure mining task.\"\\n\\n\"We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1637690235</td>\n",
              "      <td>supervised machine learning in multimodal learning analytics for estimating success in project-based learning</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,PPA,RPA</td>\n",
              "      <td>POSE,GEST,PROS,LOGS,PPA,RPA</td>\n",
              "      <td>REG,CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"In this paper, we show that MMLA and the state-of-the-art computational techniques can be used to generate insights into the \"black box\" of learning in students’ project-based activities. These insights generated from multimodal data can be used to inform teachers about the key features of project-based learning and help them support students appropriately in similar pedagogical approaches.\"\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2181637610</td>\n",
              "      <td>toward using multi-modal learning analytics to support and measure collaboration in co-located dyads</td>\n",
              "      <td>Emma L. Starr</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA,RPA,LOGS</td>\n",
              "      <td>POSE,PROS,PPA,RPA,LOGS</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ICLS</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>While this study was not able to show a clear effect of providing a real-time visualization to support\\ncollaboration, it made many other valuable contributions. First, it showed that simple verbal interventions can\\nhelp participants pay attention to particular aspects of their collaborative behavior (i.e., how much they are\\ntalking and how much space they are providing to their partner). Se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3146393211</td>\n",
              "      <td>mobile mixed reality for experiential learning and simulation in medical and health sciences education</td>\n",
              "      <td>James Birt</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>PPA,INTER</td>\n",
              "      <td>PPA,TRANS</td>\n",
              "      <td>QUAL,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>Information</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>Finally, it was clear from the two intervention studies that the simulation provided by mobile devices, although not as powerful as the desktop-computer-powered Oculus Rift mixed reality or physical simulators, has no negative impacts on the learners. In fact, the simulation improves learners’ competence and skills with the skills intervention showing statistically significant improvements in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3135645357</td>\n",
              "      <td>multimodal teaching analytics: automated extraction of orchestration graphs from wearable sensor data</td>\n",
              "      <td>Luis P. Prieto</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,VIDEO,AUDIO,MOTION</td>\n",
              "      <td>GAZE,PROS,ACT,PIXEL</td>\n",
              "      <td>NET,CLS,STATS,PATT,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"In summary, the results from our evaluation of personalized and general models to automatically extract orchestration graphs highlight the fact that machine learning models can be successfully trained with such multimodal sensor data, using relatively low-level features.\"\\n\\n\"In summary, the results from our evaluation of personalized and general models to automatically extract orchestration ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3783339081</td>\n",
              "      <td>a novel method for the in-depth multimodal analysis of student learning trajectories in intelligent tutoring systems</td>\n",
              "      <td>Ran Liu</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,SCREEN,PPA</td>\n",
              "      <td>LOGS,TRANS,ACT,QUAL,PPA</td>\n",
              "      <td>STATS,REG,QUAL</td>\n",
              "      <td>MID</td>\n",
              "      <td>JLA</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Corse-grained learning trajectories were able to identify key moments during learning that warrented futher analysis with other MMD\\n\\n\"Our results from this analysis showed how students’ early experiences struggling with a novel concept could significantly affect both their entire learning trajectories within an activity and pre-test–post-test measurements of learning gains related to that co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>3796180663</td>\n",
              "      <td>learning linkages: integrating data streams of multiple modalities and timescales</td>\n",
              "      <td>Ran Liu</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,SCREEN,PPA</td>\n",
              "      <td>TRANS,QUAL,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>We presented two empirical studies, collected in classroom studies with two distinct learning technology systems in different contexts (individual and collaborative). Our analyses and findings showcase a few different ways, in which multimodal data sources can enrich our understanding of student learning and provide a more holistic picture.\\n\\nAuthors collected student‐focused screen and webca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2345021698</td>\n",
              "      <td>exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course</td>\n",
              "      <td>René Noël</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,PPA,RPA</td>\n",
              "      <td>RPA,PROS</td>\n",
              "      <td>QUAL,NET,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>Access</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>``There was no difference between the proportion of the session spent being bored or frustrated or in the achievement scores for the two conditions. In order to determine whether the effect on achievement was different depending on length of exposure, participants with a duration less than 60 minutes were excluded from the analysis. However, there was still no significant difference between ac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2497456347</td>\n",
              "      <td>the rap system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2018</td>\n",
              "      <td>Training</td>\n",
              "      <td>AUDIO,VIDEO,PPA,SURVEY</td>\n",
              "      <td>PPA,GAZE,POSE,PROS</td>\n",
              "      <td>CLS,STATS,QUAL</td>\n",
              "      <td>LATE</td>\n",
              "      <td>LAK</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>Feedback generated by RAP is similar to human feedback across several dimensions and similar to other more complex systems\\n\\n\"It revealed an overwhelmingly positive perception of the system especially in the dimensions of perceived usefulness and feedback which were rated as excellent by 65% and 58% of the students respectively. The qualitative analysis helped discover specific issues, on the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>4019205162</td>\n",
              "      <td>introducing low-cost sensors into the classroom settings: improving the assessment in agile practices with multimodal learning analytics</td>\n",
              "      <td>Hector Cornide-Reyes</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,SURVEY,PPA,RPA</td>\n",
              "      <td>SURVEY,TRANS,PPA,RPA</td>\n",
              "      <td>NET,STATS</td>\n",
              "      <td>MID,OTH</td>\n",
              "      <td>Sensors</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>RQ1: Better communication, better collaboration\\nRQ2: Collaborative teams showed lower variability in the estimates of story points (same page)\\nRQ3: Democratic leadership in collaborative groups\\n\\nThe results obtained allowed the detection of important relationships and characteristics of the collaborative and Non-Collaborative groups, with productivity, effort, and predominant personality s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1847468084</td>\n",
              "      <td>computationally augmented ethnography: emotion tracking and learning in museum games</td>\n",
              "      <td>Kit Martin</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA,RPA</td>\n",
              "      <td>TRANS,AFFECT,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ICQE</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>Authors present an initial pilot using these methods in concert to identify key moments in multiple modalities. While the use of constructivist dialogue mapping showed that the users learned during their interaction with Ant Adaption, emotional logging identified alternative moments of learning outside of their analytic framework.\\n\\nThis paper presented a preliminary approach to augment quali...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings\\n\\nWithin heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; mode...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1576545447</td>\n",
              "      <td>artificial intelligence and multimodal data in the service of human decision-making: a case study in debate tutoring</td>\n",
              "      <td>Mutlu Cukurova</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,SURVEY</td>\n",
              "      <td>AFFECT,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>In this study, we used non-transparent prediction models of openSMILE to predict the emotional traits of tutor candidates based on their audio data, and we used transparent logistic regression models to identify exactly what personality, emotion, and experience traits lead to effective debate tutoring skills. Predictive models were very powerful to make sense of complex and nonlinear audio dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3398902089</td>\n",
              "      <td>what multimodal data can tell us about the students’ regulation of their learning process?</td>\n",
              "      <td>Sanna Järvelä</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SENSOR,VIDEO,AUDIO</td>\n",
              "      <td>EDA,AFFECT,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>LAI</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>Our major claim is that multichannel data can be potential for understanding regulatory processes in collaboration. With our five empirical case examples, we illustrate how triangulating multiple sources of data has potential to advance the theoretical and conceptual progress in social aspects of SRL theory.\\n\\nAuthors show with five empirical cases that multichannel data can be potential for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>86191824</td>\n",
              "      <td>examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes</td>\n",
              "      <td>Shiyan Jiang</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SCREEN,INTER,PPA,AUDIO</td>\n",
              "      <td>INTER,QUAL,TRANS</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ILE</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"This study contributes an initial understanding into how different modalities mediate students’ interactions and offers implications for scaffolding peer interactions during multimodal composing processes.\"\\n\\n\"Overall, students were more likely to share ideas and ask questions, and they tended to use quick- response strategies while multimodal composing.\"\\n\\n\"Students’ interview responses al...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>3448122334</td>\n",
              "      <td>investigating the impact of a real-time, multimodal student engagement analytics technology in authentic classrooms</td>\n",
              "      <td>Sinem Aslan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,SCREEN,AUDIO,SURVEY,RPA,PPA,INTER</td>\n",
              "      <td>AFFECT,LOGS,POSE,QUAL,INTER,SURVEY,RPA</td>\n",
              "      <td>QUAL,STATS,CLS</td>\n",
              "      <td>LATE</td>\n",
              "      <td>CHI</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Significant impact on the teacher's scaffolding behavior and student engagement (less bordem)\\n\\nSEAT had positive impact on student engagement and was also helpful to teachers.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1296637108</td>\n",
              "      <td>towards collaboration translucence: giving meaning to multimodal group data</td>\n",
              "      <td>Vanessa Echeverria</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER</td>\n",
              "      <td>POSE,LOGS,TRANS,EDA,ACT,PROS,INTER</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>CHI</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"We illustrate the approach from the automated analysis of healthcare simulations to train nurses, generating four visual proxies that fuse multimodal data into higher order patterns.\"\\n\\n\"we introduce the concept of collaboration translucence, an approach to make visible selected features of group activity. This is grounded both theoretically (in the physical, epistemic, social and affective ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1770989706</td>\n",
              "      <td>focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving</td>\n",
              "      <td>Hana Vrzakova</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,SCREEN,SURVEY,PPA</td>\n",
              "      <td>PROS,ACT,GEST,PPA</td>\n",
              "      <td>STATS,PATT</td>\n",
              "      <td>MID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>We found that idling with limited speech (i.e., silence or backchannel feedback only) and without movement was negatively correlated with task performance and with participants’ subjective perceptions of the collaboration. However, being silent and focused during solution execution was positively correlated with task performance. Results illustrate that in some cases, multimodal patterns impro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3051560548</td>\n",
              "      <td>temporal analysis of multimodal data to predict collaborative learning outcomes</td>\n",
              "      <td>Jennifer K. Olsen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,EYE</td>\n",
              "      <td>GAZE,LOGS,PROS,TRANS,QUAL</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.\\n\\nEvaluating how multimodal features contribute to a model's performance to predict learning gains...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>3796643912</td>\n",
              "      <td>an evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities</td>\n",
              "      <td>Penelope J. Standen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,RPA</td>\n",
              "      <td>AFFECT,POSE,LOGS,RPA,GAZE,PROS,GEST</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>HUM, OTH, STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>This is the first study to evaluate an adaptive learning system for learners with ID based on multi-modal affect recognition. Three separate states were automatically identified, with lower levels of the state labelled “boredom” having the strongest link to learning achievement. Both those labelled “frustration” and “engagement” were positively related to achievement. Our results are in line w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MF</td>\n",
              "      <td>Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1426267857</td>\n",
              "      <td>affect, support, and personal factors: multimodal causal models of one-on-one coaching</td>\n",
              "      <td>Lujie Karen Chen</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,SURVEY</td>\n",
              "      <td>PROS,GAZE,TRANS,AFFECT,SURVEY</td>\n",
              "      <td>STATS,NET</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JEDM</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"this research contributes to the relatively sparse literature in multimodal learning analytics by providing a balanced view of the teacher and student interactions with a data set collected in naturalistic home environments.\"\\n\\n\"Firstly, we note a clear causal pathway between the group of variables describing parents’ support and another group representing the child’s cognitive-affective exp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>666050348</td>\n",
              "      <td>multicraft: a multimodal interface for supporting and studying learning in minecraft</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS</td>\n",
              "      <td>PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>HCII</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"Through our user studies, we found that the platform helps fulfill some of those goals by providing capabilities that can spur on amazement and excitement among traditional Minecraft users and novices. We also find that many of the multimodal components, while not immediately intuitive for users, proved to be preferred modes of game play. In this sense, we feel that this tool is moving in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>518268671</td>\n",
              "      <td>using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game</td>\n",
              "      <td>María Ximena López</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,LOGS,AUDIO,VIDEO</td>\n",
              "      <td>LOGS,SURVEY,GAZE,PROS</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ECGBL</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>Overall, our study suggests that coupling style can be operationalised as a multimodal indicator suitable to investigate complex collaboration dynamics in games, thus confirming what has been found in other domains\\n\\n\"Firstly, we found that coupling styles can be used to characterise collaboration in a co-located SG. Consistently with previous research (Isenberg et al, 2010; Niu et al, 2018),...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2273914836</td>\n",
              "      <td>many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities</td>\n",
              "      <td>Jauwairia Nasir</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,PPA,SURVEY</td>\n",
              "      <td>PROS,AFFECT,GAZE,TRANS,LOGS</td>\n",
              "      <td>STATS,QUAL,CLUST,CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJCSCL</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"Our combined multi-modal learning analytics and interaction analysis methodology enabled us to identify two multi-modal profles of learners who have learning gains and one multi-modal profle of learners who do not have learning gains.\"\\n\\n\"Using this approach, we are able to build the multimodal behavioral profles for each group of learners.\"\\n\\n\"Our combined multi-modal learning analytics an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>32184286</td>\n",
              "      <td>once more with feeling: emotions in multimodal learning analytics</td>\n",
              "      <td>Marcus Kubsch</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,PPA,AUDIO</td>\n",
              "      <td>INTER,SURVEY,TRANS,PROS,AFFECT</td>\n",
              "      <td>CLS,REG,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MMLA Handbook</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>Results demonstrate how NLP and ML techniques allow us to use different modalities of the same data, voice and transcript, and different modalities of different data sources, voice data from interviews, answers to a goal orientation questionnaire, and answers to open-ended questions about energy, in order to better understand individual differences in students’ performances\\n\\nUsed text and au...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>2609260641</td>\n",
              "      <td>visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication</td>\n",
              "      <td>René Noël</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,RPA,INTER</td>\n",
              "      <td>PROS,POSE,RPA,INTER,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>DAMLE</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>PROF, UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"We conducted a case study to compare the visualizations provided by the system in two different situations: collaborative and competitive activities. The results suggest that the provided visualizations help to identify issues on cognitive contribution, assimilation, self-regulation, and integration of the team members. They could also support teachers to decide whether they must assist a tea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1345598079</td>\n",
              "      <td>intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning</td>\n",
              "      <td>Sofia Tancredi</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,VIDEO,AUDIO,INTER</td>\n",
              "      <td>GAZE,GEST,TRANS,POSE,INTER</td>\n",
              "      <td>PATT,QUAL,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MMLA Handbook</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Exploration, increasing confluence during Discovery, and high confluence during Fluency.\"\\n\\n\"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Explorati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>2155422499</td>\n",
              "      <td>a multimodal analysis of pair work engagement episodes: implications for emi lecturer training</td>\n",
              "      <td>Teresa Morell</td>\n",
              "      <td>2022</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>TRANS,PPA,QUAL,POSE,ACT</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>JEAP</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"In this study, we have given special emphasis to the interpersonal metafunction (see Fig. 1), which is crucial to enhance EMI students’ active participation and engagement. The outcomes of this study point to the need to address multimodal interactional competence in professional development programs, as described in Morell et al. (2022) to promote effective lecturing in the diverse EMI scena...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>3754172825</td>\n",
              "      <td>detecting impasse during collaborative problem solving with multimodal learning analytics</td>\n",
              "      <td>Yingbo Ma</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>TRANS,PROS,SPECT,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements in the video modality are the most significant unimodal indicators of impasse.\"\\n\\n\"We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a0afee19-c6b8-4443-a304-dfa90dc6ff43')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a0afee19-c6b8-4443-a304-dfa90dc6ff43 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a0afee19-c6b8-4443-a304-dfa90dc6ff43');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8d30caa0-fd09-461f-b4da-f7b27e7123ab\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8d30caa0-fd09-461f-b4da-f7b27e7123ab')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8d30caa0-fd09-461f-b4da-f7b27e7123ab button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_9492fa6a-2853-4cb7-a6c6-95c17bc7022a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_nlp')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9492fa6a-2853-4cb7-a6c6-95c17bc7022a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_nlp');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = pd.read_csv(\"drive/MyDrive/Clayton/20230420_MMLTE/S18.csv\")\n",
        "df_all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZblhdTw2wTl5",
        "outputId": "1b5dc51e-ed23-4bce-81f7-f44e73176fba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          UUID  \\\n",
              "0    818492192   \n",
              "1   3408664396   \n",
              "2   1118315889   \n",
              "3   3339002981   \n",
              "4   1609706685   \n",
              "5   3093310941   \n",
              "6   3095923626   \n",
              "7   2456887548   \n",
              "8   1374035721   \n",
              "9     85990093   \n",
              "10   957160695   \n",
              "11  1637690235   \n",
              "12  1886134458   \n",
              "13  2181637610   \n",
              "14   483140962   \n",
              "15  3146393211   \n",
              "16  3308658121   \n",
              "17  3135645357   \n",
              "18  3309250332   \n",
              "19  2836996318   \n",
              "20  3783339081   \n",
              "21  3796180663   \n",
              "22  2345021698   \n",
              "23   804659204   \n",
              "24  2497456347   \n",
              "25  2070224207   \n",
              "26  4019205162   \n",
              "27  1847468084   \n",
              "28  1326191931   \n",
              "29  4278392816   \n",
              "30  1576545447   \n",
              "31   853680639   \n",
              "32  3398902089   \n",
              "33    86191824   \n",
              "34  3448122334   \n",
              "35  1296637108   \n",
              "36  1019093033   \n",
              "37  1581261659   \n",
              "38  1598166515   \n",
              "39   205660768   \n",
              "40  3009548670   \n",
              "41  1770989706   \n",
              "42  3051560548   \n",
              "43   147203129   \n",
              "44  2000036002   \n",
              "45  2055153191   \n",
              "46  3796643912   \n",
              "47  2879332689   \n",
              "48  1877483551   \n",
              "49  3637456466   \n",
              "50  2936220551   \n",
              "51  2634033325   \n",
              "52   123412197   \n",
              "53  3809293172   \n",
              "54  1763513559   \n",
              "55  4035649049   \n",
              "56  3625722965   \n",
              "57  1426267857   \n",
              "58   666050348   \n",
              "59   518268671   \n",
              "60  3660066725   \n",
              "61  3856280479   \n",
              "62  4277812050   \n",
              "63   566043228   \n",
              "64  1315379489   \n",
              "65  1469065963   \n",
              "66   433919853   \n",
              "67  2273914836   \n",
              "68    32184286   \n",
              "69  2609260641   \n",
              "70  1345598079   \n",
              "71  2155422499   \n",
              "72  3754172825   \n",
              "\n",
              "                                                                                                                                                         Title  \\\n",
              "0                          understanding student learning trajectories using multimodal learning analytics within an embodied-interaction learning environment   \n",
              "1                                                                                                 multimodal student engagement recognition in prosocial games   \n",
              "2                                                           using multimodal learning analytics to identify aspects of collaboration in project-based learning   \n",
              "3                                                              estimation of success in collaborative learning based on multimodal learning analytics features   \n",
              "4                                      learning pulse: a machine learning approach for predicting performance in self-regulated learning using multimodal data   \n",
              "5                                      embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders   \n",
              "6                                                                                                                              a multimodal analysis of making   \n",
              "7                                                                       an unobtrusive and multimodal approach for behavioral engagement detection of students   \n",
              "8                                                                       attentivelearner2: a multimodal approach for improving mooc learning on mobile devices   \n",
              "9                                                                                   multimodal markers of persuasive speech : designing a virtual debate coach   \n",
              "10                                                                                 virtual debate coach design: assessing multimodal argumentation performance   \n",
              "11                                               supervised machine learning in multimodal learning analytics for estimating success in project-based learning   \n",
              "12                                                                        personalizing computer science education by leveraging multimodal learning analytics   \n",
              "13                                                        toward using multi-modal learning analytics to support and measure collaboration in co-located dyads   \n",
              "14                                                           investigating multimodal affect sensing in an affective tutoring system using unobtrusive sensors   \n",
              "15                                                      mobile mixed reality for experiential learning and simulation in medical and health sciences education   \n",
              "16                                                                             exploring collaboration using motion sensors and multi-modal learning analytics   \n",
              "17                                                       multimodal teaching analytics: automated extraction of orchestration graphs from wearable sensor data   \n",
              "18                                                      (dis)engagement matters: identifying efficacious learning practices with multimodal learning analytics   \n",
              "19                                                                    predicting learners' emotions in mobile mooc learning via a multimodal intelligent tutor   \n",
              "20                                        a novel method for the in-depth multimodal analysis of student learning trajectories in intelligent tutoring systems   \n",
              "21                                                                           learning linkages: integrating data streams of multiple modalities and timescales   \n",
              "22                           exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course   \n",
              "23                                                                          towards smart educational recommendations with reinforcement learning in classroom   \n",
              "24                                               the rap system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors   \n",
              "25                                                                               detecting medical simulation errors with machine learning and multimodal data   \n",
              "26                    introducing low-cost sensors into the classroom settings: improving the assessment in agile practices with multimodal learning analytics   \n",
              "27                                                                        computationally augmented ethnography: emotion tracking and learning in museum games   \n",
              "28                                                                                                     multimodal learning analytics in a laboratory classroom   \n",
              "29                                                                                            multimodal data as a means to understand the learning experience   \n",
              "30                                        artificial intelligence and multimodal data in the service of human decision-making: a case study in debate tutoring   \n",
              "31                                                                sensor-based data fusion for multimodal affect detection in game-based learning environments   \n",
              "32                                                                  what multimodal data can tell us about the students’ regulation of their learning process?   \n",
              "33                                   examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes   \n",
              "34                                         investigating the impact of a real-time, multimodal student engagement analytics technology in authentic classrooms   \n",
              "35                                                                                 towards collaboration translucence: giving meaning to multimodal group data   \n",
              "36                                                                 prime: block-wise missingness handling for multi-modalities in intelligent tutoring systems   \n",
              "37                                                                early prediction of visitor engagement in science museums with multimodal learning analytics   \n",
              "38                                                                                                       multimodal learning analytics for game-based learning   \n",
              "39                                                                   multimodal learning analytics to investigate cognitive load during online problem solving   \n",
              "40                                                                                                            real-time multimodal feedback with the cpr tutor   \n",
              "41                                                  focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving   \n",
              "42                                                                             temporal analysis of multimodal data to predict collaborative learning outcomes   \n",
              "43                                                           multimodal learning analytics to inform learning design: lessons learned from computing education   \n",
              "44                                                                       predicting learners’ effortful behaviour in adaptive assessment using multimodal data   \n",
              "45                                                        round or rectangular tables for collaborative problem solving? a multimodal learning analytics study   \n",
              "46                             an evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities   \n",
              "47                                                                    from data to insights: a layered storytelling approach for multimodal learning analytics   \n",
              "48                                                                      motion-based educational games: using multi-modal data to predict player’s performance   \n",
              "49                                    impact of inquiry interventions on students in e-learning and classroom environments using affective computing framework   \n",
              "50                                          multi-source and multimodal data fusion for predicting academic performance in blended learning university courses   \n",
              "51                                                 controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "52                                                                          utilizing multimodal data through fsqca to explain engagement in adaptive learning   \n",
              "53                   blending learning analytics and embodied design to model students' comprehension of measurement using their actions, speech, and gestures   \n",
              "54                                                                                                keep me in the loop: real-time feedback with multimodal data   \n",
              "55                                                                          storytelling with learner data: guiding student reflection on multimodal team data   \n",
              "56                                                            table tennis tutor: forehand strokes classification based on multimodal data and neural networks   \n",
              "57                                                                      affect, support, and personal factors: multimodal causal models of one-on-one coaching   \n",
              "58                                                                        multicraft: a multimodal interface for supporting and studying learning in minecraft   \n",
              "59                                                   using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game   \n",
              "60                             children's play and problem solving in motion-based educational games: synergies between human annotations and multi-modal data   \n",
              "61                                        children's play and problem-solving in motion-based learning technologies using a multi-modal mixed methods approach   \n",
              "62  improving prediction of students' performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources   \n",
              "63                                                                  automatic student engagement in online learning environment based on neural turing machine   \n",
              "64                                                                                          multimodal engagement analysis from facial videos in the classroom   \n",
              "65                                             examining socially shared regulation and shared physiological arousal events with multimodal learning analytics   \n",
              "66                                                                                          understanding fun in learning to code: a multi-modal data approach   \n",
              "67                               many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities   \n",
              "68                                                                                           once more with feeling: emotions in multimodal learning analytics   \n",
              "69                                                visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication   \n",
              "70                       intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning   \n",
              "71                                                              a multimodal analysis of pair work engagement episodes: implications for emi lecturer training   \n",
              "72                                                                   detecting impasse during collaborative problem solving with multimodal learning analytics   \n",
              "\n",
              "                  First Author  Year Environment Type  \\\n",
              "0            Alejandro Andrade  2017         Learning   \n",
              "1           Athanasios Psaltis  2017         Learning   \n",
              "2                Daniel Spikol  2017         Learning   \n",
              "3                Daniel Spikol  2017         Learning   \n",
              "4             Daniele Di Mitri  2017         Training   \n",
              "5                Hiroki Tanaka  2017         Training   \n",
              "6              Marcelo Worsley  2017         Learning   \n",
              "7                   Nese Alyuz  2017         Learning   \n",
              "8                  Phuong Pham  2017         Learning   \n",
              "9              Volha Petukhova  2017         Training   \n",
              "10             Volha Petukhova  2017         Training   \n",
              "11               Daniel Spikol  2018         Learning   \n",
              "12                David Azcona  2018         Learning   \n",
              "13               Emma L. Starr  2018         Learning   \n",
              "14               Hua Leong Fwa  2018         Learning   \n",
              "15                  James Birt  2018         Learning   \n",
              "16            Joseph M. Reilly  2018         Learning   \n",
              "17              Luis P. Prieto  2018         Learning   \n",
              "18             Marcelo Worsley  2018         Learning   \n",
              "19                 Phuong Pham  2018         Learning   \n",
              "20                     Ran Liu  2018         Learning   \n",
              "21                     Ran Liu  2018         Learning   \n",
              "22                   René Noël  2018         Learning   \n",
              "23                      Su Liu  2018         Learning   \n",
              "24                Xavier Ochoa  2018         Training   \n",
              "25            Daniele Di Mitri  2019         Training   \n",
              "26        Hector Cornide-Reyes  2019         Learning   \n",
              "27                  Kit Martin  2019         Learning   \n",
              "28       Man Ching Esther Chan  2019         Learning   \n",
              "29           Michail Giannakos  2019         Training   \n",
              "30              Mutlu Cukurova  2019         Learning   \n",
              "31            Nathan Henderson  2019         Training   \n",
              "32               Sanna Järvelä  2019         Learning   \n",
              "33                Shiyan Jiang  2019         Learning   \n",
              "34                 Sinem Aslan  2019         Learning   \n",
              "35          Vanessa Echeverria  2019         Training   \n",
              "36                     Xi Yang  2019         Learning   \n",
              "37              Andrew Emerson  2020         Learning   \n",
              "38              Andrew Emerson  2020         Learning   \n",
              "39         Charlotte Larmuseau  2020         Learning   \n",
              "40            Daniele Di Mitri  2020         Training   \n",
              "41               Hana Vrzakova  2020         Learning   \n",
              "42           Jennifer K. Olsen  2020         Learning   \n",
              "43         Katerina Mangaroska  2020         Learning   \n",
              "44              Kshitij Sharma  2020         Learning   \n",
              "45              Milica Vujovic  2020         Learning   \n",
              "46         Penelope J. Standen  2020         Learning   \n",
              "47  Roberto Martinez-Maldonado  2020         Training   \n",
              "48          Serena Lee-Cultura  2020         Learning   \n",
              "49                T. S. Ashwin  2020         Learning   \n",
              "50               Wilson Chango  2020         Learning   \n",
              "51                Xavier Ochoa  2020         Training   \n",
              "52      Zacharoula Papamitsiou  2020         Learning   \n",
              "53            Avery H. Closser  2021         Learning   \n",
              "54            Daniele Di Mitri  2021         Training   \n",
              "55      Gloria Fernández-Nieto  2021         Training   \n",
              "56  Khaleel Asyraaf Mat Sanusi  2021         Training   \n",
              "57            Lujie Karen Chen  2021         Learning   \n",
              "58             Marcelo Worsley  2021         Learning   \n",
              "59          María Ximena López  2021         Learning   \n",
              "60          Serena Lee-Cultura  2021         Learning   \n",
              "61          Serena Lee-Cultura  2021         Learning   \n",
              "62               Wilson Chango  2021         Learning   \n",
              "63                 Xiaoyang Ma  2021         Learning   \n",
              "64                  Ömer Sümer  2021         Learning   \n",
              "65                 Andy Nguyen  2022         Learning   \n",
              "66             Gabriella Tisza  2022         Learning   \n",
              "67             Jauwairia Nasir  2022         Learning   \n",
              "68               Marcus Kubsch  2022         Learning   \n",
              "69                   René Noël  2022         Learning   \n",
              "70              Sofia Tancredi  2022         Learning   \n",
              "71               Teresa Morell  2022         Training   \n",
              "72                   Yingbo Ma  2022         Learning   \n",
              "\n",
              "                          Data Collection Mediums  \\\n",
              "0                            VIDEO,LOGS,INTER,PPA   \n",
              "1                                      VIDEO,LOGS   \n",
              "2                                VIDEO,AUDIO,LOGS   \n",
              "3                            EYE,LOGS,VIDEO,AUDIO   \n",
              "4                          SENSOR,LOGS,MOTION,PPA   \n",
              "5                                 AUDIO,VIDEO,PPA   \n",
              "6                    VIDEO,AUDIO,SENSOR,PPA,INTER   \n",
              "7                           LOGS,VIDEO,SCREEN,PPA   \n",
              "8                                    VIDEO,SURVEY   \n",
              "9                                     VIDEO,AUDIO   \n",
              "10                                    VIDEO,AUDIO   \n",
              "11                       VIDEO,AUDIO,LOGS,PPA,RPA   \n",
              "12                                       LOGS,PPA   \n",
              "13                       VIDEO,AUDIO,PPA,RPA,LOGS   \n",
              "14                                     VIDEO,LOGS   \n",
              "15                                      PPA,INTER   \n",
              "16                                VIDEO,AUDIO,PPA   \n",
              "17                         EYE,VIDEO,AUDIO,MOTION   \n",
              "18                                      VIDEO,PPA   \n",
              "19                                          VIDEO   \n",
              "20                          LOGS,AUDIO,SCREEN,PPA   \n",
              "21                    VIDEO,AUDIO,LOGS,SCREEN,PPA   \n",
              "22                                  AUDIO,PPA,RPA   \n",
              "23                                   VIDEO,SENSOR   \n",
              "24                         AUDIO,VIDEO,PPA,SURVEY   \n",
              "25                              VIDEO,MOTION,LOGS   \n",
              "26                           AUDIO,SURVEY,PPA,RPA   \n",
              "27                            VIDEO,AUDIO,PPA,RPA   \n",
              "28                                    VIDEO,AUDIO   \n",
              "29                          LOGS,EYE,SENSOR,VIDEO   \n",
              "30                                   AUDIO,SURVEY   \n",
              "31                               VIDEO,SENSOR,RPA   \n",
              "32                             SENSOR,VIDEO,AUDIO   \n",
              "33                         SCREEN,INTER,PPA,AUDIO   \n",
              "34   VIDEO,LOGS,SCREEN,AUDIO,SURVEY,RPA,PPA,INTER   \n",
              "35           VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER   \n",
              "36                                  PPA,VIDEO,EYE   \n",
              "37                                 VIDEO,EYE,LOGS   \n",
              "38                                 VIDEO,LOGS,EYE   \n",
              "39                                     PPA,SENSOR   \n",
              "40                       LOGS,VIDEO,SENSOR,MOTION   \n",
              "41                  AUDIO,VIDEO,SCREEN,SURVEY,PPA   \n",
              "42                                 LOGS,AUDIO,EYE   \n",
              "43                          VIDEO,EYE,SENSOR,LOGS   \n",
              "44                               VIDEO,EYE,SENSOR   \n",
              "45                                   VIDEO,MOTION   \n",
              "46                           VIDEO,AUDIO,LOGS,RPA   \n",
              "47                   LOGS,MOTION,SENSOR,RPA,VIDEO   \n",
              "48                               VIDEO,EYE,SENSOR   \n",
              "49                                      VIDEO,PPA   \n",
              "50                                 VIDEO,LOGS,PPA   \n",
              "51                                VIDEO,AUDIO,PPA   \n",
              "52                   SURVEY,LOGS,EYE,SENSOR,VIDEO   \n",
              "53                                    VIDEO,AUDIO   \n",
              "54                SURVEY,LOGS,VIDEO,SENSOR,MOTION   \n",
              "55                                SENSOR,LOGS,RPA   \n",
              "56                             VIDEO,MOTION,INTER   \n",
              "57                             AUDIO,VIDEO,SURVEY   \n",
              "58  AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS   \n",
              "59                        SURVEY,LOGS,AUDIO,VIDEO   \n",
              "60                          VIDEO,SENSOR,EYE,LOGS   \n",
              "61                          VIDEO,SENSOR,EYE,LOGS   \n",
              "62                             LOGS,VIDEO,EYE,PPA   \n",
              "63                                          VIDEO   \n",
              "64                                          VIDEO   \n",
              "65                             VIDEO,AUDIO,SENSOR   \n",
              "66                               SENSOR,VIDEO,PPA   \n",
              "67                    VIDEO,AUDIO,LOGS,PPA,SURVEY   \n",
              "68                               SURVEY,PPA,AUDIO   \n",
              "69                          AUDIO,VIDEO,RPA,INTER   \n",
              "70                          EYE,VIDEO,AUDIO,INTER   \n",
              "71                                VIDEO,AUDIO,PPA   \n",
              "72                                    VIDEO,AUDIO   \n",
              "\n",
              "                                           Modalities  \\\n",
              "0                            GAZE,LOGS,INTER,PPA,GEST   \n",
              "1                                    POSE,AFFECT,LOGS   \n",
              "2                                           POSE,PROS   \n",
              "3                                 GAZE,LOGS,PROS,POSE   \n",
              "4                                    PULSE,ACT,AFFECT   \n",
              "5                                    POSE,PROS,AFFECT   \n",
              "6                    GEST,PPA,EDA,ACT,PROS,QUAL,INTER   \n",
              "7                                AFFECT,POSE,LOGS,PPA   \n",
              "8                                 PULSE,AFFECT,SURVEY   \n",
              "9                                           PROS,GEST   \n",
              "10                        GEST,TRANS,PROS,SURVEY,GAZE   \n",
              "11                        POSE,GEST,PROS,LOGS,PPA,RPA   \n",
              "12                                           LOGS,PPA   \n",
              "13                             POSE,PROS,PPA,RPA,LOGS   \n",
              "14                                      POSE,ACT,LOGS   \n",
              "15                                          PPA,TRANS   \n",
              "16                                  PPA,RPA,POSE,GEST   \n",
              "17                                GAZE,PROS,ACT,PIXEL   \n",
              "18                                      GEST,QUAL,PPA   \n",
              "19                                       AFFECT,PULSE   \n",
              "20                            LOGS,TRANS,ACT,QUAL,PPA   \n",
              "21                                     TRANS,QUAL,PPA   \n",
              "22                                           RPA,PROS   \n",
              "23                                  PULSE,AFFECT,GAZE   \n",
              "24                                 PPA,GAZE,POSE,PROS   \n",
              "25                                          POSE,LOGS   \n",
              "26                               SURVEY,TRANS,PPA,RPA   \n",
              "27                                  TRANS,AFFECT,QUAL   \n",
              "28                                     POSE,GAZE,PROS   \n",
              "29               EEG,GAZE,LOGS,PULSE,EDA,TEMP,BP,POSE   \n",
              "30                                        AFFECT,LOGS   \n",
              "31                                    POSE,EDA,AFFECT   \n",
              "32                                    EDA,AFFECT,QUAL   \n",
              "33                                   INTER,QUAL,TRANS   \n",
              "34             AFFECT,LOGS,POSE,QUAL,INTER,SURVEY,RPA   \n",
              "35                 POSE,LOGS,TRANS,EDA,ACT,PROS,INTER   \n",
              "36                                   LOGS,AFFECT,GAZE   \n",
              "37                         POSE,GEST,AFFECT,GAZE,LOGS   \n",
              "38                               AFFECT,GAZE,LOGS,PPA   \n",
              "39                                      PPA,PULSE,EDA   \n",
              "40                                      POSE,EMG,GEST   \n",
              "41                                  PROS,ACT,GEST,PPA   \n",
              "42                          GAZE,LOGS,PROS,TRANS,QUAL   \n",
              "43                    LOGS,GAZE,EDA,PULSE,AFFECT,TEMP   \n",
              "44                     EDA,TEMP,PULSE,EEG,GAZE,AFFECT   \n",
              "45                                      POSE,GEST,ACT   \n",
              "46                AFFECT,POSE,LOGS,RPA,GAZE,PROS,GEST   \n",
              "47                                  POSE,EDA,LOGS,RPA   \n",
              "48                           PULSE,TEMP,EDA,GAZE,POSE   \n",
              "49                                   AFFECT,POSE,GEST   \n",
              "50                                  LOGS,POSE,RPA,ACT   \n",
              "51                                      POSE,PROS,PPA   \n",
              "52      PULSE,AFFECT,EEG,GAZE,LOGS,BP,TEMP,EDA,SURVEY   \n",
              "53                                                RPA   \n",
              "54                               POSE,EMG,GEST,SURVEY   \n",
              "55                                EDA,LOGS,RPA,AFFECT   \n",
              "56                                POSE,GEST,ACT,INTER   \n",
              "57                      PROS,GAZE,TRANS,AFFECT,SURVEY   \n",
              "58             PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS   \n",
              "59                              LOGS,SURVEY,GAZE,PROS   \n",
              "60     ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP   \n",
              "61  ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP,BP   \n",
              "62                               AFFECT,LOGS,GAZE,PPA   \n",
              "63                                          POSE,GAZE   \n",
              "64                                        POSE,AFFECT   \n",
              "65                                           QUAL,EDA   \n",
              "66                           TEMP,PULSE,EDA,BP,AFFECT   \n",
              "67                        PROS,AFFECT,GAZE,TRANS,LOGS   \n",
              "68                     INTER,SURVEY,TRANS,PROS,AFFECT   \n",
              "69                           PROS,POSE,RPA,INTER,QUAL   \n",
              "70                         GAZE,GEST,TRANS,POSE,INTER   \n",
              "71                            TRANS,PPA,QUAL,POSE,ACT   \n",
              "72                         TRANS,PROS,SPECT,GAZE,POSE   \n",
              "\n",
              "           Analysis Methods Fusion Types    Publication Environment Setting  \\\n",
              "0                CLUST,QUAL       HYBRID            LAK                BLND   \n",
              "1                       CLS         LATE        T-CIAIG                BLND   \n",
              "2                       REG          MID           CSCL                PHYS   \n",
              "3                       CLS          MID          ICALT                VIRT   \n",
              "4                       REG          MID            LAK                BLND   \n",
              "5                 REG,STATS          MID           PLOS                VIRT   \n",
              "6     STATS,CLUST,QUAL,PATT        EARLY         IJAIED                PHYS   \n",
              "7                       CLS       HYBRID            MIE                VIRT   \n",
              "8                       CLS          MID           AIED                VIRT   \n",
              "9            CLS,QUAL,STATS          MID    INTERSPEECH                PHYS   \n",
              "10           STATS,CLS,QUAL          MID           ICMI                PHYS   \n",
              "11                  REG,CLS          MID           JCAL                BLND   \n",
              "12                CLS,STATS          MID            FIE                VIRT   \n",
              "13               STATS,QUAL          OTH           ICLS                BLND   \n",
              "14                      CLS          MID           PPIG                VIRT   \n",
              "15               QUAL,STATS          OTH    Information                BLND   \n",
              "16         CLUST,PATT,STATS          OTH            EDM                BLND   \n",
              "17  NET,CLS,STATS,PATT,QUAL       HYBRID           JCAL                PHYS   \n",
              "18           CLUST,CLS,QUAL       HYBRID            LAK                PHYS   \n",
              "19                      REG         LATE            ITS                VIRT   \n",
              "20           STATS,REG,QUAL          MID            JLA                VIRT   \n",
              "21                CLS,STATS          MID           JCAL                VIRT   \n",
              "22           QUAL,NET,STATS          OTH         Access                PHYS   \n",
              "23                      CLS          MID           TALE                UNSP   \n",
              "24           CLS,STATS,QUAL         LATE            LAK                BLND   \n",
              "25                      CLS          MID           CAIM                BLND   \n",
              "26                NET,STATS      MID,OTH        Sensors                PHYS   \n",
              "27                     QUAL          OTH           ICQE                VIRT   \n",
              "28                CLS,CLUST         LATE         MLPALA                PHYS   \n",
              "29                REG,STATS       HYBRID           IJIM                VIRT   \n",
              "30                      CLS          MID           BJET                UNSP   \n",
              "31                      CLS       HYBRID            EDM                VIRT   \n",
              "32                     QUAL          OTH            LAI                BLND   \n",
              "33                     QUAL          OTH            ILE                VIRT   \n",
              "34           QUAL,STATS,CLS         LATE            CHI                VIRT   \n",
              "35                     QUAL          OTH            CHI                PHYS   \n",
              "36                CLS,STATS       HYBRID            MMM                VIRT   \n",
              "37                      REG          MID           ICMI                VIRT   \n",
              "38                CLS,STATS          MID           BJET                VIRT   \n",
              "39                STATS,CLS        EARLY           BJET                VIRT   \n",
              "40                      CLS       HYBRID           AIED                PHYS   \n",
              "41               STATS,PATT          MID            LAK                VIRT   \n",
              "42                      REG          MID           BJET                VIRT   \n",
              "43                      CLS       HYBRID            JLA                VIRT   \n",
              "44           CLUST,CLS,PATT          MID            LAK                VIRT   \n",
              "45               STATS,QUAL          OTH           BJET                BLND   \n",
              "46                CLS,STATS       HYBRID           BJET                VIRT   \n",
              "47                     QUAL          OTH            CHI                BLND   \n",
              "48                      CLS          MID            COG                BLND   \n",
              "49           CLS,STATS,PATT          MID          UMUAI          PHYS, VIRT   \n",
              "50           CLS,QUAL,STATS     MID,LATE            CEE                BLND   \n",
              "51                    STATS          OTH           BJET                BLND   \n",
              "52                     PATT       HYBRID            TLT                VIRT   \n",
              "53                CLUST,REG          MID          IJCCI                PHYS   \n",
              "54           CLS,QUAL,STATS       HYBRID         IJAIED                PHYS   \n",
              "55                     QUAL          OTH            TLT                BLND   \n",
              "56                 CLS,QUAL       HYBRID        Sensors                PHYS   \n",
              "57                STATS,NET       HYBRID           JEDM                PHYS   \n",
              "58                     QUAL          OTH           HCII                VIRT   \n",
              "59                    STATS          OTH          ECGBL                BLND   \n",
              "60               STATS,QUAL          OTH            IDC                BLND   \n",
              "61           STATS,QUAL,CLS       HYBRID          IJCCI                BLND   \n",
              "62                      CLS  HYBRID,LATE           JCHE                VIRT   \n",
              "63                      CLS          MID          IJIET                VIRT   \n",
              "64                      CLS   EARLY,LATE            TAC                PHYS   \n",
              "65           PATT,CLS,CLUST       HYBRID           BJET                PHYS   \n",
              "66                REG,STATS          MID            IDC                VIRT   \n",
              "67     STATS,QUAL,CLUST,CLS       HYBRID         IJCSCL                BLND   \n",
              "68            CLS,REG,STATS          OTH  MMLA Handbook                PHYS   \n",
              "69                     QUAL          OTH          DAMLE                PHYS   \n",
              "70          PATT,QUAL,STATS          OTH  MMLA Handbook                VIRT   \n",
              "71                     QUAL          OTH           JEAP                PHYS   \n",
              "72                      CLS       HYBRID            LAK                VIRT   \n",
              "\n",
              "   Environment Subject Participant Structure Didactic Nature  \\\n",
              "0                 STEM                   IND           INSTR   \n",
              "1                  HUM                   IND             INF   \n",
              "2                 STEM                 MULTI           INSTR   \n",
              "3                 STEM                 MULTI           INSTR   \n",
              "4                 UNSP                   IND            UNSP   \n",
              "5                  HUM                   IND           TRAIN   \n",
              "6                 STEM                 MULTI             INF   \n",
              "7                 STEM                   IND           INSTR   \n",
              "8                 STEM                   IND           INSTR   \n",
              "9                  HUM                 MULTI           TRAIN   \n",
              "10                 HUM                 MULTI           TRAIN   \n",
              "11                STEM                 MULTI           INSTR   \n",
              "12                STEM                   IND           INSTR   \n",
              "13                STEM                 MULTI             INF   \n",
              "14                STEM                   IND           INSTR   \n",
              "15                STEM                   IND           INSTR   \n",
              "16                STEM                 MULTI           INSTR   \n",
              "17                STEM                 MULTI           INSTR   \n",
              "18                STEM                 MULTI           INSTR   \n",
              "19                STEM                   IND           INSTR   \n",
              "20                STEM                   IND           INSTR   \n",
              "21                STEM            IND, MULTI           INSTR   \n",
              "22                STEM                 MULTI             INF   \n",
              "23                UNSP                   IND           INSTR   \n",
              "24                 HUM                   IND           TRAIN   \n",
              "25                 PSY                   IND           TRAIN   \n",
              "26                STEM                 MULTI           INSTR   \n",
              "27                STEM                 MULTI             INF   \n",
              "28                STEM            IND, MULTI           INSTR   \n",
              "29                 PSY                   IND             INF   \n",
              "30                 HUM                   IND           TRAIN   \n",
              "31                STEM                   IND           TRAIN   \n",
              "32                STEM                 MULTI           INSTR   \n",
              "33                STEM                 MULTI           INSTR   \n",
              "34                STEM                   IND           INSTR   \n",
              "35                STEM                 MULTI           TRAIN   \n",
              "36                STEM                   IND           INSTR   \n",
              "37                STEM                   IND             INF   \n",
              "38                STEM                   IND             INF   \n",
              "39                STEM                   IND           INSTR   \n",
              "40                 PSY                   IND           TRAIN   \n",
              "41                STEM                 MULTI           INSTR   \n",
              "42                STEM                 MULTI           INSTR   \n",
              "43                STEM                   IND           INSTR   \n",
              "44                STEM                   IND           INSTR   \n",
              "45           HUM, STEM                 MULTI           INSTR   \n",
              "46      HUM, OTH, STEM                   IND           INSTR   \n",
              "47                STEM                 MULTI           TRAIN   \n",
              "48                STEM                   IND           INSTR   \n",
              "49                UNSP            IND, MULTI           INSTR   \n",
              "50                STEM                   IND           INSTR   \n",
              "51                 HUM                   IND           TRAIN   \n",
              "52                STEM                   IND             INF   \n",
              "53                STEM                   IND             INF   \n",
              "54                 PSY                   IND           TRAIN   \n",
              "55                STEM                 MULTI           TRAIN   \n",
              "56                 PSY                   IND           TRAIN   \n",
              "57                STEM                   IND           INSTR   \n",
              "58                STEM                   IND             INF   \n",
              "59                STEM                 MULTI           INSTR   \n",
              "60                STEM                   IND           INSTR   \n",
              "61                STEM                   IND           INSTR   \n",
              "62                STEM                   IND           INSTR   \n",
              "63                UNSP                   IND           INSTR   \n",
              "64           HUM, STEM                 MULTI           INSTR   \n",
              "65                STEM                 MULTI           INSTR   \n",
              "66                STEM                   IND           INSTR   \n",
              "67                STEM                 MULTI           INSTR   \n",
              "68                STEM                   IND           INSTR   \n",
              "69                 HUM                 MULTI             INF   \n",
              "70                STEM                   IND           INSTR   \n",
              "71                 OTH                 MULTI           TRAIN   \n",
              "72                STEM                 MULTI           INSTR   \n",
              "\n",
              "   Level of Instruction or Training Analysis Approach  \n",
              "0                               K12                MB  \n",
              "1                               K12                MB  \n",
              "2                               UNI                MB  \n",
              "3                               UNI                MB  \n",
              "4                               UNI                MB  \n",
              "5                          K12, UNI                MF  \n",
              "6                          K12, UNI                MB  \n",
              "7                               K12                MB  \n",
              "8                              UNSP                MB  \n",
              "9                               K12                MB  \n",
              "10                              K12                MB  \n",
              "11                              UNI                MB  \n",
              "12                              UNI                MB  \n",
              "13                              UNI                MF  \n",
              "14                              UNI                MB  \n",
              "15                              UNI                MF  \n",
              "16                              UNI            MB, MF  \n",
              "17                             PROF                MB  \n",
              "18                              UNI                MB  \n",
              "19                              UNI                MB  \n",
              "20                              K12                MB  \n",
              "21                              K12            MB, MF  \n",
              "22                              UNI            MB, MF  \n",
              "23                             UNSP                MB  \n",
              "24                              UNI                MB  \n",
              "25                              UNI                MB  \n",
              "26                              UNI            MB, MF  \n",
              "27                             UNSP            MB, MF  \n",
              "28                             UNSP                MB  \n",
              "29                              UNI                MB  \n",
              "30                             UNSP            MB, MF  \n",
              "31                              UNI                MB  \n",
              "32                              K12                MF  \n",
              "33                              K12                MB  \n",
              "34                              K12                MB  \n",
              "35                              UNI                MB  \n",
              "36                              UNI                MB  \n",
              "37                              K12                MB  \n",
              "38                              UNI                MB  \n",
              "39                         K12, UNI            MB, MF  \n",
              "40                             PROF                MB  \n",
              "41                              UNI                MF  \n",
              "42                              K12                MB  \n",
              "43                              UNI                MB  \n",
              "44                              UNI                MB  \n",
              "45                              K12                MF  \n",
              "46                              K12                MB  \n",
              "47                              UNI                MF  \n",
              "48                              K12                MB  \n",
              "49                              UNI                MB  \n",
              "50                              UNI                MB  \n",
              "51                             UNSP                MF  \n",
              "52                              UNI                MF  \n",
              "53                         K12, UNI            MB, MF  \n",
              "54                             PROF                MB  \n",
              "55                              UNI                MF  \n",
              "56                              UNI                MB  \n",
              "57                              K12                MB  \n",
              "58                              K12                MF  \n",
              "59                              UNI                MF  \n",
              "60                              K12                MF  \n",
              "61                              K12            MB, MF  \n",
              "62                              UNI                MB  \n",
              "63                             UNSP                MB  \n",
              "64                              K12                MB  \n",
              "65                              K12                MB  \n",
              "66                              K12            MB, MF  \n",
              "67                              K12                MB  \n",
              "68                              K12            MB, MF  \n",
              "69                        PROF, UNI                MF  \n",
              "70                              K12                MF  \n",
              "71                             PROF                MF  \n",
              "72                              K12                MB  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d242c021-6a86-4c83-b07f-5c70903722e8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type</th>\n",
              "      <th>Data Collection Mediums</th>\n",
              "      <th>Modalities</th>\n",
              "      <th>Analysis Methods</th>\n",
              "      <th>Fusion Types</th>\n",
              "      <th>Publication</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>818492192</td>\n",
              "      <td>understanding student learning trajectories using multimodal learning analytics within an embodied-interaction learning environment</td>\n",
              "      <td>Alejandro Andrade</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,INTER,PPA</td>\n",
              "      <td>GAZE,LOGS,INTER,PPA,GEST</td>\n",
              "      <td>CLUST,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3408664396</td>\n",
              "      <td>multimodal student engagement recognition in prosocial games</td>\n",
              "      <td>Athanasios Psaltis</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS</td>\n",
              "      <td>POSE,AFFECT,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>LATE</td>\n",
              "      <td>T-CIAIG</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1118315889</td>\n",
              "      <td>using multimodal learning analytics to identify aspects of collaboration in project-based learning</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS</td>\n",
              "      <td>POSE,PROS</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>CSCL</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3339002981</td>\n",
              "      <td>estimation of success in collaborative learning based on multimodal learning analytics features</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,LOGS,VIDEO,AUDIO</td>\n",
              "      <td>GAZE,LOGS,PROS,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICALT</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1609706685</td>\n",
              "      <td>learning pulse: a machine learning approach for predicting performance in self-regulated learning using multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>SENSOR,LOGS,MOTION,PPA</td>\n",
              "      <td>PULSE,ACT,AFFECT</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>BLND</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3093310941</td>\n",
              "      <td>embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders</td>\n",
              "      <td>Hiroki Tanaka</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>AUDIO,VIDEO,PPA</td>\n",
              "      <td>POSE,PROS,AFFECT</td>\n",
              "      <td>REG,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>PLOS</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3095923626</td>\n",
              "      <td>a multimodal analysis of making</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR,PPA,INTER</td>\n",
              "      <td>GEST,PPA,EDA,ACT,PROS,QUAL,INTER</td>\n",
              "      <td>STATS,CLUST,QUAL,PATT</td>\n",
              "      <td>EARLY</td>\n",
              "      <td>IJAIED</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2456887548</td>\n",
              "      <td>an unobtrusive and multimodal approach for behavioral engagement detection of students</td>\n",
              "      <td>Nese Alyuz</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,VIDEO,SCREEN,PPA</td>\n",
              "      <td>AFFECT,POSE,LOGS,PPA</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>MIE</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1374035721</td>\n",
              "      <td>attentivelearner2: a multimodal approach for improving mooc learning on mobile devices</td>\n",
              "      <td>Phuong Pham</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SURVEY</td>\n",
              "      <td>PULSE,AFFECT,SURVEY</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>AIED</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>85990093</td>\n",
              "      <td>multimodal markers of persuasive speech : designing a virtual debate coach</td>\n",
              "      <td>Volha Petukhova</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>PROS,GEST</td>\n",
              "      <td>CLS,QUAL,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>INTERSPEECH</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>957160695</td>\n",
              "      <td>virtual debate coach design: assessing multimodal argumentation performance</td>\n",
              "      <td>Volha Petukhova</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>GEST,TRANS,PROS,SURVEY,GAZE</td>\n",
              "      <td>STATS,CLS,QUAL</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICMI</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1637690235</td>\n",
              "      <td>supervised machine learning in multimodal learning analytics for estimating success in project-based learning</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,PPA,RPA</td>\n",
              "      <td>POSE,GEST,PROS,LOGS,PPA,RPA</td>\n",
              "      <td>REG,CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1886134458</td>\n",
              "      <td>personalizing computer science education by leveraging multimodal learning analytics</td>\n",
              "      <td>David Azcona</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,PPA</td>\n",
              "      <td>LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>FIE</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2181637610</td>\n",
              "      <td>toward using multi-modal learning analytics to support and measure collaboration in co-located dyads</td>\n",
              "      <td>Emma L. Starr</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA,RPA,LOGS</td>\n",
              "      <td>POSE,PROS,PPA,RPA,LOGS</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ICLS</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>483140962</td>\n",
              "      <td>investigating multimodal affect sensing in an affective tutoring system using unobtrusive sensors</td>\n",
              "      <td>Hua Leong Fwa</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS</td>\n",
              "      <td>POSE,ACT,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>PPIG</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3146393211</td>\n",
              "      <td>mobile mixed reality for experiential learning and simulation in medical and health sciences education</td>\n",
              "      <td>James Birt</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>PPA,INTER</td>\n",
              "      <td>PPA,TRANS</td>\n",
              "      <td>QUAL,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>Information</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3308658121</td>\n",
              "      <td>exploring collaboration using motion sensors and multi-modal learning analytics</td>\n",
              "      <td>Joseph M. Reilly</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>PPA,RPA,POSE,GEST</td>\n",
              "      <td>CLUST,PATT,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>EDM</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>3135645357</td>\n",
              "      <td>multimodal teaching analytics: automated extraction of orchestration graphs from wearable sensor data</td>\n",
              "      <td>Luis P. Prieto</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,VIDEO,AUDIO,MOTION</td>\n",
              "      <td>GAZE,PROS,ACT,PIXEL</td>\n",
              "      <td>NET,CLS,STATS,PATT,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3309250332</td>\n",
              "      <td>(dis)engagement matters: identifying efficacious learning practices with multimodal learning analytics</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,PPA</td>\n",
              "      <td>GEST,QUAL,PPA</td>\n",
              "      <td>CLUST,CLS,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2836996318</td>\n",
              "      <td>predicting learners' emotions in mobile mooc learning via a multimodal intelligent tutor</td>\n",
              "      <td>Phuong Pham</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO</td>\n",
              "      <td>AFFECT,PULSE</td>\n",
              "      <td>REG</td>\n",
              "      <td>LATE</td>\n",
              "      <td>ITS</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>3783339081</td>\n",
              "      <td>a novel method for the in-depth multimodal analysis of student learning trajectories in intelligent tutoring systems</td>\n",
              "      <td>Ran Liu</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,SCREEN,PPA</td>\n",
              "      <td>LOGS,TRANS,ACT,QUAL,PPA</td>\n",
              "      <td>STATS,REG,QUAL</td>\n",
              "      <td>MID</td>\n",
              "      <td>JLA</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3796180663</td>\n",
              "      <td>learning linkages: integrating data streams of multiple modalities and timescales</td>\n",
              "      <td>Ran Liu</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,SCREEN,PPA</td>\n",
              "      <td>TRANS,QUAL,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2345021698</td>\n",
              "      <td>exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course</td>\n",
              "      <td>René Noël</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,PPA,RPA</td>\n",
              "      <td>RPA,PROS</td>\n",
              "      <td>QUAL,NET,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>Access</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>804659204</td>\n",
              "      <td>towards smart educational recommendations with reinforcement learning in classroom</td>\n",
              "      <td>Su Liu</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SENSOR</td>\n",
              "      <td>PULSE,AFFECT,GAZE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>TALE</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2497456347</td>\n",
              "      <td>the rap system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2018</td>\n",
              "      <td>Training</td>\n",
              "      <td>AUDIO,VIDEO,PPA,SURVEY</td>\n",
              "      <td>PPA,GAZE,POSE,PROS</td>\n",
              "      <td>CLS,STATS,QUAL</td>\n",
              "      <td>LATE</td>\n",
              "      <td>LAK</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2070224207</td>\n",
              "      <td>detecting medical simulation errors with machine learning and multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,LOGS</td>\n",
              "      <td>POSE,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>CAIM</td>\n",
              "      <td>BLND</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>4019205162</td>\n",
              "      <td>introducing low-cost sensors into the classroom settings: improving the assessment in agile practices with multimodal learning analytics</td>\n",
              "      <td>Hector Cornide-Reyes</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,SURVEY,PPA,RPA</td>\n",
              "      <td>SURVEY,TRANS,PPA,RPA</td>\n",
              "      <td>NET,STATS</td>\n",
              "      <td>MID,OTH</td>\n",
              "      <td>Sensors</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1847468084</td>\n",
              "      <td>computationally augmented ethnography: emotion tracking and learning in museum games</td>\n",
              "      <td>Kit Martin</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA,RPA</td>\n",
              "      <td>TRANS,AFFECT,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ICQE</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>4278392816</td>\n",
              "      <td>multimodal data as a means to understand the learning experience</td>\n",
              "      <td>Michail Giannakos</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>LOGS,EYE,SENSOR,VIDEO</td>\n",
              "      <td>EEG,GAZE,LOGS,PULSE,EDA,TEMP,BP,POSE</td>\n",
              "      <td>REG,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJIM</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1576545447</td>\n",
              "      <td>artificial intelligence and multimodal data in the service of human decision-making: a case study in debate tutoring</td>\n",
              "      <td>Mutlu Cukurova</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,SURVEY</td>\n",
              "      <td>AFFECT,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>853680639</td>\n",
              "      <td>sensor-based data fusion for multimodal affect detection in game-based learning environments</td>\n",
              "      <td>Nathan Henderson</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,SENSOR,RPA</td>\n",
              "      <td>POSE,EDA,AFFECT</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>EDM</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>3398902089</td>\n",
              "      <td>what multimodal data can tell us about the students’ regulation of their learning process?</td>\n",
              "      <td>Sanna Järvelä</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SENSOR,VIDEO,AUDIO</td>\n",
              "      <td>EDA,AFFECT,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>LAI</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>86191824</td>\n",
              "      <td>examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes</td>\n",
              "      <td>Shiyan Jiang</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SCREEN,INTER,PPA,AUDIO</td>\n",
              "      <td>INTER,QUAL,TRANS</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ILE</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>3448122334</td>\n",
              "      <td>investigating the impact of a real-time, multimodal student engagement analytics technology in authentic classrooms</td>\n",
              "      <td>Sinem Aslan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,SCREEN,AUDIO,SURVEY,RPA,PPA,INTER</td>\n",
              "      <td>AFFECT,LOGS,POSE,QUAL,INTER,SURVEY,RPA</td>\n",
              "      <td>QUAL,STATS,CLS</td>\n",
              "      <td>LATE</td>\n",
              "      <td>CHI</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1296637108</td>\n",
              "      <td>towards collaboration translucence: giving meaning to multimodal group data</td>\n",
              "      <td>Vanessa Echeverria</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER</td>\n",
              "      <td>POSE,LOGS,TRANS,EDA,ACT,PROS,INTER</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>CHI</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1019093033</td>\n",
              "      <td>prime: block-wise missingness handling for multi-modalities in intelligent tutoring systems</td>\n",
              "      <td>Xi Yang</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>PPA,VIDEO,EYE</td>\n",
              "      <td>LOGS,AFFECT,GAZE</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>MMM</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1581261659</td>\n",
              "      <td>early prediction of visitor engagement in science museums with multimodal learning analytics</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,LOGS</td>\n",
              "      <td>POSE,GEST,AFFECT,GAZE,LOGS</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICMI</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>205660768</td>\n",
              "      <td>multimodal learning analytics to investigate cognitive load during online problem solving</td>\n",
              "      <td>Charlotte Larmuseau</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>PPA,SENSOR</td>\n",
              "      <td>PPA,PULSE,EDA</td>\n",
              "      <td>STATS,CLS</td>\n",
              "      <td>EARLY</td>\n",
              "      <td>BJET</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>3009548670</td>\n",
              "      <td>real-time multimodal feedback with the cpr tutor</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>LOGS,VIDEO,SENSOR,MOTION</td>\n",
              "      <td>POSE,EMG,GEST</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>AIED</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>1770989706</td>\n",
              "      <td>focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving</td>\n",
              "      <td>Hana Vrzakova</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,SCREEN,SURVEY,PPA</td>\n",
              "      <td>PROS,ACT,GEST,PPA</td>\n",
              "      <td>STATS,PATT</td>\n",
              "      <td>MID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>3051560548</td>\n",
              "      <td>temporal analysis of multimodal data to predict collaborative learning outcomes</td>\n",
              "      <td>Jennifer K. Olsen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,EYE</td>\n",
              "      <td>GAZE,LOGS,PROS,TRANS,QUAL</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>147203129</td>\n",
              "      <td>multimodal learning analytics to inform learning design: lessons learned from computing education</td>\n",
              "      <td>Katerina Mangaroska</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR,LOGS</td>\n",
              "      <td>LOGS,GAZE,EDA,PULSE,AFFECT,TEMP</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JLA</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>2000036002</td>\n",
              "      <td>predicting learners’ effortful behaviour in adaptive assessment using multimodal data</td>\n",
              "      <td>Kshitij Sharma</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR</td>\n",
              "      <td>EDA,TEMP,PULSE,EEG,GAZE,AFFECT</td>\n",
              "      <td>CLUST,CLS,PATT</td>\n",
              "      <td>MID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>2055153191</td>\n",
              "      <td>round or rectangular tables for collaborative problem solving? a multimodal learning analytics study</td>\n",
              "      <td>Milica Vujovic</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,MOTION</td>\n",
              "      <td>POSE,GEST,ACT</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM, STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>3796643912</td>\n",
              "      <td>an evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities</td>\n",
              "      <td>Penelope J. Standen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,RPA</td>\n",
              "      <td>AFFECT,POSE,LOGS,RPA,GAZE,PROS,GEST</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>HUM, OTH, STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>2879332689</td>\n",
              "      <td>from data to insights: a layered storytelling approach for multimodal learning analytics</td>\n",
              "      <td>Roberto Martinez-Maldonado</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>LOGS,MOTION,SENSOR,RPA,VIDEO</td>\n",
              "      <td>POSE,EDA,LOGS,RPA</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>CHI</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>1877483551</td>\n",
              "      <td>motion-based educational games: using multi-modal data to predict player’s performance</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR</td>\n",
              "      <td>PULSE,TEMP,EDA,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>COG</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>3637456466</td>\n",
              "      <td>impact of inquiry interventions on students in e-learning and classroom environments using affective computing framework</td>\n",
              "      <td>T. S. Ashwin</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,PPA</td>\n",
              "      <td>AFFECT,POSE,GEST</td>\n",
              "      <td>CLS,STATS,PATT</td>\n",
              "      <td>MID</td>\n",
              "      <td>UMUAI</td>\n",
              "      <td>PHYS, VIRT</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>2936220551</td>\n",
              "      <td>multi-source and multimodal data fusion for predicting academic performance in blended learning university courses</td>\n",
              "      <td>Wilson Chango</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,PPA</td>\n",
              "      <td>LOGS,POSE,RPA,ACT</td>\n",
              "      <td>CLS,QUAL,STATS</td>\n",
              "      <td>MID,LATE</td>\n",
              "      <td>CEE</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>123412197</td>\n",
              "      <td>utilizing multimodal data through fsqca to explain engagement in adaptive learning</td>\n",
              "      <td>Zacharoula Papamitsiou</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,LOGS,EYE,SENSOR,VIDEO</td>\n",
              "      <td>PULSE,AFFECT,EEG,GAZE,LOGS,BP,TEMP,EDA,SURVEY</td>\n",
              "      <td>PATT</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>TLT</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>3809293172</td>\n",
              "      <td>blending learning analytics and embodied design to model students' comprehension of measurement using their actions, speech, and gestures</td>\n",
              "      <td>Avery H. Closser</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>RPA</td>\n",
              "      <td>CLUST,REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>IJCCI</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>1763513559</td>\n",
              "      <td>keep me in the loop: real-time feedback with multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2021</td>\n",
              "      <td>Training</td>\n",
              "      <td>SURVEY,LOGS,VIDEO,SENSOR,MOTION</td>\n",
              "      <td>POSE,EMG,GEST,SURVEY</td>\n",
              "      <td>CLS,QUAL,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJAIED</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>4035649049</td>\n",
              "      <td>storytelling with learner data: guiding student reflection on multimodal team data</td>\n",
              "      <td>Gloria Fernández-Nieto</td>\n",
              "      <td>2021</td>\n",
              "      <td>Training</td>\n",
              "      <td>SENSOR,LOGS,RPA</td>\n",
              "      <td>EDA,LOGS,RPA,AFFECT</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>TLT</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>3625722965</td>\n",
              "      <td>table tennis tutor: forehand strokes classification based on multimodal data and neural networks</td>\n",
              "      <td>Khaleel Asyraaf Mat Sanusi</td>\n",
              "      <td>2021</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,INTER</td>\n",
              "      <td>POSE,GEST,ACT,INTER</td>\n",
              "      <td>CLS,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>Sensors</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>1426267857</td>\n",
              "      <td>affect, support, and personal factors: multimodal causal models of one-on-one coaching</td>\n",
              "      <td>Lujie Karen Chen</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,SURVEY</td>\n",
              "      <td>PROS,GAZE,TRANS,AFFECT,SURVEY</td>\n",
              "      <td>STATS,NET</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JEDM</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>666050348</td>\n",
              "      <td>multicraft: a multimodal interface for supporting and studying learning in minecraft</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS</td>\n",
              "      <td>PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>HCII</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>518268671</td>\n",
              "      <td>using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game</td>\n",
              "      <td>María Ximena López</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,LOGS,AUDIO,VIDEO</td>\n",
              "      <td>LOGS,SURVEY,GAZE,PROS</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ECGBL</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>3660066725</td>\n",
              "      <td>children's play and problem solving in motion-based educational games: synergies between human annotations and multi-modal data</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SENSOR,EYE,LOGS</td>\n",
              "      <td>ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>IDC</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>3856280479</td>\n",
              "      <td>children's play and problem-solving in motion-based learning technologies using a multi-modal mixed methods approach</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SENSOR,EYE,LOGS</td>\n",
              "      <td>ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP,BP</td>\n",
              "      <td>STATS,QUAL,CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJCCI</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>4277812050</td>\n",
              "      <td>improving prediction of students' performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources</td>\n",
              "      <td>Wilson Chango</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,VIDEO,EYE,PPA</td>\n",
              "      <td>AFFECT,LOGS,GAZE,PPA</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID,LATE</td>\n",
              "      <td>JCHE</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>566043228</td>\n",
              "      <td>automatic student engagement in online learning environment based on neural turing machine</td>\n",
              "      <td>Xiaoyang Ma</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO</td>\n",
              "      <td>POSE,GAZE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>IJIET</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>1315379489</td>\n",
              "      <td>multimodal engagement analysis from facial videos in the classroom</td>\n",
              "      <td>Ömer Sümer</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO</td>\n",
              "      <td>POSE,AFFECT</td>\n",
              "      <td>CLS</td>\n",
              "      <td>EARLY,LATE</td>\n",
              "      <td>TAC</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM, STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>1469065963</td>\n",
              "      <td>examining socially shared regulation and shared physiological arousal events with multimodal learning analytics</td>\n",
              "      <td>Andy Nguyen</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR</td>\n",
              "      <td>QUAL,EDA</td>\n",
              "      <td>PATT,CLS,CLUST</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>433919853</td>\n",
              "      <td>understanding fun in learning to code: a multi-modal data approach</td>\n",
              "      <td>Gabriella Tisza</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SENSOR,VIDEO,PPA</td>\n",
              "      <td>TEMP,PULSE,EDA,BP,AFFECT</td>\n",
              "      <td>REG,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>IDC</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>2273914836</td>\n",
              "      <td>many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities</td>\n",
              "      <td>Jauwairia Nasir</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,PPA,SURVEY</td>\n",
              "      <td>PROS,AFFECT,GAZE,TRANS,LOGS</td>\n",
              "      <td>STATS,QUAL,CLUST,CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJCSCL</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>32184286</td>\n",
              "      <td>once more with feeling: emotions in multimodal learning analytics</td>\n",
              "      <td>Marcus Kubsch</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,PPA,AUDIO</td>\n",
              "      <td>INTER,SURVEY,TRANS,PROS,AFFECT</td>\n",
              "      <td>CLS,REG,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MMLA Handbook</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>2609260641</td>\n",
              "      <td>visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication</td>\n",
              "      <td>René Noël</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,RPA,INTER</td>\n",
              "      <td>PROS,POSE,RPA,INTER,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>DAMLE</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>PROF, UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>1345598079</td>\n",
              "      <td>intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning</td>\n",
              "      <td>Sofia Tancredi</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,VIDEO,AUDIO,INTER</td>\n",
              "      <td>GAZE,GEST,TRANS,POSE,INTER</td>\n",
              "      <td>PATT,QUAL,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MMLA Handbook</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>2155422499</td>\n",
              "      <td>a multimodal analysis of pair work engagement episodes: implications for emi lecturer training</td>\n",
              "      <td>Teresa Morell</td>\n",
              "      <td>2022</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>TRANS,PPA,QUAL,POSE,ACT</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>JEAP</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>3754172825</td>\n",
              "      <td>detecting impasse during collaborative problem solving with multimodal learning analytics</td>\n",
              "      <td>Yingbo Ma</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>TRANS,PROS,SPECT,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d242c021-6a86-4c83-b07f-5c70903722e8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d242c021-6a86-4c83-b07f-5c70903722e8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d242c021-6a86-4c83-b07f-5c70903722e8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c7272f27-0f9b-41f4-bf98-11fe5a8ef70c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c7272f27-0f9b-41f4-bf98-11fe5a8ef70c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c7272f27-0f9b-41f4-bf98-11fe5a8ef70c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_c127a889-f478-4a54-bc89-449b6de7f98e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_all')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c127a889-f478-4a54-bc89-449b6de7f98e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_all');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descriptive Statistics"
      ],
      "metadata": {
        "id": "RgAG_MOGlky2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Individual modalities."
      ],
      "metadata": {
        "id": "mSdam83Gnkl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROS = TRANS = TEXT = SPECT = 0\n",
        "\n",
        "for _,row in df_nlp.iterrows():\n",
        "    mods = set(row[\"Modalities\"].split(\",\"))\n",
        "    if \"PROS\" in mods:\n",
        "        PROS += 1\n",
        "    if \"TRANS\" in mods:\n",
        "        TRANS += 1\n",
        "    if \"TEXT\" in mods:\n",
        "        TEXT += 1\n",
        "    if \"SPECT\" in mods:\n",
        "        SPECT += 1\n",
        "\n",
        "print(f\"PROS: {PROS}\")\n",
        "print(f\"TRANS: {TRANS}\")\n",
        "print(f\"TEXT: {TEXT}\")\n",
        "print(f\"SPECT: {SPECT}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlpAqiL1lkWF",
        "outputId": "5c11b047-d067-4578-ac27-aadd350ba77e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROS: 24\n",
            "TRANS: 16\n",
            "TEXT: 1\n",
            "SPECT: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combined modalities."
      ],
      "metadata": {
        "id": "F2KzeuTxnmWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pros_aud = df_nlp[df_nlp[\"Modalities\"].str.contains(\"PROS\") & \\\n",
        "                     df_nlp[\"Modalities\"].str.contains(\"TRANS\")]\n",
        "print(len(df_pros_aud))\n",
        "df_pros_aud"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RcS3OwDinpJr",
        "outputId": "1d376982-b59e-4d8b-cb4a-64aed5051328"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          UUID  \\\n",
              "5    957160695   \n",
              "21  1296637108   \n",
              "23  3051560548   \n",
              "26  1426267857   \n",
              "27   666050348   \n",
              "29  2273914836   \n",
              "30    32184286   \n",
              "34  3754172825   \n",
              "\n",
              "                                                                                                                            Title  \\\n",
              "5                                                     virtual debate coach design: assessing multimodal argumentation performance   \n",
              "21                                                    towards collaboration translucence: giving meaning to multimodal group data   \n",
              "23                                                temporal analysis of multimodal data to predict collaborative learning outcomes   \n",
              "26                                         affect, support, and personal factors: multimodal causal models of one-on-one coaching   \n",
              "27                                           multicraft: a multimodal interface for supporting and studying learning in minecraft   \n",
              "29  many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities   \n",
              "30                                                              once more with feeling: emotions in multimodal learning analytics   \n",
              "34                                      detecting impasse during collaborative problem solving with multimodal learning analytics   \n",
              "\n",
              "          First Author  Year Environment Type  \\\n",
              "5      Volha Petukhova  2017         Training   \n",
              "21  Vanessa Echeverria  2019         Training   \n",
              "23   Jennifer K. Olsen  2020         Learning   \n",
              "26    Lujie Karen Chen  2021         Learning   \n",
              "27     Marcelo Worsley  2021         Learning   \n",
              "29     Jauwairia Nasir  2022         Learning   \n",
              "30       Marcus Kubsch  2022         Learning   \n",
              "34           Yingbo Ma  2022         Learning   \n",
              "\n",
              "                          Data Collection Mediums  \\\n",
              "5                                     VIDEO,AUDIO   \n",
              "21           VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER   \n",
              "23                                 LOGS,AUDIO,EYE   \n",
              "26                             AUDIO,VIDEO,SURVEY   \n",
              "27  AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS   \n",
              "29                    VIDEO,AUDIO,LOGS,PPA,SURVEY   \n",
              "30                               SURVEY,PPA,AUDIO   \n",
              "34                                    VIDEO,AUDIO   \n",
              "\n",
              "                                Modalities      Analysis Methods Fusion Types  \\\n",
              "5              GEST,TRANS,PROS,SURVEY,GAZE        STATS,CLS,QUAL          MID   \n",
              "21      POSE,LOGS,TRANS,EDA,ACT,PROS,INTER                  QUAL          OTH   \n",
              "23               GAZE,LOGS,PROS,TRANS,QUAL                   REG          MID   \n",
              "26           PROS,GAZE,TRANS,AFFECT,SURVEY             STATS,NET       HYBRID   \n",
              "27  PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS                  QUAL          OTH   \n",
              "29             PROS,AFFECT,GAZE,TRANS,LOGS  STATS,QUAL,CLUST,CLS       HYBRID   \n",
              "30          INTER,SURVEY,TRANS,PROS,AFFECT         CLS,REG,STATS          OTH   \n",
              "34              TRANS,PROS,SPECT,GAZE,POSE                   CLS       HYBRID   \n",
              "\n",
              "      Publication Environment Setting Environment Subject  \\\n",
              "5            ICMI                PHYS                 HUM   \n",
              "21            CHI                PHYS                STEM   \n",
              "23           BJET                VIRT                STEM   \n",
              "26           JEDM                PHYS                STEM   \n",
              "27           HCII                VIRT                STEM   \n",
              "29         IJCSCL                BLND                STEM   \n",
              "30  MMLA Handbook                PHYS                STEM   \n",
              "34            LAK                VIRT                STEM   \n",
              "\n",
              "   Participant Structure Didactic Nature Level of Instruction or Training  \\\n",
              "5                  MULTI           TRAIN                              K12   \n",
              "21                 MULTI           TRAIN                              UNI   \n",
              "23                 MULTI           INSTR                              K12   \n",
              "26                   IND           INSTR                              K12   \n",
              "27                   IND             INF                              K12   \n",
              "29                 MULTI           INSTR                              K12   \n",
              "30                   IND           INSTR                              K12   \n",
              "34                 MULTI           INSTR                              K12   \n",
              "\n",
              "   Analysis Approach  \\\n",
              "5                 MB   \n",
              "21                MB   \n",
              "23                MB   \n",
              "26                MB   \n",
              "27                MF   \n",
              "29                MB   \n",
              "30            MB, MF   \n",
              "34                MB   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                            Results  \n",
              "5   \"We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain corpora and classification procedures resulted in the best performance on an argument structure mining task.\"\\n\\n\"We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain c...  \n",
              "21  \"We illustrate the approach from the automated analysis of healthcare simulations to train nurses, generating four visual proxies that fuse multimodal data into higher order patterns.\"\\n\\n\"we introduce the concept of collaboration translucence, an approach to make visible selected features of group activity. This is grounded both theoretically (in the physical, epistemic, social and affective ...  \n",
              "23  Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.\\n\\nEvaluating how multimodal features contribute to a model's performance to predict learning gains...  \n",
              "26  \"this research contributes to the relatively sparse literature in multimodal learning analytics by providing a balanced view of the teacher and student interactions with a data set collected in naturalistic home environments.\"\\n\\n\"Firstly, we note a clear causal pathway between the group of variables describing parents’ support and another group representing the child’s cognitive-affective exp...  \n",
              "27  \"Through our user studies, we found that the platform helps fulfill some of those goals by providing capabilities that can spur on amazement and excitement among traditional Minecraft users and novices. We also find that many of the multimodal components, while not immediately intuitive for users, proved to be preferred modes of game play. In this sense, we feel that this tool is moving in the...  \n",
              "29  \"Our combined multi-modal learning analytics and interaction analysis methodology enabled us to identify two multi-modal profles of learners who have learning gains and one multi-modal profle of learners who do not have learning gains.\"\\n\\n\"Using this approach, we are able to build the multimodal behavioral profles for each group of learners.\"\\n\\n\"Our combined multi-modal learning analytics an...  \n",
              "30  Results demonstrate how NLP and ML techniques allow us to use different modalities of the same data, voice and transcript, and different modalities of different data sources, voice data from interviews, answers to a goal orientation questionnaire, and answers to open-ended questions about energy, in order to better understand individual differences in students’ performances\\n\\nUsed text and au...  \n",
              "34  \"We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements in the video modality are the most significant unimodal indicators of impasse.\"\\n\\n\"We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements i...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5a115fd2-cc94-49a3-9a36-466e32330d54\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type</th>\n",
              "      <th>Data Collection Mediums</th>\n",
              "      <th>Modalities</th>\n",
              "      <th>Analysis Methods</th>\n",
              "      <th>Fusion Types</th>\n",
              "      <th>Publication</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Results</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>957160695</td>\n",
              "      <td>virtual debate coach design: assessing multimodal argumentation performance</td>\n",
              "      <td>Volha Petukhova</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>GEST,TRANS,PROS,SURVEY,GAZE</td>\n",
              "      <td>STATS,CLS,QUAL</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICMI</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain corpora and classification procedures resulted in the best performance on an argument structure mining task.\"\\n\\n\"We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1296637108</td>\n",
              "      <td>towards collaboration translucence: giving meaning to multimodal group data</td>\n",
              "      <td>Vanessa Echeverria</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER</td>\n",
              "      <td>POSE,LOGS,TRANS,EDA,ACT,PROS,INTER</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>CHI</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"We illustrate the approach from the automated analysis of healthcare simulations to train nurses, generating four visual proxies that fuse multimodal data into higher order patterns.\"\\n\\n\"we introduce the concept of collaboration translucence, an approach to make visible selected features of group activity. This is grounded both theoretically (in the physical, epistemic, social and affective ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3051560548</td>\n",
              "      <td>temporal analysis of multimodal data to predict collaborative learning outcomes</td>\n",
              "      <td>Jennifer K. Olsen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,EYE</td>\n",
              "      <td>GAZE,LOGS,PROS,TRANS,QUAL</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.\\n\\nEvaluating how multimodal features contribute to a model's performance to predict learning gains...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1426267857</td>\n",
              "      <td>affect, support, and personal factors: multimodal causal models of one-on-one coaching</td>\n",
              "      <td>Lujie Karen Chen</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,SURVEY</td>\n",
              "      <td>PROS,GAZE,TRANS,AFFECT,SURVEY</td>\n",
              "      <td>STATS,NET</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JEDM</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"this research contributes to the relatively sparse literature in multimodal learning analytics by providing a balanced view of the teacher and student interactions with a data set collected in naturalistic home environments.\"\\n\\n\"Firstly, we note a clear causal pathway between the group of variables describing parents’ support and another group representing the child’s cognitive-affective exp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>666050348</td>\n",
              "      <td>multicraft: a multimodal interface for supporting and studying learning in minecraft</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS</td>\n",
              "      <td>PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>HCII</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"Through our user studies, we found that the platform helps fulfill some of those goals by providing capabilities that can spur on amazement and excitement among traditional Minecraft users and novices. We also find that many of the multimodal components, while not immediately intuitive for users, proved to be preferred modes of game play. In this sense, we feel that this tool is moving in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2273914836</td>\n",
              "      <td>many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities</td>\n",
              "      <td>Jauwairia Nasir</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,PPA,SURVEY</td>\n",
              "      <td>PROS,AFFECT,GAZE,TRANS,LOGS</td>\n",
              "      <td>STATS,QUAL,CLUST,CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJCSCL</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"Our combined multi-modal learning analytics and interaction analysis methodology enabled us to identify two multi-modal profles of learners who have learning gains and one multi-modal profle of learners who do not have learning gains.\"\\n\\n\"Using this approach, we are able to build the multimodal behavioral profles for each group of learners.\"\\n\\n\"Our combined multi-modal learning analytics an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>32184286</td>\n",
              "      <td>once more with feeling: emotions in multimodal learning analytics</td>\n",
              "      <td>Marcus Kubsch</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,PPA,AUDIO</td>\n",
              "      <td>INTER,SURVEY,TRANS,PROS,AFFECT</td>\n",
              "      <td>CLS,REG,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MMLA Handbook</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>Results demonstrate how NLP and ML techniques allow us to use different modalities of the same data, voice and transcript, and different modalities of different data sources, voice data from interviews, answers to a goal orientation questionnaire, and answers to open-ended questions about energy, in order to better understand individual differences in students’ performances\\n\\nUsed text and au...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>3754172825</td>\n",
              "      <td>detecting impasse during collaborative problem solving with multimodal learning analytics</td>\n",
              "      <td>Yingbo Ma</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>TRANS,PROS,SPECT,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements in the video modality are the most significant unimodal indicators of impasse.\"\\n\\n\"We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5a115fd2-cc94-49a3-9a36-466e32330d54')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5a115fd2-cc94-49a3-9a36-466e32330d54 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5a115fd2-cc94-49a3-9a36-466e32330d54');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7332b26f-8bc3-40e7-827f-c4c624722b87\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7332b26f-8bc3-40e7-827f-c4c624722b87')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7332b26f-8bc3-40e7-827f-c4c624722b87 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_0a98c51d-0216-4fe4-8e9d-c21f8e3da688\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_pros_aud')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_0a98c51d-0216-4fe4-8e9d-c21f8e3da688 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_pros_aud');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pros_xor_aud = df_nlp[df_nlp[\"Modalities\"].str.contains(\"PROS\") ^ \\\n",
        "                     df_nlp[\"Modalities\"].str.contains(\"TRANS\")]\n",
        "print(len(df_pros_xor_aud))\n",
        "df_pros_xor_aud"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lrgmPJPmqjE9",
        "outputId": "a7666679-6374-4eec-f881-7611fff6b3df"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          UUID  \\\n",
              "0   1118315889   \n",
              "1   3339002981   \n",
              "2   3093310941   \n",
              "3   3095923626   \n",
              "4     85990093   \n",
              "6   1637690235   \n",
              "7   2181637610   \n",
              "8   3146393211   \n",
              "9   3135645357   \n",
              "10  3783339081   \n",
              "11  3796180663   \n",
              "12  2345021698   \n",
              "13  2497456347   \n",
              "14  4019205162   \n",
              "15  1847468084   \n",
              "16  1326191931   \n",
              "19    86191824   \n",
              "22  1770989706   \n",
              "24  3796643912   \n",
              "25  2634033325   \n",
              "28   518268671   \n",
              "31  2609260641   \n",
              "32  1345598079   \n",
              "33  2155422499   \n",
              "\n",
              "                                                                                                                                       Title  \\\n",
              "0                                         using multimodal learning analytics to identify aspects of collaboration in project-based learning   \n",
              "1                                            estimation of success in collaborative learning based on multimodal learning analytics features   \n",
              "2                    embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders   \n",
              "3                                                                                                            a multimodal analysis of making   \n",
              "4                                                                 multimodal markers of persuasive speech : designing a virtual debate coach   \n",
              "6                              supervised machine learning in multimodal learning analytics for estimating success in project-based learning   \n",
              "7                                       toward using multi-modal learning analytics to support and measure collaboration in co-located dyads   \n",
              "8                                     mobile mixed reality for experiential learning and simulation in medical and health sciences education   \n",
              "9                                      multimodal teaching analytics: automated extraction of orchestration graphs from wearable sensor data   \n",
              "10                      a novel method for the in-depth multimodal analysis of student learning trajectories in intelligent tutoring systems   \n",
              "11                                                         learning linkages: integrating data streams of multiple modalities and timescales   \n",
              "12         exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course   \n",
              "13                             the rap system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors   \n",
              "14  introducing low-cost sensors into the classroom settings: improving the assessment in agile practices with multimodal learning analytics   \n",
              "15                                                      computationally augmented ethnography: emotion tracking and learning in museum games   \n",
              "16                                                                                   multimodal learning analytics in a laboratory classroom   \n",
              "19                 examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes   \n",
              "22                                focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving   \n",
              "24           an evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities   \n",
              "25                               controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "28                                 using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game   \n",
              "31                              visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication   \n",
              "32     intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning   \n",
              "33                                            a multimodal analysis of pair work engagement episodes: implications for emi lecturer training   \n",
              "\n",
              "             First Author  Year Environment Type  \\\n",
              "0           Daniel Spikol  2017         Learning   \n",
              "1           Daniel Spikol  2017         Learning   \n",
              "2           Hiroki Tanaka  2017         Training   \n",
              "3         Marcelo Worsley  2017         Learning   \n",
              "4         Volha Petukhova  2017         Training   \n",
              "6           Daniel Spikol  2018         Learning   \n",
              "7           Emma L. Starr  2018         Learning   \n",
              "8              James Birt  2018         Learning   \n",
              "9          Luis P. Prieto  2018         Learning   \n",
              "10                Ran Liu  2018         Learning   \n",
              "11                Ran Liu  2018         Learning   \n",
              "12              René Noël  2018         Learning   \n",
              "13           Xavier Ochoa  2018         Training   \n",
              "14   Hector Cornide-Reyes  2019         Learning   \n",
              "15             Kit Martin  2019         Learning   \n",
              "16  Man Ching Esther Chan  2019         Learning   \n",
              "19           Shiyan Jiang  2019         Learning   \n",
              "22          Hana Vrzakova  2020         Learning   \n",
              "24    Penelope J. Standen  2020         Learning   \n",
              "25           Xavier Ochoa  2020         Training   \n",
              "28     María Ximena López  2021         Learning   \n",
              "31              René Noël  2022         Learning   \n",
              "32         Sofia Tancredi  2022         Learning   \n",
              "33          Teresa Morell  2022         Training   \n",
              "\n",
              "          Data Collection Mediums                           Modalities  \\\n",
              "0                VIDEO,AUDIO,LOGS                            POSE,PROS   \n",
              "1            EYE,LOGS,VIDEO,AUDIO                  GAZE,LOGS,PROS,POSE   \n",
              "2                 AUDIO,VIDEO,PPA                     POSE,PROS,AFFECT   \n",
              "3    VIDEO,AUDIO,SENSOR,PPA,INTER     GEST,PPA,EDA,ACT,PROS,QUAL,INTER   \n",
              "4                     VIDEO,AUDIO                            PROS,GEST   \n",
              "6        VIDEO,AUDIO,LOGS,PPA,RPA          POSE,GEST,PROS,LOGS,PPA,RPA   \n",
              "7        VIDEO,AUDIO,PPA,RPA,LOGS               POSE,PROS,PPA,RPA,LOGS   \n",
              "8                       PPA,INTER                            PPA,TRANS   \n",
              "9          EYE,VIDEO,AUDIO,MOTION                  GAZE,PROS,ACT,PIXEL   \n",
              "10          LOGS,AUDIO,SCREEN,PPA              LOGS,TRANS,ACT,QUAL,PPA   \n",
              "11    VIDEO,AUDIO,LOGS,SCREEN,PPA                       TRANS,QUAL,PPA   \n",
              "12                  AUDIO,PPA,RPA                             RPA,PROS   \n",
              "13         AUDIO,VIDEO,PPA,SURVEY                   PPA,GAZE,POSE,PROS   \n",
              "14           AUDIO,SURVEY,PPA,RPA                 SURVEY,TRANS,PPA,RPA   \n",
              "15            VIDEO,AUDIO,PPA,RPA                    TRANS,AFFECT,QUAL   \n",
              "16                    VIDEO,AUDIO                       POSE,GAZE,PROS   \n",
              "19         SCREEN,INTER,PPA,AUDIO                     INTER,QUAL,TRANS   \n",
              "22  AUDIO,VIDEO,SCREEN,SURVEY,PPA                    PROS,ACT,GEST,PPA   \n",
              "24           VIDEO,AUDIO,LOGS,RPA  AFFECT,POSE,LOGS,RPA,GAZE,PROS,GEST   \n",
              "25                VIDEO,AUDIO,PPA                        POSE,PROS,PPA   \n",
              "28        SURVEY,LOGS,AUDIO,VIDEO                LOGS,SURVEY,GAZE,PROS   \n",
              "31          AUDIO,VIDEO,RPA,INTER             PROS,POSE,RPA,INTER,QUAL   \n",
              "32          EYE,VIDEO,AUDIO,INTER           GAZE,GEST,TRANS,POSE,INTER   \n",
              "33                VIDEO,AUDIO,PPA              TRANS,PPA,QUAL,POSE,ACT   \n",
              "\n",
              "           Analysis Methods Fusion Types    Publication Environment Setting  \\\n",
              "0                       REG          MID           CSCL                PHYS   \n",
              "1                       CLS          MID          ICALT                VIRT   \n",
              "2                 REG,STATS          MID           PLOS                VIRT   \n",
              "3     STATS,CLUST,QUAL,PATT        EARLY         IJAIED                PHYS   \n",
              "4            CLS,QUAL,STATS          MID    INTERSPEECH                PHYS   \n",
              "6                   REG,CLS          MID           JCAL                BLND   \n",
              "7                STATS,QUAL          OTH           ICLS                BLND   \n",
              "8                QUAL,STATS          OTH    Information                BLND   \n",
              "9   NET,CLS,STATS,PATT,QUAL       HYBRID           JCAL                PHYS   \n",
              "10           STATS,REG,QUAL          MID            JLA                VIRT   \n",
              "11                CLS,STATS          MID           JCAL                VIRT   \n",
              "12           QUAL,NET,STATS          OTH         Access                PHYS   \n",
              "13           CLS,STATS,QUAL         LATE            LAK                BLND   \n",
              "14                NET,STATS      MID,OTH        Sensors                PHYS   \n",
              "15                     QUAL          OTH           ICQE                VIRT   \n",
              "16                CLS,CLUST         LATE         MLPALA                PHYS   \n",
              "19                     QUAL          OTH            ILE                VIRT   \n",
              "22               STATS,PATT          MID            LAK                VIRT   \n",
              "24                CLS,STATS       HYBRID           BJET                VIRT   \n",
              "25                    STATS          OTH           BJET                BLND   \n",
              "28                    STATS          OTH          ECGBL                BLND   \n",
              "31                     QUAL          OTH          DAMLE                PHYS   \n",
              "32          PATT,QUAL,STATS          OTH  MMLA Handbook                VIRT   \n",
              "33                     QUAL          OTH           JEAP                PHYS   \n",
              "\n",
              "   Environment Subject Participant Structure Didactic Nature  \\\n",
              "0                 STEM                 MULTI           INSTR   \n",
              "1                 STEM                 MULTI           INSTR   \n",
              "2                  HUM                   IND           TRAIN   \n",
              "3                 STEM                 MULTI             INF   \n",
              "4                  HUM                 MULTI           TRAIN   \n",
              "6                 STEM                 MULTI           INSTR   \n",
              "7                 STEM                 MULTI             INF   \n",
              "8                 STEM                   IND           INSTR   \n",
              "9                 STEM                 MULTI           INSTR   \n",
              "10                STEM                   IND           INSTR   \n",
              "11                STEM            IND, MULTI           INSTR   \n",
              "12                STEM                 MULTI             INF   \n",
              "13                 HUM                   IND           TRAIN   \n",
              "14                STEM                 MULTI           INSTR   \n",
              "15                STEM                 MULTI             INF   \n",
              "16                STEM            IND, MULTI           INSTR   \n",
              "19                STEM                 MULTI           INSTR   \n",
              "22                STEM                 MULTI           INSTR   \n",
              "24      HUM, OTH, STEM                   IND           INSTR   \n",
              "25                 HUM                   IND           TRAIN   \n",
              "28                STEM                 MULTI           INSTR   \n",
              "31                 HUM                 MULTI             INF   \n",
              "32                STEM                   IND           INSTR   \n",
              "33                 OTH                 MULTI           TRAIN   \n",
              "\n",
              "   Level of Instruction or Training Analysis Approach  \\\n",
              "0                               UNI                MB   \n",
              "1                               UNI                MB   \n",
              "2                          K12, UNI                MF   \n",
              "3                          K12, UNI                MB   \n",
              "4                               K12                MB   \n",
              "6                               UNI                MB   \n",
              "7                               UNI                MF   \n",
              "8                               UNI                MF   \n",
              "9                              PROF                MB   \n",
              "10                              K12                MB   \n",
              "11                              K12            MB, MF   \n",
              "12                              UNI            MB, MF   \n",
              "13                              UNI                MB   \n",
              "14                              UNI            MB, MF   \n",
              "15                             UNSP            MB, MF   \n",
              "16                             UNSP                MB   \n",
              "19                              K12                MB   \n",
              "22                              UNI                MF   \n",
              "24                              K12                MB   \n",
              "25                             UNSP                MF   \n",
              "28                              UNI                MF   \n",
              "31                        PROF, UNI                MF   \n",
              "32                              K12                MF   \n",
              "33                             PROF                MF   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                            Results  \n",
              "0   \"physical aspect of collaborative is an important part of this type of learning and that learning analytics systems can identify features that are relevant for helping researchers, teachers, and learners unpack what is happening.\"\\n\\n\"In this research study, we presented that where the students are looking, the distance between them, the motion of their hands our key features for a learning an...  \n",
              "1   Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, authors found that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.\\n\\nPredicting the learning gains via classification (bad, ok, good) through gaze, logs, audio...  \n",
              "2   We identified a positive correlation between overall narrative skills and other non-verbal behaviors. This result indicates that the overall narrative skills were subjectively decided based on other non-verbal behavior skills [22]. Although the automated social skills trainer did not provide feedback regarding narrative structure or eye gaze, these elements were also improved after the trainin...  \n",
              "3   \"each approach provides different affordances depending on the similarity metric and the dependent variable.\"\\n \"The take-away is that there are several strategies for doing multimodal learning analytics, and that many of these approaches can provide a meaningful glimpse into a complex data set, glimpses that may be difficult to identify using traditional approaches.\"\\n\\n\"Looking across analys...  \n",
              "4   \"Experiments of different types supported fairly reliable identification of markers from multimodal data, and linking these to assessments of debater confidence level and intensification behaviour.\"\\n\\n\"In line with previous empirical findings, we acknowledge that persuasive speech is rather difficult to characterize. Neverthe- less, based on theoretical and empirical frameworks set up by Gric...  \n",
              "6   \"In this paper, we show that MMLA and the state-of-the-art computational techniques can be used to generate insights into the \"black box\" of learning in students’ project-based activities. These insights generated from multimodal data can be used to inform teachers about the key features of project-based learning and help them support students appropriately in similar pedagogical approaches.\"\\...  \n",
              "7   While this study was not able to show a clear effect of providing a real-time visualization to support\\ncollaboration, it made many other valuable contributions. First, it showed that simple verbal interventions can\\nhelp participants pay attention to particular aspects of their collaborative behavior (i.e., how much they are\\ntalking and how much space they are providing to their partner). Se...  \n",
              "8   Finally, it was clear from the two intervention studies that the simulation provided by mobile devices, although not as powerful as the desktop-computer-powered Oculus Rift mixed reality or physical simulators, has no negative impacts on the learners. In fact, the simulation improves learners’ competence and skills with the skills intervention showing statistically significant improvements in ...  \n",
              "9   \"In summary, the results from our evaluation of personalized and general models to automatically extract orchestration graphs highlight the fact that machine learning models can be successfully trained with such multimodal sensor data, using relatively low-level features.\"\\n\\n\"In summary, the results from our evaluation of personalized and general models to automatically extract orchestration ...  \n",
              "10  Corse-grained learning trajectories were able to identify key moments during learning that warrented futher analysis with other MMD\\n\\n\"Our results from this analysis showed how students’ early experiences struggling with a novel concept could significantly affect both their entire learning trajectories within an activity and pre-test–post-test measurements of learning gains related to that co...  \n",
              "11  We presented two empirical studies, collected in classroom studies with two distinct learning technology systems in different contexts (individual and collaborative). Our analyses and findings showcase a few different ways, in which multimodal data sources can enrich our understanding of student learning and provide a more holistic picture.\\n\\nAuthors collected student‐focused screen and webca...  \n",
              "12  ``There was no difference between the proportion of the session spent being bored or frustrated or in the achievement scores for the two conditions. In order to determine whether the effect on achievement was different depending on length of exposure, participants with a duration less than 60 minutes were excluded from the analysis. However, there was still no significant difference between ac...  \n",
              "13  Feedback generated by RAP is similar to human feedback across several dimensions and similar to other more complex systems\\n\\n\"It revealed an overwhelmingly positive perception of the system especially in the dimensions of perceived usefulness and feedback which were rated as excellent by 65% and 58% of the students respectively. The qualitative analysis helped discover specific issues, on the...  \n",
              "14  RQ1: Better communication, better collaboration\\nRQ2: Collaborative teams showed lower variability in the estimates of story points (same page)\\nRQ3: Democratic leadership in collaborative groups\\n\\nThe results obtained allowed the detection of important relationships and characteristics of the collaborative and Non-Collaborative groups, with productivity, effort, and predominant personality s...  \n",
              "15  Authors present an initial pilot using these methods in concert to identify key moments in multiple modalities. While the use of constructivist dialogue mapping showed that the users learned during their interaction with Ant Adaption, emotional logging identified alternative moments of learning outside of their analytic framework.\\n\\nThis paper presented a preliminary approach to augment quali...  \n",
              "16  The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings\\n\\nWithin heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> mode...  \n",
              "19  \"This study contributes an initial understanding into how different modalities mediate students’ interactions and offers implications for scaffolding peer interactions during multimodal composing processes.\"\\n\\n\"Overall, students were more likely to share ideas and ask questions, and they tended to use quick- response strategies while multimodal composing.\"\\n\\n\"Students’ interview responses al...  \n",
              "22  We found that idling with limited speech (i.e., silence or backchannel feedback only) and without movement was negatively correlated with task performance and with participants’ subjective perceptions of the collaboration. However, being silent and focused during solution execution was positively correlated with task performance. Results illustrate that in some cases, multimodal patterns impro...  \n",
              "24  This is the first study to evaluate an adaptive learning system for learners with ID based on multi-modal affect recognition. Three separate states were automatically identified, with lower levels of the state labelled “boredom” having the strongest link to learning achievement. Both those labelled “frustration” and “engagement” were positively related to achievement. Our results are in line w...  \n",
              "25  Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoid...  \n",
              "28  Overall, our study suggests that coupling style can be operationalised as a multimodal indicator suitable to investigate complex collaboration dynamics in games, thus confirming what has been found in other domains\\n\\n\"Firstly, we found that coupling styles can be used to characterise collaboration in a co-located SG. Consistently with previous research (Isenberg et al, 2010; Niu et al, 2018),...  \n",
              "31  \"We conducted a case study to compare the visualizations provided by the system in two different situations: collaborative and competitive activities. The results suggest that the provided visualizations help to identify issues on cognitive contribution, assimilation, self-regulation, and integration of the team members. They could also support teachers to decide whether they must assist a tea...  \n",
              "32  \"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Exploration, increasing confluence during Discovery, and high confluence during Fluency.\"\\n\\n\"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Explorati...  \n",
              "33  \"In this study, we have given special emphasis to the interpersonal metafunction (see Fig. 1), which is crucial to enhance EMI students’ active participation and engagement. The outcomes of this study point to the need to address multimodal interactional competence in professional development programs, as described in Morell et al. (2022) to promote effective lecturing in the diverse EMI scena...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3faf5312-9b2a-45b4-97a6-45c1e8190e21\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type</th>\n",
              "      <th>Data Collection Mediums</th>\n",
              "      <th>Modalities</th>\n",
              "      <th>Analysis Methods</th>\n",
              "      <th>Fusion Types</th>\n",
              "      <th>Publication</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Results</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1118315889</td>\n",
              "      <td>using multimodal learning analytics to identify aspects of collaboration in project-based learning</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS</td>\n",
              "      <td>POSE,PROS</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>CSCL</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"physical aspect of collaborative is an important part of this type of learning and that learning analytics systems can identify features that are relevant for helping researchers, teachers, and learners unpack what is happening.\"\\n\\n\"In this research study, we presented that where the students are looking, the distance between them, the motion of their hands our key features for a learning an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3339002981</td>\n",
              "      <td>estimation of success in collaborative learning based on multimodal learning analytics features</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,LOGS,VIDEO,AUDIO</td>\n",
              "      <td>GAZE,LOGS,PROS,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICALT</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, authors found that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.\\n\\nPredicting the learning gains via classification (bad, ok, good) through gaze, logs, audio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3093310941</td>\n",
              "      <td>embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders</td>\n",
              "      <td>Hiroki Tanaka</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>AUDIO,VIDEO,PPA</td>\n",
              "      <td>POSE,PROS,AFFECT</td>\n",
              "      <td>REG,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>PLOS</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>We identified a positive correlation between overall narrative skills and other non-verbal behaviors. This result indicates that the overall narrative skills were subjectively decided based on other non-verbal behavior skills [22]. Although the automated social skills trainer did not provide feedback regarding narrative structure or eye gaze, these elements were also improved after the trainin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3095923626</td>\n",
              "      <td>a multimodal analysis of making</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR,PPA,INTER</td>\n",
              "      <td>GEST,PPA,EDA,ACT,PROS,QUAL,INTER</td>\n",
              "      <td>STATS,CLUST,QUAL,PATT</td>\n",
              "      <td>EARLY</td>\n",
              "      <td>IJAIED</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"each approach provides different affordances depending on the similarity metric and the dependent variable.\"\\n \"The take-away is that there are several strategies for doing multimodal learning analytics, and that many of these approaches can provide a meaningful glimpse into a complex data set, glimpses that may be difficult to identify using traditional approaches.\"\\n\\n\"Looking across analys...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>85990093</td>\n",
              "      <td>multimodal markers of persuasive speech : designing a virtual debate coach</td>\n",
              "      <td>Volha Petukhova</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>PROS,GEST</td>\n",
              "      <td>CLS,QUAL,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>INTERSPEECH</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"Experiments of different types supported fairly reliable identification of markers from multimodal data, and linking these to assessments of debater confidence level and intensification behaviour.\"\\n\\n\"In line with previous empirical findings, we acknowledge that persuasive speech is rather difficult to characterize. Neverthe- less, based on theoretical and empirical frameworks set up by Gric...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1637690235</td>\n",
              "      <td>supervised machine learning in multimodal learning analytics for estimating success in project-based learning</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,PPA,RPA</td>\n",
              "      <td>POSE,GEST,PROS,LOGS,PPA,RPA</td>\n",
              "      <td>REG,CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"In this paper, we show that MMLA and the state-of-the-art computational techniques can be used to generate insights into the \"black box\" of learning in students’ project-based activities. These insights generated from multimodal data can be used to inform teachers about the key features of project-based learning and help them support students appropriately in similar pedagogical approaches.\"\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2181637610</td>\n",
              "      <td>toward using multi-modal learning analytics to support and measure collaboration in co-located dyads</td>\n",
              "      <td>Emma L. Starr</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA,RPA,LOGS</td>\n",
              "      <td>POSE,PROS,PPA,RPA,LOGS</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ICLS</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>While this study was not able to show a clear effect of providing a real-time visualization to support\\ncollaboration, it made many other valuable contributions. First, it showed that simple verbal interventions can\\nhelp participants pay attention to particular aspects of their collaborative behavior (i.e., how much they are\\ntalking and how much space they are providing to their partner). Se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3146393211</td>\n",
              "      <td>mobile mixed reality for experiential learning and simulation in medical and health sciences education</td>\n",
              "      <td>James Birt</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>PPA,INTER</td>\n",
              "      <td>PPA,TRANS</td>\n",
              "      <td>QUAL,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>Information</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>Finally, it was clear from the two intervention studies that the simulation provided by mobile devices, although not as powerful as the desktop-computer-powered Oculus Rift mixed reality or physical simulators, has no negative impacts on the learners. In fact, the simulation improves learners’ competence and skills with the skills intervention showing statistically significant improvements in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3135645357</td>\n",
              "      <td>multimodal teaching analytics: automated extraction of orchestration graphs from wearable sensor data</td>\n",
              "      <td>Luis P. Prieto</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,VIDEO,AUDIO,MOTION</td>\n",
              "      <td>GAZE,PROS,ACT,PIXEL</td>\n",
              "      <td>NET,CLS,STATS,PATT,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"In summary, the results from our evaluation of personalized and general models to automatically extract orchestration graphs highlight the fact that machine learning models can be successfully trained with such multimodal sensor data, using relatively low-level features.\"\\n\\n\"In summary, the results from our evaluation of personalized and general models to automatically extract orchestration ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3783339081</td>\n",
              "      <td>a novel method for the in-depth multimodal analysis of student learning trajectories in intelligent tutoring systems</td>\n",
              "      <td>Ran Liu</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,SCREEN,PPA</td>\n",
              "      <td>LOGS,TRANS,ACT,QUAL,PPA</td>\n",
              "      <td>STATS,REG,QUAL</td>\n",
              "      <td>MID</td>\n",
              "      <td>JLA</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>Corse-grained learning trajectories were able to identify key moments during learning that warrented futher analysis with other MMD\\n\\n\"Our results from this analysis showed how students’ early experiences struggling with a novel concept could significantly affect both their entire learning trajectories within an activity and pre-test–post-test measurements of learning gains related to that co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>3796180663</td>\n",
              "      <td>learning linkages: integrating data streams of multiple modalities and timescales</td>\n",
              "      <td>Ran Liu</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,SCREEN,PPA</td>\n",
              "      <td>TRANS,QUAL,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>We presented two empirical studies, collected in classroom studies with two distinct learning technology systems in different contexts (individual and collaborative). Our analyses and findings showcase a few different ways, in which multimodal data sources can enrich our understanding of student learning and provide a more holistic picture.\\n\\nAuthors collected student‐focused screen and webca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2345021698</td>\n",
              "      <td>exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course</td>\n",
              "      <td>René Noël</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,PPA,RPA</td>\n",
              "      <td>RPA,PROS</td>\n",
              "      <td>QUAL,NET,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>Access</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>``There was no difference between the proportion of the session spent being bored or frustrated or in the achievement scores for the two conditions. In order to determine whether the effect on achievement was different depending on length of exposure, participants with a duration less than 60 minutes were excluded from the analysis. However, there was still no significant difference between ac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2497456347</td>\n",
              "      <td>the rap system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2018</td>\n",
              "      <td>Training</td>\n",
              "      <td>AUDIO,VIDEO,PPA,SURVEY</td>\n",
              "      <td>PPA,GAZE,POSE,PROS</td>\n",
              "      <td>CLS,STATS,QUAL</td>\n",
              "      <td>LATE</td>\n",
              "      <td>LAK</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>Feedback generated by RAP is similar to human feedback across several dimensions and similar to other more complex systems\\n\\n\"It revealed an overwhelmingly positive perception of the system especially in the dimensions of perceived usefulness and feedback which were rated as excellent by 65% and 58% of the students respectively. The qualitative analysis helped discover specific issues, on the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>4019205162</td>\n",
              "      <td>introducing low-cost sensors into the classroom settings: improving the assessment in agile practices with multimodal learning analytics</td>\n",
              "      <td>Hector Cornide-Reyes</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,SURVEY,PPA,RPA</td>\n",
              "      <td>SURVEY,TRANS,PPA,RPA</td>\n",
              "      <td>NET,STATS</td>\n",
              "      <td>MID,OTH</td>\n",
              "      <td>Sensors</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>RQ1: Better communication, better collaboration\\nRQ2: Collaborative teams showed lower variability in the estimates of story points (same page)\\nRQ3: Democratic leadership in collaborative groups\\n\\nThe results obtained allowed the detection of important relationships and characteristics of the collaborative and Non-Collaborative groups, with productivity, effort, and predominant personality s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1847468084</td>\n",
              "      <td>computationally augmented ethnography: emotion tracking and learning in museum games</td>\n",
              "      <td>Kit Martin</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA,RPA</td>\n",
              "      <td>TRANS,AFFECT,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ICQE</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>Authors present an initial pilot using these methods in concert to identify key moments in multiple modalities. While the use of constructivist dialogue mapping showed that the users learned during their interaction with Ant Adaption, emotional logging identified alternative moments of learning outside of their analytic framework.\\n\\nThis paper presented a preliminary approach to augment quali...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "      <td>The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings\\n\\nWithin heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -&gt; mode...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>86191824</td>\n",
              "      <td>examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes</td>\n",
              "      <td>Shiyan Jiang</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SCREEN,INTER,PPA,AUDIO</td>\n",
              "      <td>INTER,QUAL,TRANS</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ILE</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"This study contributes an initial understanding into how different modalities mediate students’ interactions and offers implications for scaffolding peer interactions during multimodal composing processes.\"\\n\\n\"Overall, students were more likely to share ideas and ask questions, and they tended to use quick- response strategies while multimodal composing.\"\\n\\n\"Students’ interview responses al...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1770989706</td>\n",
              "      <td>focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving</td>\n",
              "      <td>Hana Vrzakova</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,SCREEN,SURVEY,PPA</td>\n",
              "      <td>PROS,ACT,GEST,PPA</td>\n",
              "      <td>STATS,PATT</td>\n",
              "      <td>MID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>We found that idling with limited speech (i.e., silence or backchannel feedback only) and without movement was negatively correlated with task performance and with participants’ subjective perceptions of the collaboration. However, being silent and focused during solution execution was positively correlated with task performance. Results illustrate that in some cases, multimodal patterns impro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>3796643912</td>\n",
              "      <td>an evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities</td>\n",
              "      <td>Penelope J. Standen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,RPA</td>\n",
              "      <td>AFFECT,POSE,LOGS,RPA,GAZE,PROS,GEST</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>HUM, OTH, STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>This is the first study to evaluate an adaptive learning system for learners with ID based on multi-modal affect recognition. Three separate states were automatically identified, with lower levels of the state labelled “boredom” having the strongest link to learning achievement. Both those labelled “frustration” and “engagement” were positively related to achievement. Our results are in line w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MF</td>\n",
              "      <td>Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>518268671</td>\n",
              "      <td>using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game</td>\n",
              "      <td>María Ximena López</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,LOGS,AUDIO,VIDEO</td>\n",
              "      <td>LOGS,SURVEY,GAZE,PROS</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ECGBL</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>Overall, our study suggests that coupling style can be operationalised as a multimodal indicator suitable to investigate complex collaboration dynamics in games, thus confirming what has been found in other domains\\n\\n\"Firstly, we found that coupling styles can be used to characterise collaboration in a co-located SG. Consistently with previous research (Isenberg et al, 2010; Niu et al, 2018),...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>2609260641</td>\n",
              "      <td>visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication</td>\n",
              "      <td>René Noël</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,RPA,INTER</td>\n",
              "      <td>PROS,POSE,RPA,INTER,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>DAMLE</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>PROF, UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"We conducted a case study to compare the visualizations provided by the system in two different situations: collaborative and competitive activities. The results suggest that the provided visualizations help to identify issues on cognitive contribution, assimilation, self-regulation, and integration of the team members. They could also support teachers to decide whether they must assist a tea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1345598079</td>\n",
              "      <td>intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning</td>\n",
              "      <td>Sofia Tancredi</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,VIDEO,AUDIO,INTER</td>\n",
              "      <td>GAZE,GEST,TRANS,POSE,INTER</td>\n",
              "      <td>PATT,QUAL,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MMLA Handbook</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Exploration, increasing confluence during Discovery, and high confluence during Fluency.\"\\n\\n\"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Explorati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>2155422499</td>\n",
              "      <td>a multimodal analysis of pair work engagement episodes: implications for emi lecturer training</td>\n",
              "      <td>Teresa Morell</td>\n",
              "      <td>2022</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>TRANS,PPA,QUAL,POSE,ACT</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>JEAP</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"In this study, we have given special emphasis to the interpersonal metafunction (see Fig. 1), which is crucial to enhance EMI students’ active participation and engagement. The outcomes of this study point to the need to address multimodal interactional competence in professional development programs, as described in Morell et al. (2022) to promote effective lecturing in the diverse EMI scena...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3faf5312-9b2a-45b4-97a6-45c1e8190e21')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3faf5312-9b2a-45b4-97a6-45c1e8190e21 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3faf5312-9b2a-45b4-97a6-45c1e8190e21');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f0b42031-e7f2-4a7b-bbd7-e02c20b52d21\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f0b42031-e7f2-4a7b-bbd7-e02c20b52d21')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f0b42031-e7f2-4a7b-bbd7-e02c20b52d21 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b028b4e7-af36-4ceb-a2bd-813960b33423\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_pros_xor_aud')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b028b4e7-af36-4ceb-a2bd-813960b33423 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_pros_xor_aud');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis methods."
      ],
      "metadata": {
        "id": "bMYMCJasrEqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analysis = {}\n",
        "for _,row in df_nlp.iterrows():\n",
        "    anal = row[\"Analysis Methods\"].split(\",\")\n",
        "    for a in anal:\n",
        "        analysis[a] = analysis.get(a, 0) + 1\n",
        "analysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggJetWtVrGlG",
        "outputId": "3cd71c30-a40c-4450-af7c-ca5e57a710dd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'REG': 6, 'CLS': 14, 'STATS': 21, 'CLUST': 3, 'QUAL': 19, 'PATT': 4, 'NET': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analysis_all = {}\n",
        "for _,row in df_all.iterrows():\n",
        "    anal = row[\"Analysis Methods\"].split(\",\")\n",
        "    for a in anal:\n",
        "        analysis_all[a] = analysis_all.get(a, 0) + 1\n",
        "analysis_all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyYiUvSj7otf",
        "outputId": "a692f19b-03bc-40bc-f7bd-c68b63ace130"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CLUST': 9,\n",
              " 'QUAL': 29,\n",
              " 'CLS': 39,\n",
              " 'REG': 12,\n",
              " 'STATS': 34,\n",
              " 'PATT': 9,\n",
              " 'NET': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qualitative analysis only."
      ],
      "metadata": {
        "id": "-21Z5H6joYzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quals_count = 0\n",
        "for _,row in df_nlp.iterrows():\n",
        "    if row[\"Analysis Methods\"] == \"QUAL\":\n",
        "      quals_count+=1\n",
        "quals_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvXxlAiBoayv",
        "outputId": "60d0005d-5b2c-47d2-ffc3-83964ee2f07c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quals_count_all = 0\n",
        "for _,row in df_all.iterrows():\n",
        "    if row[\"Analysis Methods\"] == \"QUAL\":\n",
        "      quals_count_all+=1\n",
        "quals_count_all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFOLkApUoyq3",
        "outputId": "4c83dac7-7190-48b0-847c-bc3e2be50160"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fusion."
      ],
      "metadata": {
        "id": "zs-TLHkJsN5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fusion = {}\n",
        "for _,row in df_nlp.iterrows():\n",
        "    fus = row[\"Fusion Types\"].split(\",\")\n",
        "    for f in fus:\n",
        "        fusion[f] = fusion.get(f, 0) + 1\n",
        "fusion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "182a18ac-7763-4aef-9570-b4d693b780f6",
        "id": "qOj8YN4DsN6A"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'MID': 12, 'EARLY': 1, 'OTH': 15, 'HYBRID': 5, 'LATE': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_no_fusion = df_nlp[~df_nlp[\"Fusion Types\"].str.contains(\"MID\") & \\\n",
        "                      ~df_nlp[\"Fusion Types\"].str.contains(\"EARLY\") & \\\n",
        "                      ~df_nlp[\"Fusion Types\"].str.contains(\"LATE\") & \\\n",
        "                      ~df_nlp[\"Fusion Types\"].str.contains(\"HYBRID\")]\n",
        "print(len(df_no_fusion))\n",
        "df_no_fusion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OO5abnS7tZIK",
        "outputId": "9d7833dd-cbe5-455b-96b7-cd4c18021f8b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          UUID  \\\n",
              "7   2181637610   \n",
              "8   3146393211   \n",
              "12  2345021698   \n",
              "15  1847468084   \n",
              "18  3398902089   \n",
              "19    86191824   \n",
              "21  1296637108   \n",
              "25  2634033325   \n",
              "27   666050348   \n",
              "28   518268671   \n",
              "30    32184286   \n",
              "31  2609260641   \n",
              "32  1345598079   \n",
              "33  2155422499   \n",
              "\n",
              "                                                                                                                                    Title  \\\n",
              "7                                    toward using multi-modal learning analytics to support and measure collaboration in co-located dyads   \n",
              "8                                  mobile mixed reality for experiential learning and simulation in medical and health sciences education   \n",
              "12      exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course   \n",
              "15                                                   computationally augmented ethnography: emotion tracking and learning in museum games   \n",
              "18                                             what multimodal data can tell us about the students’ regulation of their learning process?   \n",
              "19              examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes   \n",
              "21                                                            towards collaboration translucence: giving meaning to multimodal group data   \n",
              "25                            controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "27                                                   multicraft: a multimodal interface for supporting and studying learning in minecraft   \n",
              "28                              using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game   \n",
              "30                                                                      once more with feeling: emotions in multimodal learning analytics   \n",
              "31                           visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication   \n",
              "32  intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning   \n",
              "33                                         a multimodal analysis of pair work engagement episodes: implications for emi lecturer training   \n",
              "\n",
              "          First Author  Year Environment Type  \\\n",
              "7        Emma L. Starr  2018         Learning   \n",
              "8           James Birt  2018         Learning   \n",
              "12           René Noël  2018         Learning   \n",
              "15          Kit Martin  2019         Learning   \n",
              "18       Sanna Järvelä  2019         Learning   \n",
              "19        Shiyan Jiang  2019         Learning   \n",
              "21  Vanessa Echeverria  2019         Training   \n",
              "25        Xavier Ochoa  2020         Training   \n",
              "27     Marcelo Worsley  2021         Learning   \n",
              "28  María Ximena López  2021         Learning   \n",
              "30       Marcus Kubsch  2022         Learning   \n",
              "31           René Noël  2022         Learning   \n",
              "32      Sofia Tancredi  2022         Learning   \n",
              "33       Teresa Morell  2022         Training   \n",
              "\n",
              "                          Data Collection Mediums  \\\n",
              "7                        VIDEO,AUDIO,PPA,RPA,LOGS   \n",
              "8                                       PPA,INTER   \n",
              "12                                  AUDIO,PPA,RPA   \n",
              "15                            VIDEO,AUDIO,PPA,RPA   \n",
              "18                             SENSOR,VIDEO,AUDIO   \n",
              "19                         SCREEN,INTER,PPA,AUDIO   \n",
              "21           VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER   \n",
              "25                                VIDEO,AUDIO,PPA   \n",
              "27  AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS   \n",
              "28                        SURVEY,LOGS,AUDIO,VIDEO   \n",
              "30                               SURVEY,PPA,AUDIO   \n",
              "31                          AUDIO,VIDEO,RPA,INTER   \n",
              "32                          EYE,VIDEO,AUDIO,INTER   \n",
              "33                                VIDEO,AUDIO,PPA   \n",
              "\n",
              "                                Modalities Analysis Methods Fusion Types  \\\n",
              "7                   POSE,PROS,PPA,RPA,LOGS       STATS,QUAL          OTH   \n",
              "8                                PPA,TRANS       QUAL,STATS          OTH   \n",
              "12                                RPA,PROS   QUAL,NET,STATS          OTH   \n",
              "15                       TRANS,AFFECT,QUAL             QUAL          OTH   \n",
              "18                         EDA,AFFECT,QUAL             QUAL          OTH   \n",
              "19                        INTER,QUAL,TRANS             QUAL          OTH   \n",
              "21      POSE,LOGS,TRANS,EDA,ACT,PROS,INTER             QUAL          OTH   \n",
              "25                           POSE,PROS,PPA            STATS          OTH   \n",
              "27  PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS             QUAL          OTH   \n",
              "28                   LOGS,SURVEY,GAZE,PROS            STATS          OTH   \n",
              "30          INTER,SURVEY,TRANS,PROS,AFFECT    CLS,REG,STATS          OTH   \n",
              "31                PROS,POSE,RPA,INTER,QUAL             QUAL          OTH   \n",
              "32              GAZE,GEST,TRANS,POSE,INTER  PATT,QUAL,STATS          OTH   \n",
              "33                 TRANS,PPA,QUAL,POSE,ACT             QUAL          OTH   \n",
              "\n",
              "      Publication Environment Setting Environment Subject  \\\n",
              "7            ICLS                BLND                STEM   \n",
              "8     Information                BLND                STEM   \n",
              "12         Access                PHYS                STEM   \n",
              "15           ICQE                VIRT                STEM   \n",
              "18            LAI                BLND                STEM   \n",
              "19            ILE                VIRT                STEM   \n",
              "21            CHI                PHYS                STEM   \n",
              "25           BJET                BLND                 HUM   \n",
              "27           HCII                VIRT                STEM   \n",
              "28          ECGBL                BLND                STEM   \n",
              "30  MMLA Handbook                PHYS                STEM   \n",
              "31          DAMLE                PHYS                 HUM   \n",
              "32  MMLA Handbook                VIRT                STEM   \n",
              "33           JEAP                PHYS                 OTH   \n",
              "\n",
              "   Participant Structure Didactic Nature Level of Instruction or Training  \\\n",
              "7                  MULTI             INF                              UNI   \n",
              "8                    IND           INSTR                              UNI   \n",
              "12                 MULTI             INF                              UNI   \n",
              "15                 MULTI             INF                             UNSP   \n",
              "18                 MULTI           INSTR                              K12   \n",
              "19                 MULTI           INSTR                              K12   \n",
              "21                 MULTI           TRAIN                              UNI   \n",
              "25                   IND           TRAIN                             UNSP   \n",
              "27                   IND             INF                              K12   \n",
              "28                 MULTI           INSTR                              UNI   \n",
              "30                   IND           INSTR                              K12   \n",
              "31                 MULTI             INF                        PROF, UNI   \n",
              "32                   IND           INSTR                              K12   \n",
              "33                 MULTI           TRAIN                             PROF   \n",
              "\n",
              "   Analysis Approach  \\\n",
              "7                 MF   \n",
              "8                 MF   \n",
              "12            MB, MF   \n",
              "15            MB, MF   \n",
              "18                MF   \n",
              "19                MB   \n",
              "21                MB   \n",
              "25                MF   \n",
              "27                MF   \n",
              "28                MF   \n",
              "30            MB, MF   \n",
              "31                MF   \n",
              "32                MF   \n",
              "33                MF   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                            Results  \n",
              "7   While this study was not able to show a clear effect of providing a real-time visualization to support\\ncollaboration, it made many other valuable contributions. First, it showed that simple verbal interventions can\\nhelp participants pay attention to particular aspects of their collaborative behavior (i.e., how much they are\\ntalking and how much space they are providing to their partner). Se...  \n",
              "8   Finally, it was clear from the two intervention studies that the simulation provided by mobile devices, although not as powerful as the desktop-computer-powered Oculus Rift mixed reality or physical simulators, has no negative impacts on the learners. In fact, the simulation improves learners’ competence and skills with the skills intervention showing statistically significant improvements in ...  \n",
              "12  ``There was no difference between the proportion of the session spent being bored or frustrated or in the achievement scores for the two conditions. In order to determine whether the effect on achievement was different depending on length of exposure, participants with a duration less than 60 minutes were excluded from the analysis. However, there was still no significant difference between ac...  \n",
              "15  Authors present an initial pilot using these methods in concert to identify key moments in multiple modalities. While the use of constructivist dialogue mapping showed that the users learned during their interaction with Ant Adaption, emotional logging identified alternative moments of learning outside of their analytic framework.\\n\\nThis paper presented a preliminary approach to augment quali...  \n",
              "18  Our major claim is that multichannel data can be potential for understanding regulatory processes in collaboration. With our five empirical case examples, we illustrate how triangulating multiple sources of data has potential to advance the theoretical and conceptual progress in social aspects of SRL theory.\\n\\nAuthors show with five empirical cases that multichannel data can be potential for ...  \n",
              "19  \"This study contributes an initial understanding into how different modalities mediate students’ interactions and offers implications for scaffolding peer interactions during multimodal composing processes.\"\\n\\n\"Overall, students were more likely to share ideas and ask questions, and they tended to use quick- response strategies while multimodal composing.\"\\n\\n\"Students’ interview responses al...  \n",
              "21  \"We illustrate the approach from the automated analysis of healthcare simulations to train nurses, generating four visual proxies that fuse multimodal data into higher order patterns.\"\\n\\n\"we introduce the concept of collaboration translucence, an approach to make visible selected features of group activity. This is grounded both theoretically (in the physical, epistemic, social and affective ...  \n",
              "25  Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoid...  \n",
              "27  \"Through our user studies, we found that the platform helps fulfill some of those goals by providing capabilities that can spur on amazement and excitement among traditional Minecraft users and novices. We also find that many of the multimodal components, while not immediately intuitive for users, proved to be preferred modes of game play. In this sense, we feel that this tool is moving in the...  \n",
              "28  Overall, our study suggests that coupling style can be operationalised as a multimodal indicator suitable to investigate complex collaboration dynamics in games, thus confirming what has been found in other domains\\n\\n\"Firstly, we found that coupling styles can be used to characterise collaboration in a co-located SG. Consistently with previous research (Isenberg et al, 2010; Niu et al, 2018),...  \n",
              "30  Results demonstrate how NLP and ML techniques allow us to use different modalities of the same data, voice and transcript, and different modalities of different data sources, voice data from interviews, answers to a goal orientation questionnaire, and answers to open-ended questions about energy, in order to better understand individual differences in students’ performances\\n\\nUsed text and au...  \n",
              "31  \"We conducted a case study to compare the visualizations provided by the system in two different situations: collaborative and competitive activities. The results suggest that the provided visualizations help to identify issues on cognitive contribution, assimilation, self-regulation, and integration of the team members. They could also support teachers to decide whether they must assist a tea...  \n",
              "32  \"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Exploration, increasing confluence during Discovery, and high confluence during Fluency.\"\\n\\n\"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Explorati...  \n",
              "33  \"In this study, we have given special emphasis to the interpersonal metafunction (see Fig. 1), which is crucial to enhance EMI students’ active participation and engagement. The outcomes of this study point to the need to address multimodal interactional competence in professional development programs, as described in Morell et al. (2022) to promote effective lecturing in the diverse EMI scena...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6321a32d-a858-4d48-b9bd-7f9c5744f90a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type</th>\n",
              "      <th>Data Collection Mediums</th>\n",
              "      <th>Modalities</th>\n",
              "      <th>Analysis Methods</th>\n",
              "      <th>Fusion Types</th>\n",
              "      <th>Publication</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "      <th>Results</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2181637610</td>\n",
              "      <td>toward using multi-modal learning analytics to support and measure collaboration in co-located dyads</td>\n",
              "      <td>Emma L. Starr</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA,RPA,LOGS</td>\n",
              "      <td>POSE,PROS,PPA,RPA,LOGS</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ICLS</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>While this study was not able to show a clear effect of providing a real-time visualization to support\\ncollaboration, it made many other valuable contributions. First, it showed that simple verbal interventions can\\nhelp participants pay attention to particular aspects of their collaborative behavior (i.e., how much they are\\ntalking and how much space they are providing to their partner). Se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3146393211</td>\n",
              "      <td>mobile mixed reality for experiential learning and simulation in medical and health sciences education</td>\n",
              "      <td>James Birt</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>PPA,INTER</td>\n",
              "      <td>PPA,TRANS</td>\n",
              "      <td>QUAL,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>Information</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>Finally, it was clear from the two intervention studies that the simulation provided by mobile devices, although not as powerful as the desktop-computer-powered Oculus Rift mixed reality or physical simulators, has no negative impacts on the learners. In fact, the simulation improves learners’ competence and skills with the skills intervention showing statistically significant improvements in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2345021698</td>\n",
              "      <td>exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course</td>\n",
              "      <td>René Noël</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,PPA,RPA</td>\n",
              "      <td>RPA,PROS</td>\n",
              "      <td>QUAL,NET,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>Access</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>``There was no difference between the proportion of the session spent being bored or frustrated or in the achievement scores for the two conditions. In order to determine whether the effect on achievement was different depending on length of exposure, participants with a duration less than 60 minutes were excluded from the analysis. However, there was still no significant difference between ac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1847468084</td>\n",
              "      <td>computationally augmented ethnography: emotion tracking and learning in museum games</td>\n",
              "      <td>Kit Martin</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA,RPA</td>\n",
              "      <td>TRANS,AFFECT,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ICQE</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>Authors present an initial pilot using these methods in concert to identify key moments in multiple modalities. While the use of constructivist dialogue mapping showed that the users learned during their interaction with Ant Adaption, emotional logging identified alternative moments of learning outside of their analytic framework.\\n\\nThis paper presented a preliminary approach to augment quali...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3398902089</td>\n",
              "      <td>what multimodal data can tell us about the students’ regulation of their learning process?</td>\n",
              "      <td>Sanna Järvelä</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SENSOR,VIDEO,AUDIO</td>\n",
              "      <td>EDA,AFFECT,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>LAI</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>Our major claim is that multichannel data can be potential for understanding regulatory processes in collaboration. With our five empirical case examples, we illustrate how triangulating multiple sources of data has potential to advance the theoretical and conceptual progress in social aspects of SRL theory.\\n\\nAuthors show with five empirical cases that multichannel data can be potential for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>86191824</td>\n",
              "      <td>examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes</td>\n",
              "      <td>Shiyan Jiang</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SCREEN,INTER,PPA,AUDIO</td>\n",
              "      <td>INTER,QUAL,TRANS</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ILE</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"This study contributes an initial understanding into how different modalities mediate students’ interactions and offers implications for scaffolding peer interactions during multimodal composing processes.\"\\n\\n\"Overall, students were more likely to share ideas and ask questions, and they tended to use quick- response strategies while multimodal composing.\"\\n\\n\"Students’ interview responses al...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1296637108</td>\n",
              "      <td>towards collaboration translucence: giving meaning to multimodal group data</td>\n",
              "      <td>Vanessa Echeverria</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER</td>\n",
              "      <td>POSE,LOGS,TRANS,EDA,ACT,PROS,INTER</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>CHI</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "      <td>\"We illustrate the approach from the automated analysis of healthcare simulations to train nurses, generating four visual proxies that fuse multimodal data into higher order patterns.\"\\n\\n\"we introduce the concept of collaboration translucence, an approach to make visible selected features of group activity. This is grounded both theoretically (in the physical, epistemic, social and affective ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MF</td>\n",
              "      <td>Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>666050348</td>\n",
              "      <td>multicraft: a multimodal interface for supporting and studying learning in minecraft</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS</td>\n",
              "      <td>PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>HCII</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"Through our user studies, we found that the platform helps fulfill some of those goals by providing capabilities that can spur on amazement and excitement among traditional Minecraft users and novices. We also find that many of the multimodal components, while not immediately intuitive for users, proved to be preferred modes of game play. In this sense, we feel that this tool is moving in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>518268671</td>\n",
              "      <td>using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game</td>\n",
              "      <td>María Ximena López</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,LOGS,AUDIO,VIDEO</td>\n",
              "      <td>LOGS,SURVEY,GAZE,PROS</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ECGBL</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>Overall, our study suggests that coupling style can be operationalised as a multimodal indicator suitable to investigate complex collaboration dynamics in games, thus confirming what has been found in other domains\\n\\n\"Firstly, we found that coupling styles can be used to characterise collaboration in a co-located SG. Consistently with previous research (Isenberg et al, 2010; Niu et al, 2018),...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>32184286</td>\n",
              "      <td>once more with feeling: emotions in multimodal learning analytics</td>\n",
              "      <td>Marcus Kubsch</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,PPA,AUDIO</td>\n",
              "      <td>INTER,SURVEY,TRANS,PROS,AFFECT</td>\n",
              "      <td>CLS,REG,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MMLA Handbook</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "      <td>Results demonstrate how NLP and ML techniques allow us to use different modalities of the same data, voice and transcript, and different modalities of different data sources, voice data from interviews, answers to a goal orientation questionnaire, and answers to open-ended questions about energy, in order to better understand individual differences in students’ performances\\n\\nUsed text and au...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>2609260641</td>\n",
              "      <td>visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication</td>\n",
              "      <td>René Noël</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,RPA,INTER</td>\n",
              "      <td>PROS,POSE,RPA,INTER,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>DAMLE</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>PROF, UNI</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"We conducted a case study to compare the visualizations provided by the system in two different situations: collaborative and competitive activities. The results suggest that the provided visualizations help to identify issues on cognitive contribution, assimilation, self-regulation, and integration of the team members. They could also support teachers to decide whether they must assist a tea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1345598079</td>\n",
              "      <td>intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning</td>\n",
              "      <td>Sofia Tancredi</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,VIDEO,AUDIO,INTER</td>\n",
              "      <td>GAZE,GEST,TRANS,POSE,INTER</td>\n",
              "      <td>PATT,QUAL,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MMLA Handbook</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Exploration, increasing confluence during Discovery, and high confluence during Fluency.\"\\n\\n\"Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Explorati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>2155422499</td>\n",
              "      <td>a multimodal analysis of pair work engagement episodes: implications for emi lecturer training</td>\n",
              "      <td>Teresa Morell</td>\n",
              "      <td>2022</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>TRANS,PPA,QUAL,POSE,ACT</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>JEAP</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MF</td>\n",
              "      <td>\"In this study, we have given special emphasis to the interpersonal metafunction (see Fig. 1), which is crucial to enhance EMI students’ active participation and engagement. The outcomes of this study point to the need to address multimodal interactional competence in professional development programs, as described in Morell et al. (2022) to promote effective lecturing in the diverse EMI scena...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6321a32d-a858-4d48-b9bd-7f9c5744f90a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6321a32d-a858-4d48-b9bd-7f9c5744f90a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6321a32d-a858-4d48-b9bd-7f9c5744f90a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dfebc168-5400-4fc5-8e61-122ea53fcb6d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dfebc168-5400-4fc5-8e61-122ea53fcb6d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dfebc168-5400-4fc5-8e61-122ea53fcb6d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_71695679-887e-4079-a387-aee51fd634d5\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_no_fusion')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_71695679-887e-4079-a387-aee51fd634d5 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_no_fusion');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fusion_all = {}\n",
        "for _,row in df_all.iterrows():\n",
        "    fus = row[\"Fusion Types\"].split(\",\")\n",
        "    for f in fus:\n",
        "        fusion_all[f] = fusion_all.get(f, 0) + 1\n",
        "fusion_all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e5e1f11-1d31-4fef-accf-f0b6167601cb",
        "id": "V2pR1gTv6UrG"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'HYBRID': 19, 'LATE': 8, 'MID': 27, 'EARLY': 3, 'OTH': 20}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_no_fusion_all = df_all[~df_all[\"Fusion Types\"].str.contains(\"MID\") & \\\n",
        "                      ~df_all[\"Fusion Types\"].str.contains(\"EARLY\") & \\\n",
        "                      ~df_all[\"Fusion Types\"].str.contains(\"LATE\") & \\\n",
        "                      ~df_all[\"Fusion Types\"].str.contains(\"HYBRID\")]\n",
        "print(len(df_no_fusion_all))\n",
        "df_no_fusion_all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c687f7f-f327-4538-c87d-17078cab2cf1",
        "id": "0ph4Ezwe6UrH"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          UUID  \\\n",
              "13  2181637610   \n",
              "15  3146393211   \n",
              "16  3308658121   \n",
              "22  2345021698   \n",
              "27  1847468084   \n",
              "32  3398902089   \n",
              "33    86191824   \n",
              "35  1296637108   \n",
              "45  2055153191   \n",
              "47  2879332689   \n",
              "51  2634033325   \n",
              "55  4035649049   \n",
              "58   666050348   \n",
              "59   518268671   \n",
              "60  3660066725   \n",
              "68    32184286   \n",
              "69  2609260641   \n",
              "70  1345598079   \n",
              "71  2155422499   \n",
              "\n",
              "                                                                                                                                    Title  \\\n",
              "13                                   toward using multi-modal learning analytics to support and measure collaboration in co-located dyads   \n",
              "15                                 mobile mixed reality for experiential learning and simulation in medical and health sciences education   \n",
              "16                                                        exploring collaboration using motion sensors and multi-modal learning analytics   \n",
              "22      exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course   \n",
              "27                                                   computationally augmented ethnography: emotion tracking and learning in museum games   \n",
              "32                                             what multimodal data can tell us about the students’ regulation of their learning process?   \n",
              "33              examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes   \n",
              "35                                                            towards collaboration translucence: giving meaning to multimodal group data   \n",
              "45                                   round or rectangular tables for collaborative problem solving? a multimodal learning analytics study   \n",
              "47                                               from data to insights: a layered storytelling approach for multimodal learning analytics   \n",
              "51                            controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "55                                                     storytelling with learner data: guiding student reflection on multimodal team data   \n",
              "58                                                   multicraft: a multimodal interface for supporting and studying learning in minecraft   \n",
              "59                              using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game   \n",
              "60        children's play and problem solving in motion-based educational games: synergies between human annotations and multi-modal data   \n",
              "68                                                                      once more with feeling: emotions in multimodal learning analytics   \n",
              "69                           visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication   \n",
              "70  intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning   \n",
              "71                                         a multimodal analysis of pair work engagement episodes: implications for emi lecturer training   \n",
              "\n",
              "                  First Author  Year Environment Type  \\\n",
              "13               Emma L. Starr  2018         Learning   \n",
              "15                  James Birt  2018         Learning   \n",
              "16            Joseph M. Reilly  2018         Learning   \n",
              "22                   René Noël  2018         Learning   \n",
              "27                  Kit Martin  2019         Learning   \n",
              "32               Sanna Järvelä  2019         Learning   \n",
              "33                Shiyan Jiang  2019         Learning   \n",
              "35          Vanessa Echeverria  2019         Training   \n",
              "45              Milica Vujovic  2020         Learning   \n",
              "47  Roberto Martinez-Maldonado  2020         Training   \n",
              "51                Xavier Ochoa  2020         Training   \n",
              "55      Gloria Fernández-Nieto  2021         Training   \n",
              "58             Marcelo Worsley  2021         Learning   \n",
              "59          María Ximena López  2021         Learning   \n",
              "60          Serena Lee-Cultura  2021         Learning   \n",
              "68               Marcus Kubsch  2022         Learning   \n",
              "69                   René Noël  2022         Learning   \n",
              "70              Sofia Tancredi  2022         Learning   \n",
              "71               Teresa Morell  2022         Training   \n",
              "\n",
              "                          Data Collection Mediums  \\\n",
              "13                       VIDEO,AUDIO,PPA,RPA,LOGS   \n",
              "15                                      PPA,INTER   \n",
              "16                                VIDEO,AUDIO,PPA   \n",
              "22                                  AUDIO,PPA,RPA   \n",
              "27                            VIDEO,AUDIO,PPA,RPA   \n",
              "32                             SENSOR,VIDEO,AUDIO   \n",
              "33                         SCREEN,INTER,PPA,AUDIO   \n",
              "35           VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER   \n",
              "45                                   VIDEO,MOTION   \n",
              "47                   LOGS,MOTION,SENSOR,RPA,VIDEO   \n",
              "51                                VIDEO,AUDIO,PPA   \n",
              "55                                SENSOR,LOGS,RPA   \n",
              "58  AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS   \n",
              "59                        SURVEY,LOGS,AUDIO,VIDEO   \n",
              "60                          VIDEO,SENSOR,EYE,LOGS   \n",
              "68                               SURVEY,PPA,AUDIO   \n",
              "69                          AUDIO,VIDEO,RPA,INTER   \n",
              "70                          EYE,VIDEO,AUDIO,INTER   \n",
              "71                                VIDEO,AUDIO,PPA   \n",
              "\n",
              "                                        Modalities  Analysis Methods  \\\n",
              "13                          POSE,PROS,PPA,RPA,LOGS        STATS,QUAL   \n",
              "15                                       PPA,TRANS        QUAL,STATS   \n",
              "16                               PPA,RPA,POSE,GEST  CLUST,PATT,STATS   \n",
              "22                                        RPA,PROS    QUAL,NET,STATS   \n",
              "27                               TRANS,AFFECT,QUAL              QUAL   \n",
              "32                                 EDA,AFFECT,QUAL              QUAL   \n",
              "33                                INTER,QUAL,TRANS              QUAL   \n",
              "35              POSE,LOGS,TRANS,EDA,ACT,PROS,INTER              QUAL   \n",
              "45                                   POSE,GEST,ACT        STATS,QUAL   \n",
              "47                               POSE,EDA,LOGS,RPA              QUAL   \n",
              "51                                   POSE,PROS,PPA             STATS   \n",
              "55                             EDA,LOGS,RPA,AFFECT              QUAL   \n",
              "58          PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS              QUAL   \n",
              "59                           LOGS,SURVEY,GAZE,PROS             STATS   \n",
              "60  ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP        STATS,QUAL   \n",
              "68                  INTER,SURVEY,TRANS,PROS,AFFECT     CLS,REG,STATS   \n",
              "69                        PROS,POSE,RPA,INTER,QUAL              QUAL   \n",
              "70                      GAZE,GEST,TRANS,POSE,INTER   PATT,QUAL,STATS   \n",
              "71                         TRANS,PPA,QUAL,POSE,ACT              QUAL   \n",
              "\n",
              "   Fusion Types    Publication Environment Setting Environment Subject  \\\n",
              "13          OTH           ICLS                BLND                STEM   \n",
              "15          OTH    Information                BLND                STEM   \n",
              "16          OTH            EDM                BLND                STEM   \n",
              "22          OTH         Access                PHYS                STEM   \n",
              "27          OTH           ICQE                VIRT                STEM   \n",
              "32          OTH            LAI                BLND                STEM   \n",
              "33          OTH            ILE                VIRT                STEM   \n",
              "35          OTH            CHI                PHYS                STEM   \n",
              "45          OTH           BJET                BLND           HUM, STEM   \n",
              "47          OTH            CHI                BLND                STEM   \n",
              "51          OTH           BJET                BLND                 HUM   \n",
              "55          OTH            TLT                BLND                STEM   \n",
              "58          OTH           HCII                VIRT                STEM   \n",
              "59          OTH          ECGBL                BLND                STEM   \n",
              "60          OTH            IDC                BLND                STEM   \n",
              "68          OTH  MMLA Handbook                PHYS                STEM   \n",
              "69          OTH          DAMLE                PHYS                 HUM   \n",
              "70          OTH  MMLA Handbook                VIRT                STEM   \n",
              "71          OTH           JEAP                PHYS                 OTH   \n",
              "\n",
              "   Participant Structure Didactic Nature Level of Instruction or Training  \\\n",
              "13                 MULTI             INF                              UNI   \n",
              "15                   IND           INSTR                              UNI   \n",
              "16                 MULTI           INSTR                              UNI   \n",
              "22                 MULTI             INF                              UNI   \n",
              "27                 MULTI             INF                             UNSP   \n",
              "32                 MULTI           INSTR                              K12   \n",
              "33                 MULTI           INSTR                              K12   \n",
              "35                 MULTI           TRAIN                              UNI   \n",
              "45                 MULTI           INSTR                              K12   \n",
              "47                 MULTI           TRAIN                              UNI   \n",
              "51                   IND           TRAIN                             UNSP   \n",
              "55                 MULTI           TRAIN                              UNI   \n",
              "58                   IND             INF                              K12   \n",
              "59                 MULTI           INSTR                              UNI   \n",
              "60                   IND           INSTR                              K12   \n",
              "68                   IND           INSTR                              K12   \n",
              "69                 MULTI             INF                        PROF, UNI   \n",
              "70                   IND           INSTR                              K12   \n",
              "71                 MULTI           TRAIN                             PROF   \n",
              "\n",
              "   Analysis Approach  \n",
              "13                MF  \n",
              "15                MF  \n",
              "16            MB, MF  \n",
              "22            MB, MF  \n",
              "27            MB, MF  \n",
              "32                MF  \n",
              "33                MB  \n",
              "35                MB  \n",
              "45                MF  \n",
              "47                MF  \n",
              "51                MF  \n",
              "55                MF  \n",
              "58                MF  \n",
              "59                MF  \n",
              "60                MF  \n",
              "68            MB, MF  \n",
              "69                MF  \n",
              "70                MF  \n",
              "71                MF  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cced5803-eaf2-4d04-967b-d72dbaf887a3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type</th>\n",
              "      <th>Data Collection Mediums</th>\n",
              "      <th>Modalities</th>\n",
              "      <th>Analysis Methods</th>\n",
              "      <th>Fusion Types</th>\n",
              "      <th>Publication</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2181637610</td>\n",
              "      <td>toward using multi-modal learning analytics to support and measure collaboration in co-located dyads</td>\n",
              "      <td>Emma L. Starr</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA,RPA,LOGS</td>\n",
              "      <td>POSE,PROS,PPA,RPA,LOGS</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ICLS</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3146393211</td>\n",
              "      <td>mobile mixed reality for experiential learning and simulation in medical and health sciences education</td>\n",
              "      <td>James Birt</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>PPA,INTER</td>\n",
              "      <td>PPA,TRANS</td>\n",
              "      <td>QUAL,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>Information</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3308658121</td>\n",
              "      <td>exploring collaboration using motion sensors and multi-modal learning analytics</td>\n",
              "      <td>Joseph M. Reilly</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>PPA,RPA,POSE,GEST</td>\n",
              "      <td>CLUST,PATT,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>EDM</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2345021698</td>\n",
              "      <td>exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course</td>\n",
              "      <td>René Noël</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,PPA,RPA</td>\n",
              "      <td>RPA,PROS</td>\n",
              "      <td>QUAL,NET,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>Access</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1847468084</td>\n",
              "      <td>computationally augmented ethnography: emotion tracking and learning in museum games</td>\n",
              "      <td>Kit Martin</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA,RPA</td>\n",
              "      <td>TRANS,AFFECT,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ICQE</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>3398902089</td>\n",
              "      <td>what multimodal data can tell us about the students’ regulation of their learning process?</td>\n",
              "      <td>Sanna Järvelä</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SENSOR,VIDEO,AUDIO</td>\n",
              "      <td>EDA,AFFECT,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>LAI</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>86191824</td>\n",
              "      <td>examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes</td>\n",
              "      <td>Shiyan Jiang</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SCREEN,INTER,PPA,AUDIO</td>\n",
              "      <td>INTER,QUAL,TRANS</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ILE</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1296637108</td>\n",
              "      <td>towards collaboration translucence: giving meaning to multimodal group data</td>\n",
              "      <td>Vanessa Echeverria</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER</td>\n",
              "      <td>POSE,LOGS,TRANS,EDA,ACT,PROS,INTER</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>CHI</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>2055153191</td>\n",
              "      <td>round or rectangular tables for collaborative problem solving? a multimodal learning analytics study</td>\n",
              "      <td>Milica Vujovic</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,MOTION</td>\n",
              "      <td>POSE,GEST,ACT</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM, STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>2879332689</td>\n",
              "      <td>from data to insights: a layered storytelling approach for multimodal learning analytics</td>\n",
              "      <td>Roberto Martinez-Maldonado</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>LOGS,MOTION,SENSOR,RPA,VIDEO</td>\n",
              "      <td>POSE,EDA,LOGS,RPA</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>CHI</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>4035649049</td>\n",
              "      <td>storytelling with learner data: guiding student reflection on multimodal team data</td>\n",
              "      <td>Gloria Fernández-Nieto</td>\n",
              "      <td>2021</td>\n",
              "      <td>Training</td>\n",
              "      <td>SENSOR,LOGS,RPA</td>\n",
              "      <td>EDA,LOGS,RPA,AFFECT</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>TLT</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>666050348</td>\n",
              "      <td>multicraft: a multimodal interface for supporting and studying learning in minecraft</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS</td>\n",
              "      <td>PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>HCII</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>518268671</td>\n",
              "      <td>using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game</td>\n",
              "      <td>María Ximena López</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,LOGS,AUDIO,VIDEO</td>\n",
              "      <td>LOGS,SURVEY,GAZE,PROS</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ECGBL</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>3660066725</td>\n",
              "      <td>children's play and problem solving in motion-based educational games: synergies between human annotations and multi-modal data</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SENSOR,EYE,LOGS</td>\n",
              "      <td>ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>IDC</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>32184286</td>\n",
              "      <td>once more with feeling: emotions in multimodal learning analytics</td>\n",
              "      <td>Marcus Kubsch</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,PPA,AUDIO</td>\n",
              "      <td>INTER,SURVEY,TRANS,PROS,AFFECT</td>\n",
              "      <td>CLS,REG,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MMLA Handbook</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>2609260641</td>\n",
              "      <td>visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication</td>\n",
              "      <td>René Noël</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,RPA,INTER</td>\n",
              "      <td>PROS,POSE,RPA,INTER,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>DAMLE</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>PROF, UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>1345598079</td>\n",
              "      <td>intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning</td>\n",
              "      <td>Sofia Tancredi</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,VIDEO,AUDIO,INTER</td>\n",
              "      <td>GAZE,GEST,TRANS,POSE,INTER</td>\n",
              "      <td>PATT,QUAL,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MMLA Handbook</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>2155422499</td>\n",
              "      <td>a multimodal analysis of pair work engagement episodes: implications for emi lecturer training</td>\n",
              "      <td>Teresa Morell</td>\n",
              "      <td>2022</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>TRANS,PPA,QUAL,POSE,ACT</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>JEAP</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cced5803-eaf2-4d04-967b-d72dbaf887a3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cced5803-eaf2-4d04-967b-d72dbaf887a3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cced5803-eaf2-4d04-967b-d72dbaf887a3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2876b478-07a2-41e7-a06c-e638942b908e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2876b478-07a2-41e7-a06c-e638942b908e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2876b478-07a2-41e7-a06c-e638942b908e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_938d7212-a142-4bae-aafc-e993392a9864\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_no_fusion_all')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_938d7212-a142-4bae-aafc-e993392a9864 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_no_fusion_all');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Env setting."
      ],
      "metadata": {
        "id": "9BCqtDZ6uWrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envs = {}\n",
        "for _,row in df_nlp.iterrows():\n",
        "    env = row[\"Environment Setting\"].split(\",\")\n",
        "    for e in env:\n",
        "        envs[e] = envs.get(e, 0) + 1\n",
        "envs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO8xsaf9uX6u",
        "outputId": "37c2e77a-08da-4363-afce-e9ff93e06a22"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'PHYS': 13, 'VIRT': 13, 'BLND': 8, 'UNSP': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "envs_all = {}\n",
        "for _,row in df_all.iterrows():\n",
        "    env = row[\"Environment Setting\"].split(\", \")\n",
        "    for e in env:\n",
        "        envs_all[e] = envs_all.get(e, 0) + 1\n",
        "envs_all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2B6WX5B5m77",
        "outputId": "74bcc95c-d06a-4242-f386-39ca9b3c0e2e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BLND': 20, 'PHYS': 21, 'VIRT': 31, 'UNSP': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Participant Structure."
      ],
      "metadata": {
        "id": "wHJ6wyiSsGY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "participants = {}\n",
        "for _,row in df_nlp.iterrows():\n",
        "    part = row[\"Participant Structure\"].split(\", \")\n",
        "    for p in part:\n",
        "        participants[p] = participants.get(p, 0) + 1\n",
        "participants"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEDMVpV1vAbH",
        "outputId": "d4affcf7-0689-4a30-8fc4-d2e0a0ee7263"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'MULTI': 23, 'IND': 14}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "participants_all = {}\n",
        "for _,row in df_all.iterrows():\n",
        "    part = row[\"Participant Structure\"].split(\", \")\n",
        "    for p in part:\n",
        "        participants_all[p] = participants_all.get(p, 0) + 1\n",
        "participants_all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvWZlY81wgpz",
        "outputId": "8523eaa4-8f18-48f9-d15a-f2bdd934b0c4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'IND': 45, 'MULTI': 31}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Didactic Nature."
      ],
      "metadata": {
        "id": "5-g6gj902Lfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "did_natures = {}\n",
        "for _,row in df_nlp.iterrows():\n",
        "    did = row[\"Didactic Nature\"].split(\", \")\n",
        "    for d in did:\n",
        "        did_natures[d] = did_natures.get(d, 0) + 1\n",
        "did_natures"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2w5NdJn2RWx",
        "outputId": "74b136f0-a7f0-438f-974d-5eeb74c86ded"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'INSTR': 21, 'TRAIN': 8, 'INF': 6}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "did_natures_all = {}\n",
        "for _,row in df_all.iterrows():\n",
        "    did = row[\"Didactic Nature\"].split(\", \")\n",
        "    for d in did:\n",
        "        did_natures_all[d] = did_natures_all.get(d, 0) + 1\n",
        "did_natures_all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4exrhu1I2o1N",
        "outputId": "163ee4a5-6a75-4e2d-b02d-ad9497a46820"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'INSTR': 45, 'INF': 12, 'UNSP': 1, 'TRAIN': 15}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Level of Instruction or Training."
      ],
      "metadata": {
        "id": "BKckY77443dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "levels = {}\n",
        "for _,row in df_nlp.iterrows():\n",
        "    levs = row[\"Level of Instruction or Training\"].split(\", \")\n",
        "    for l in levs:\n",
        "        levels[l] = levels.get(l, 0) + 1\n",
        "levels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MUYDoBG44E4",
        "outputId": "148689df-b7d1-455d-c068-a539913355f1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'UNI': 14, 'K12': 17, 'PROF': 3, 'UNSP': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "levels_all = {}\n",
        "for _,row in df_all.iterrows():\n",
        "    levs = row[\"Level of Instruction or Training\"].split(\", \")\n",
        "    for l in levs:\n",
        "        levels_all[l] = levels_all.get(l, 0) + 1\n",
        "levels_all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5lfSCMe5En7",
        "outputId": "b9aab5ab-061b-4804-f0fb-4ed676b3a00e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'K12': 30, 'UNI': 36, 'UNSP': 7, 'PROF': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis Approach."
      ],
      "metadata": {
        "id": "Dxc-RmYa8IYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anals_app = {}\n",
        "for _,row in df_nlp.iterrows():\n",
        "    anals = row[\"Analysis Approach\"].split(\", \")\n",
        "    for a in anals:\n",
        "        anals_app[a] = anals_app.get(a, 0) + 1\n",
        "anals_app"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTj_vS6C8UBj",
        "outputId": "5cf4b392-ac5f-4806-c835-9b270f2207d6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'MB': 24, 'MF': 17}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "anals_app_all = {}\n",
        "for _,row in df_all.iterrows():\n",
        "    anals = row[\"Analysis Approach\"].split(\", \")\n",
        "    for a in anals:\n",
        "        anals_app_all[a] = anals_app_all.get(a, 0) + 1\n",
        "anals_app_all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzZ2ljXK8n3C",
        "outputId": "befd20d5-74b4-4440-b006-a4b24c8ceb24"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'MB': 57, 'MF': 27}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Common modalities:\n",
        "\n",
        "df_no_common_mods = df_all[df_all[\"Modalities\"].str.contains(\"PROS\") | \\\n",
        "                      df_all[\"Modalities\"].str.contains(\"LOGS\") | \\\n",
        "                      df_all[\"Modalities\"].str.contains(\"AFFECT\") | \\\n",
        "                      df_all[\"Modalities\"].str.contains(\"GAZE\") | \\\n",
        "                      df_all[\"Modalities\"].str.contains(\"POSE\")]\n",
        "df_no_common_mods.reset_index(drop=True, inplace=True)\n",
        "print(\"Num:\",len(df_no_common_mods))\n",
        "df_no_common_mods"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j8ZSJQY3PqVN",
        "outputId": "49f76ffb-1768-4696-ade5-5b36dc59105c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num: 65\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          UUID  \\\n",
              "0    818492192   \n",
              "1   3408664396   \n",
              "2   1118315889   \n",
              "3   3339002981   \n",
              "4   1609706685   \n",
              "5   3093310941   \n",
              "6   3095923626   \n",
              "7   2456887548   \n",
              "8   1374035721   \n",
              "9     85990093   \n",
              "10   957160695   \n",
              "11  1637690235   \n",
              "12  1886134458   \n",
              "13  2181637610   \n",
              "14   483140962   \n",
              "15  3308658121   \n",
              "16  3135645357   \n",
              "17  2836996318   \n",
              "18  3783339081   \n",
              "19  2345021698   \n",
              "20   804659204   \n",
              "21  2497456347   \n",
              "22  2070224207   \n",
              "23  1847468084   \n",
              "24  1326191931   \n",
              "25  4278392816   \n",
              "26  1576545447   \n",
              "27   853680639   \n",
              "28  3398902089   \n",
              "29  3448122334   \n",
              "30  1296637108   \n",
              "31  1019093033   \n",
              "32  1581261659   \n",
              "33  1598166515   \n",
              "34  3009548670   \n",
              "35  1770989706   \n",
              "36  3051560548   \n",
              "37   147203129   \n",
              "38  2000036002   \n",
              "39  2055153191   \n",
              "40  3796643912   \n",
              "41  2879332689   \n",
              "42  1877483551   \n",
              "43  3637456466   \n",
              "44  2936220551   \n",
              "45  2634033325   \n",
              "46   123412197   \n",
              "47  1763513559   \n",
              "48  4035649049   \n",
              "49  3625722965   \n",
              "50  1426267857   \n",
              "51   666050348   \n",
              "52   518268671   \n",
              "53  3660066725   \n",
              "54  3856280479   \n",
              "55  4277812050   \n",
              "56   566043228   \n",
              "57  1315379489   \n",
              "58   433919853   \n",
              "59  2273914836   \n",
              "60    32184286   \n",
              "61  2609260641   \n",
              "62  1345598079   \n",
              "63  2155422499   \n",
              "64  3754172825   \n",
              "\n",
              "                                                                                                                                                         Title  \\\n",
              "0                          understanding student learning trajectories using multimodal learning analytics within an embodied-interaction learning environment   \n",
              "1                                                                                                 multimodal student engagement recognition in prosocial games   \n",
              "2                                                           using multimodal learning analytics to identify aspects of collaboration in project-based learning   \n",
              "3                                                              estimation of success in collaborative learning based on multimodal learning analytics features   \n",
              "4                                      learning pulse: a machine learning approach for predicting performance in self-regulated learning using multimodal data   \n",
              "5                                      embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders   \n",
              "6                                                                                                                              a multimodal analysis of making   \n",
              "7                                                                       an unobtrusive and multimodal approach for behavioral engagement detection of students   \n",
              "8                                                                       attentivelearner2: a multimodal approach for improving mooc learning on mobile devices   \n",
              "9                                                                                   multimodal markers of persuasive speech : designing a virtual debate coach   \n",
              "10                                                                                 virtual debate coach design: assessing multimodal argumentation performance   \n",
              "11                                               supervised machine learning in multimodal learning analytics for estimating success in project-based learning   \n",
              "12                                                                        personalizing computer science education by leveraging multimodal learning analytics   \n",
              "13                                                        toward using multi-modal learning analytics to support and measure collaboration in co-located dyads   \n",
              "14                                                           investigating multimodal affect sensing in an affective tutoring system using unobtrusive sensors   \n",
              "15                                                                             exploring collaboration using motion sensors and multi-modal learning analytics   \n",
              "16                                                       multimodal teaching analytics: automated extraction of orchestration graphs from wearable sensor data   \n",
              "17                                                                    predicting learners' emotions in mobile mooc learning via a multimodal intelligent tutor   \n",
              "18                                        a novel method for the in-depth multimodal analysis of student learning trajectories in intelligent tutoring systems   \n",
              "19                           exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course   \n",
              "20                                                                          towards smart educational recommendations with reinforcement learning in classroom   \n",
              "21                                               the rap system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors   \n",
              "22                                                                               detecting medical simulation errors with machine learning and multimodal data   \n",
              "23                                                                        computationally augmented ethnography: emotion tracking and learning in museum games   \n",
              "24                                                                                                     multimodal learning analytics in a laboratory classroom   \n",
              "25                                                                                            multimodal data as a means to understand the learning experience   \n",
              "26                                        artificial intelligence and multimodal data in the service of human decision-making: a case study in debate tutoring   \n",
              "27                                                                sensor-based data fusion for multimodal affect detection in game-based learning environments   \n",
              "28                                                                  what multimodal data can tell us about the students’ regulation of their learning process?   \n",
              "29                                         investigating the impact of a real-time, multimodal student engagement analytics technology in authentic classrooms   \n",
              "30                                                                                 towards collaboration translucence: giving meaning to multimodal group data   \n",
              "31                                                                 prime: block-wise missingness handling for multi-modalities in intelligent tutoring systems   \n",
              "32                                                                early prediction of visitor engagement in science museums with multimodal learning analytics   \n",
              "33                                                                                                       multimodal learning analytics for game-based learning   \n",
              "34                                                                                                            real-time multimodal feedback with the cpr tutor   \n",
              "35                                                  focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving   \n",
              "36                                                                             temporal analysis of multimodal data to predict collaborative learning outcomes   \n",
              "37                                                           multimodal learning analytics to inform learning design: lessons learned from computing education   \n",
              "38                                                                       predicting learners’ effortful behaviour in adaptive assessment using multimodal data   \n",
              "39                                                        round or rectangular tables for collaborative problem solving? a multimodal learning analytics study   \n",
              "40                             an evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities   \n",
              "41                                                                    from data to insights: a layered storytelling approach for multimodal learning analytics   \n",
              "42                                                                      motion-based educational games: using multi-modal data to predict player’s performance   \n",
              "43                                    impact of inquiry interventions on students in e-learning and classroom environments using affective computing framework   \n",
              "44                                          multi-source and multimodal data fusion for predicting academic performance in blended learning university courses   \n",
              "45                                                 controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting   \n",
              "46                                                                          utilizing multimodal data through fsqca to explain engagement in adaptive learning   \n",
              "47                                                                                                keep me in the loop: real-time feedback with multimodal data   \n",
              "48                                                                          storytelling with learner data: guiding student reflection on multimodal team data   \n",
              "49                                                            table tennis tutor: forehand strokes classification based on multimodal data and neural networks   \n",
              "50                                                                      affect, support, and personal factors: multimodal causal models of one-on-one coaching   \n",
              "51                                                                        multicraft: a multimodal interface for supporting and studying learning in minecraft   \n",
              "52                                                   using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game   \n",
              "53                             children's play and problem solving in motion-based educational games: synergies between human annotations and multi-modal data   \n",
              "54                                        children's play and problem-solving in motion-based learning technologies using a multi-modal mixed methods approach   \n",
              "55  improving prediction of students' performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources   \n",
              "56                                                                  automatic student engagement in online learning environment based on neural turing machine   \n",
              "57                                                                                          multimodal engagement analysis from facial videos in the classroom   \n",
              "58                                                                                          understanding fun in learning to code: a multi-modal data approach   \n",
              "59                               many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities   \n",
              "60                                                                                           once more with feeling: emotions in multimodal learning analytics   \n",
              "61                                                visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication   \n",
              "62                       intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning   \n",
              "63                                                              a multimodal analysis of pair work engagement episodes: implications for emi lecturer training   \n",
              "64                                                                   detecting impasse during collaborative problem solving with multimodal learning analytics   \n",
              "\n",
              "                  First Author  Year Environment Type  \\\n",
              "0            Alejandro Andrade  2017         Learning   \n",
              "1           Athanasios Psaltis  2017         Learning   \n",
              "2                Daniel Spikol  2017         Learning   \n",
              "3                Daniel Spikol  2017         Learning   \n",
              "4             Daniele Di Mitri  2017         Training   \n",
              "5                Hiroki Tanaka  2017         Training   \n",
              "6              Marcelo Worsley  2017         Learning   \n",
              "7                   Nese Alyuz  2017         Learning   \n",
              "8                  Phuong Pham  2017         Learning   \n",
              "9              Volha Petukhova  2017         Training   \n",
              "10             Volha Petukhova  2017         Training   \n",
              "11               Daniel Spikol  2018         Learning   \n",
              "12                David Azcona  2018         Learning   \n",
              "13               Emma L. Starr  2018         Learning   \n",
              "14               Hua Leong Fwa  2018         Learning   \n",
              "15            Joseph M. Reilly  2018         Learning   \n",
              "16              Luis P. Prieto  2018         Learning   \n",
              "17                 Phuong Pham  2018         Learning   \n",
              "18                     Ran Liu  2018         Learning   \n",
              "19                   René Noël  2018         Learning   \n",
              "20                      Su Liu  2018         Learning   \n",
              "21                Xavier Ochoa  2018         Training   \n",
              "22            Daniele Di Mitri  2019         Training   \n",
              "23                  Kit Martin  2019         Learning   \n",
              "24       Man Ching Esther Chan  2019         Learning   \n",
              "25           Michail Giannakos  2019         Training   \n",
              "26              Mutlu Cukurova  2019         Learning   \n",
              "27            Nathan Henderson  2019         Training   \n",
              "28               Sanna Järvelä  2019         Learning   \n",
              "29                 Sinem Aslan  2019         Learning   \n",
              "30          Vanessa Echeverria  2019         Training   \n",
              "31                     Xi Yang  2019         Learning   \n",
              "32              Andrew Emerson  2020         Learning   \n",
              "33              Andrew Emerson  2020         Learning   \n",
              "34            Daniele Di Mitri  2020         Training   \n",
              "35               Hana Vrzakova  2020         Learning   \n",
              "36           Jennifer K. Olsen  2020         Learning   \n",
              "37         Katerina Mangaroska  2020         Learning   \n",
              "38              Kshitij Sharma  2020         Learning   \n",
              "39              Milica Vujovic  2020         Learning   \n",
              "40         Penelope J. Standen  2020         Learning   \n",
              "41  Roberto Martinez-Maldonado  2020         Training   \n",
              "42          Serena Lee-Cultura  2020         Learning   \n",
              "43                T. S. Ashwin  2020         Learning   \n",
              "44               Wilson Chango  2020         Learning   \n",
              "45                Xavier Ochoa  2020         Training   \n",
              "46      Zacharoula Papamitsiou  2020         Learning   \n",
              "47            Daniele Di Mitri  2021         Training   \n",
              "48      Gloria Fernández-Nieto  2021         Training   \n",
              "49  Khaleel Asyraaf Mat Sanusi  2021         Training   \n",
              "50            Lujie Karen Chen  2021         Learning   \n",
              "51             Marcelo Worsley  2021         Learning   \n",
              "52          María Ximena López  2021         Learning   \n",
              "53          Serena Lee-Cultura  2021         Learning   \n",
              "54          Serena Lee-Cultura  2021         Learning   \n",
              "55               Wilson Chango  2021         Learning   \n",
              "56                 Xiaoyang Ma  2021         Learning   \n",
              "57                  Ömer Sümer  2021         Learning   \n",
              "58             Gabriella Tisza  2022         Learning   \n",
              "59             Jauwairia Nasir  2022         Learning   \n",
              "60               Marcus Kubsch  2022         Learning   \n",
              "61                   René Noël  2022         Learning   \n",
              "62              Sofia Tancredi  2022         Learning   \n",
              "63               Teresa Morell  2022         Training   \n",
              "64                   Yingbo Ma  2022         Learning   \n",
              "\n",
              "                          Data Collection Mediums  \\\n",
              "0                            VIDEO,LOGS,INTER,PPA   \n",
              "1                                      VIDEO,LOGS   \n",
              "2                                VIDEO,AUDIO,LOGS   \n",
              "3                            EYE,LOGS,VIDEO,AUDIO   \n",
              "4                          SENSOR,LOGS,MOTION,PPA   \n",
              "5                                 AUDIO,VIDEO,PPA   \n",
              "6                    VIDEO,AUDIO,SENSOR,PPA,INTER   \n",
              "7                           LOGS,VIDEO,SCREEN,PPA   \n",
              "8                                    VIDEO,SURVEY   \n",
              "9                                     VIDEO,AUDIO   \n",
              "10                                    VIDEO,AUDIO   \n",
              "11                       VIDEO,AUDIO,LOGS,PPA,RPA   \n",
              "12                                       LOGS,PPA   \n",
              "13                       VIDEO,AUDIO,PPA,RPA,LOGS   \n",
              "14                                     VIDEO,LOGS   \n",
              "15                                VIDEO,AUDIO,PPA   \n",
              "16                         EYE,VIDEO,AUDIO,MOTION   \n",
              "17                                          VIDEO   \n",
              "18                          LOGS,AUDIO,SCREEN,PPA   \n",
              "19                                  AUDIO,PPA,RPA   \n",
              "20                                   VIDEO,SENSOR   \n",
              "21                         AUDIO,VIDEO,PPA,SURVEY   \n",
              "22                              VIDEO,MOTION,LOGS   \n",
              "23                            VIDEO,AUDIO,PPA,RPA   \n",
              "24                                    VIDEO,AUDIO   \n",
              "25                          LOGS,EYE,SENSOR,VIDEO   \n",
              "26                                   AUDIO,SURVEY   \n",
              "27                               VIDEO,SENSOR,RPA   \n",
              "28                             SENSOR,VIDEO,AUDIO   \n",
              "29   VIDEO,LOGS,SCREEN,AUDIO,SURVEY,RPA,PPA,INTER   \n",
              "30           VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER   \n",
              "31                                  PPA,VIDEO,EYE   \n",
              "32                                 VIDEO,EYE,LOGS   \n",
              "33                                 VIDEO,LOGS,EYE   \n",
              "34                       LOGS,VIDEO,SENSOR,MOTION   \n",
              "35                  AUDIO,VIDEO,SCREEN,SURVEY,PPA   \n",
              "36                                 LOGS,AUDIO,EYE   \n",
              "37                          VIDEO,EYE,SENSOR,LOGS   \n",
              "38                               VIDEO,EYE,SENSOR   \n",
              "39                                   VIDEO,MOTION   \n",
              "40                           VIDEO,AUDIO,LOGS,RPA   \n",
              "41                   LOGS,MOTION,SENSOR,RPA,VIDEO   \n",
              "42                               VIDEO,EYE,SENSOR   \n",
              "43                                      VIDEO,PPA   \n",
              "44                                 VIDEO,LOGS,PPA   \n",
              "45                                VIDEO,AUDIO,PPA   \n",
              "46                   SURVEY,LOGS,EYE,SENSOR,VIDEO   \n",
              "47                SURVEY,LOGS,VIDEO,SENSOR,MOTION   \n",
              "48                                SENSOR,LOGS,RPA   \n",
              "49                             VIDEO,MOTION,INTER   \n",
              "50                             AUDIO,VIDEO,SURVEY   \n",
              "51  AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS   \n",
              "52                        SURVEY,LOGS,AUDIO,VIDEO   \n",
              "53                          VIDEO,SENSOR,EYE,LOGS   \n",
              "54                          VIDEO,SENSOR,EYE,LOGS   \n",
              "55                             LOGS,VIDEO,EYE,PPA   \n",
              "56                                          VIDEO   \n",
              "57                                          VIDEO   \n",
              "58                               SENSOR,VIDEO,PPA   \n",
              "59                    VIDEO,AUDIO,LOGS,PPA,SURVEY   \n",
              "60                               SURVEY,PPA,AUDIO   \n",
              "61                          AUDIO,VIDEO,RPA,INTER   \n",
              "62                          EYE,VIDEO,AUDIO,INTER   \n",
              "63                                VIDEO,AUDIO,PPA   \n",
              "64                                    VIDEO,AUDIO   \n",
              "\n",
              "                                           Modalities  \\\n",
              "0                            GAZE,LOGS,INTER,PPA,GEST   \n",
              "1                                    POSE,AFFECT,LOGS   \n",
              "2                                           POSE,PROS   \n",
              "3                                 GAZE,LOGS,PROS,POSE   \n",
              "4                                    PULSE,ACT,AFFECT   \n",
              "5                                    POSE,PROS,AFFECT   \n",
              "6                    GEST,PPA,EDA,ACT,PROS,QUAL,INTER   \n",
              "7                                AFFECT,POSE,LOGS,PPA   \n",
              "8                                 PULSE,AFFECT,SURVEY   \n",
              "9                                           PROS,GEST   \n",
              "10                        GEST,TRANS,PROS,SURVEY,GAZE   \n",
              "11                        POSE,GEST,PROS,LOGS,PPA,RPA   \n",
              "12                                           LOGS,PPA   \n",
              "13                             POSE,PROS,PPA,RPA,LOGS   \n",
              "14                                      POSE,ACT,LOGS   \n",
              "15                                  PPA,RPA,POSE,GEST   \n",
              "16                                GAZE,PROS,ACT,PIXEL   \n",
              "17                                       AFFECT,PULSE   \n",
              "18                            LOGS,TRANS,ACT,QUAL,PPA   \n",
              "19                                           RPA,PROS   \n",
              "20                                  PULSE,AFFECT,GAZE   \n",
              "21                                 PPA,GAZE,POSE,PROS   \n",
              "22                                          POSE,LOGS   \n",
              "23                                  TRANS,AFFECT,QUAL   \n",
              "24                                     POSE,GAZE,PROS   \n",
              "25               EEG,GAZE,LOGS,PULSE,EDA,TEMP,BP,POSE   \n",
              "26                                        AFFECT,LOGS   \n",
              "27                                    POSE,EDA,AFFECT   \n",
              "28                                    EDA,AFFECT,QUAL   \n",
              "29             AFFECT,LOGS,POSE,QUAL,INTER,SURVEY,RPA   \n",
              "30                 POSE,LOGS,TRANS,EDA,ACT,PROS,INTER   \n",
              "31                                   LOGS,AFFECT,GAZE   \n",
              "32                         POSE,GEST,AFFECT,GAZE,LOGS   \n",
              "33                               AFFECT,GAZE,LOGS,PPA   \n",
              "34                                      POSE,EMG,GEST   \n",
              "35                                  PROS,ACT,GEST,PPA   \n",
              "36                          GAZE,LOGS,PROS,TRANS,QUAL   \n",
              "37                    LOGS,GAZE,EDA,PULSE,AFFECT,TEMP   \n",
              "38                     EDA,TEMP,PULSE,EEG,GAZE,AFFECT   \n",
              "39                                      POSE,GEST,ACT   \n",
              "40                AFFECT,POSE,LOGS,RPA,GAZE,PROS,GEST   \n",
              "41                                  POSE,EDA,LOGS,RPA   \n",
              "42                           PULSE,TEMP,EDA,GAZE,POSE   \n",
              "43                                   AFFECT,POSE,GEST   \n",
              "44                                  LOGS,POSE,RPA,ACT   \n",
              "45                                      POSE,PROS,PPA   \n",
              "46      PULSE,AFFECT,EEG,GAZE,LOGS,BP,TEMP,EDA,SURVEY   \n",
              "47                               POSE,EMG,GEST,SURVEY   \n",
              "48                                EDA,LOGS,RPA,AFFECT   \n",
              "49                                POSE,GEST,ACT,INTER   \n",
              "50                      PROS,GAZE,TRANS,AFFECT,SURVEY   \n",
              "51             PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS   \n",
              "52                              LOGS,SURVEY,GAZE,PROS   \n",
              "53     ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP   \n",
              "54  ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP,BP   \n",
              "55                               AFFECT,LOGS,GAZE,PPA   \n",
              "56                                          POSE,GAZE   \n",
              "57                                        POSE,AFFECT   \n",
              "58                           TEMP,PULSE,EDA,BP,AFFECT   \n",
              "59                        PROS,AFFECT,GAZE,TRANS,LOGS   \n",
              "60                     INTER,SURVEY,TRANS,PROS,AFFECT   \n",
              "61                           PROS,POSE,RPA,INTER,QUAL   \n",
              "62                         GAZE,GEST,TRANS,POSE,INTER   \n",
              "63                            TRANS,PPA,QUAL,POSE,ACT   \n",
              "64                         TRANS,PROS,SPECT,GAZE,POSE   \n",
              "\n",
              "           Analysis Methods Fusion Types    Publication Environment Setting  \\\n",
              "0                CLUST,QUAL       HYBRID            LAK                BLND   \n",
              "1                       CLS         LATE        T-CIAIG                BLND   \n",
              "2                       REG          MID           CSCL                PHYS   \n",
              "3                       CLS          MID          ICALT                VIRT   \n",
              "4                       REG          MID            LAK                BLND   \n",
              "5                 REG,STATS          MID           PLOS                VIRT   \n",
              "6     STATS,CLUST,QUAL,PATT        EARLY         IJAIED                PHYS   \n",
              "7                       CLS       HYBRID            MIE                VIRT   \n",
              "8                       CLS          MID           AIED                VIRT   \n",
              "9            CLS,QUAL,STATS          MID    INTERSPEECH                PHYS   \n",
              "10           STATS,CLS,QUAL          MID           ICMI                PHYS   \n",
              "11                  REG,CLS          MID           JCAL                BLND   \n",
              "12                CLS,STATS          MID            FIE                VIRT   \n",
              "13               STATS,QUAL          OTH           ICLS                BLND   \n",
              "14                      CLS          MID           PPIG                VIRT   \n",
              "15         CLUST,PATT,STATS          OTH            EDM                BLND   \n",
              "16  NET,CLS,STATS,PATT,QUAL       HYBRID           JCAL                PHYS   \n",
              "17                      REG         LATE            ITS                VIRT   \n",
              "18           STATS,REG,QUAL          MID            JLA                VIRT   \n",
              "19           QUAL,NET,STATS          OTH         Access                PHYS   \n",
              "20                      CLS          MID           TALE                UNSP   \n",
              "21           CLS,STATS,QUAL         LATE            LAK                BLND   \n",
              "22                      CLS          MID           CAIM                BLND   \n",
              "23                     QUAL          OTH           ICQE                VIRT   \n",
              "24                CLS,CLUST         LATE         MLPALA                PHYS   \n",
              "25                REG,STATS       HYBRID           IJIM                VIRT   \n",
              "26                      CLS          MID           BJET                UNSP   \n",
              "27                      CLS       HYBRID            EDM                VIRT   \n",
              "28                     QUAL          OTH            LAI                BLND   \n",
              "29           QUAL,STATS,CLS         LATE            CHI                VIRT   \n",
              "30                     QUAL          OTH            CHI                PHYS   \n",
              "31                CLS,STATS       HYBRID            MMM                VIRT   \n",
              "32                      REG          MID           ICMI                VIRT   \n",
              "33                CLS,STATS          MID           BJET                VIRT   \n",
              "34                      CLS       HYBRID           AIED                PHYS   \n",
              "35               STATS,PATT          MID            LAK                VIRT   \n",
              "36                      REG          MID           BJET                VIRT   \n",
              "37                      CLS       HYBRID            JLA                VIRT   \n",
              "38           CLUST,CLS,PATT          MID            LAK                VIRT   \n",
              "39               STATS,QUAL          OTH           BJET                BLND   \n",
              "40                CLS,STATS       HYBRID           BJET                VIRT   \n",
              "41                     QUAL          OTH            CHI                BLND   \n",
              "42                      CLS          MID            COG                BLND   \n",
              "43           CLS,STATS,PATT          MID          UMUAI          PHYS, VIRT   \n",
              "44           CLS,QUAL,STATS     MID,LATE            CEE                BLND   \n",
              "45                    STATS          OTH           BJET                BLND   \n",
              "46                     PATT       HYBRID            TLT                VIRT   \n",
              "47           CLS,QUAL,STATS       HYBRID         IJAIED                PHYS   \n",
              "48                     QUAL          OTH            TLT                BLND   \n",
              "49                 CLS,QUAL       HYBRID        Sensors                PHYS   \n",
              "50                STATS,NET       HYBRID           JEDM                PHYS   \n",
              "51                     QUAL          OTH           HCII                VIRT   \n",
              "52                    STATS          OTH          ECGBL                BLND   \n",
              "53               STATS,QUAL          OTH            IDC                BLND   \n",
              "54           STATS,QUAL,CLS       HYBRID          IJCCI                BLND   \n",
              "55                      CLS  HYBRID,LATE           JCHE                VIRT   \n",
              "56                      CLS          MID          IJIET                VIRT   \n",
              "57                      CLS   EARLY,LATE            TAC                PHYS   \n",
              "58                REG,STATS          MID            IDC                VIRT   \n",
              "59     STATS,QUAL,CLUST,CLS       HYBRID         IJCSCL                BLND   \n",
              "60            CLS,REG,STATS          OTH  MMLA Handbook                PHYS   \n",
              "61                     QUAL          OTH          DAMLE                PHYS   \n",
              "62          PATT,QUAL,STATS          OTH  MMLA Handbook                VIRT   \n",
              "63                     QUAL          OTH           JEAP                PHYS   \n",
              "64                      CLS       HYBRID            LAK                VIRT   \n",
              "\n",
              "   Environment Subject Participant Structure Didactic Nature  \\\n",
              "0                 STEM                   IND           INSTR   \n",
              "1                  HUM                   IND             INF   \n",
              "2                 STEM                 MULTI           INSTR   \n",
              "3                 STEM                 MULTI           INSTR   \n",
              "4                 UNSP                   IND            UNSP   \n",
              "5                  HUM                   IND           TRAIN   \n",
              "6                 STEM                 MULTI             INF   \n",
              "7                 STEM                   IND           INSTR   \n",
              "8                 STEM                   IND           INSTR   \n",
              "9                  HUM                 MULTI           TRAIN   \n",
              "10                 HUM                 MULTI           TRAIN   \n",
              "11                STEM                 MULTI           INSTR   \n",
              "12                STEM                   IND           INSTR   \n",
              "13                STEM                 MULTI             INF   \n",
              "14                STEM                   IND           INSTR   \n",
              "15                STEM                 MULTI           INSTR   \n",
              "16                STEM                 MULTI           INSTR   \n",
              "17                STEM                   IND           INSTR   \n",
              "18                STEM                   IND           INSTR   \n",
              "19                STEM                 MULTI             INF   \n",
              "20                UNSP                   IND           INSTR   \n",
              "21                 HUM                   IND           TRAIN   \n",
              "22                 PSY                   IND           TRAIN   \n",
              "23                STEM                 MULTI             INF   \n",
              "24                STEM            IND, MULTI           INSTR   \n",
              "25                 PSY                   IND             INF   \n",
              "26                 HUM                   IND           TRAIN   \n",
              "27                STEM                   IND           TRAIN   \n",
              "28                STEM                 MULTI           INSTR   \n",
              "29                STEM                   IND           INSTR   \n",
              "30                STEM                 MULTI           TRAIN   \n",
              "31                STEM                   IND           INSTR   \n",
              "32                STEM                   IND             INF   \n",
              "33                STEM                   IND             INF   \n",
              "34                 PSY                   IND           TRAIN   \n",
              "35                STEM                 MULTI           INSTR   \n",
              "36                STEM                 MULTI           INSTR   \n",
              "37                STEM                   IND           INSTR   \n",
              "38                STEM                   IND           INSTR   \n",
              "39           HUM, STEM                 MULTI           INSTR   \n",
              "40      HUM, OTH, STEM                   IND           INSTR   \n",
              "41                STEM                 MULTI           TRAIN   \n",
              "42                STEM                   IND           INSTR   \n",
              "43                UNSP            IND, MULTI           INSTR   \n",
              "44                STEM                   IND           INSTR   \n",
              "45                 HUM                   IND           TRAIN   \n",
              "46                STEM                   IND             INF   \n",
              "47                 PSY                   IND           TRAIN   \n",
              "48                STEM                 MULTI           TRAIN   \n",
              "49                 PSY                   IND           TRAIN   \n",
              "50                STEM                   IND           INSTR   \n",
              "51                STEM                   IND             INF   \n",
              "52                STEM                 MULTI           INSTR   \n",
              "53                STEM                   IND           INSTR   \n",
              "54                STEM                   IND           INSTR   \n",
              "55                STEM                   IND           INSTR   \n",
              "56                UNSP                   IND           INSTR   \n",
              "57           HUM, STEM                 MULTI           INSTR   \n",
              "58                STEM                   IND           INSTR   \n",
              "59                STEM                 MULTI           INSTR   \n",
              "60                STEM                   IND           INSTR   \n",
              "61                 HUM                 MULTI             INF   \n",
              "62                STEM                   IND           INSTR   \n",
              "63                 OTH                 MULTI           TRAIN   \n",
              "64                STEM                 MULTI           INSTR   \n",
              "\n",
              "   Level of Instruction or Training Analysis Approach  \n",
              "0                               K12                MB  \n",
              "1                               K12                MB  \n",
              "2                               UNI                MB  \n",
              "3                               UNI                MB  \n",
              "4                               UNI                MB  \n",
              "5                          K12, UNI                MF  \n",
              "6                          K12, UNI                MB  \n",
              "7                               K12                MB  \n",
              "8                              UNSP                MB  \n",
              "9                               K12                MB  \n",
              "10                              K12                MB  \n",
              "11                              UNI                MB  \n",
              "12                              UNI                MB  \n",
              "13                              UNI                MF  \n",
              "14                              UNI                MB  \n",
              "15                              UNI            MB, MF  \n",
              "16                             PROF                MB  \n",
              "17                              UNI                MB  \n",
              "18                              K12                MB  \n",
              "19                              UNI            MB, MF  \n",
              "20                             UNSP                MB  \n",
              "21                              UNI                MB  \n",
              "22                              UNI                MB  \n",
              "23                             UNSP            MB, MF  \n",
              "24                             UNSP                MB  \n",
              "25                              UNI                MB  \n",
              "26                             UNSP            MB, MF  \n",
              "27                              UNI                MB  \n",
              "28                              K12                MF  \n",
              "29                              K12                MB  \n",
              "30                              UNI                MB  \n",
              "31                              UNI                MB  \n",
              "32                              K12                MB  \n",
              "33                              UNI                MB  \n",
              "34                             PROF                MB  \n",
              "35                              UNI                MF  \n",
              "36                              K12                MB  \n",
              "37                              UNI                MB  \n",
              "38                              UNI                MB  \n",
              "39                              K12                MF  \n",
              "40                              K12                MB  \n",
              "41                              UNI                MF  \n",
              "42                              K12                MB  \n",
              "43                              UNI                MB  \n",
              "44                              UNI                MB  \n",
              "45                             UNSP                MF  \n",
              "46                              UNI                MF  \n",
              "47                             PROF                MB  \n",
              "48                              UNI                MF  \n",
              "49                              UNI                MB  \n",
              "50                              K12                MB  \n",
              "51                              K12                MF  \n",
              "52                              UNI                MF  \n",
              "53                              K12                MF  \n",
              "54                              K12            MB, MF  \n",
              "55                              UNI                MB  \n",
              "56                             UNSP                MB  \n",
              "57                              K12                MB  \n",
              "58                              K12            MB, MF  \n",
              "59                              K12                MB  \n",
              "60                              K12            MB, MF  \n",
              "61                        PROF, UNI                MF  \n",
              "62                              K12                MF  \n",
              "63                             PROF                MF  \n",
              "64                              K12                MB  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4ba9b298-2bc1-45c0-b02b-86593af0f9b7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UUID</th>\n",
              "      <th>Title</th>\n",
              "      <th>First Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Environment Type</th>\n",
              "      <th>Data Collection Mediums</th>\n",
              "      <th>Modalities</th>\n",
              "      <th>Analysis Methods</th>\n",
              "      <th>Fusion Types</th>\n",
              "      <th>Publication</th>\n",
              "      <th>Environment Setting</th>\n",
              "      <th>Environment Subject</th>\n",
              "      <th>Participant Structure</th>\n",
              "      <th>Didactic Nature</th>\n",
              "      <th>Level of Instruction or Training</th>\n",
              "      <th>Analysis Approach</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>818492192</td>\n",
              "      <td>understanding student learning trajectories using multimodal learning analytics within an embodied-interaction learning environment</td>\n",
              "      <td>Alejandro Andrade</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,INTER,PPA</td>\n",
              "      <td>GAZE,LOGS,INTER,PPA,GEST</td>\n",
              "      <td>CLUST,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3408664396</td>\n",
              "      <td>multimodal student engagement recognition in prosocial games</td>\n",
              "      <td>Athanasios Psaltis</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS</td>\n",
              "      <td>POSE,AFFECT,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>LATE</td>\n",
              "      <td>T-CIAIG</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1118315889</td>\n",
              "      <td>using multimodal learning analytics to identify aspects of collaboration in project-based learning</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS</td>\n",
              "      <td>POSE,PROS</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>CSCL</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3339002981</td>\n",
              "      <td>estimation of success in collaborative learning based on multimodal learning analytics features</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,LOGS,VIDEO,AUDIO</td>\n",
              "      <td>GAZE,LOGS,PROS,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICALT</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1609706685</td>\n",
              "      <td>learning pulse: a machine learning approach for predicting performance in self-regulated learning using multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>SENSOR,LOGS,MOTION,PPA</td>\n",
              "      <td>PULSE,ACT,AFFECT</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>BLND</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3093310941</td>\n",
              "      <td>embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders</td>\n",
              "      <td>Hiroki Tanaka</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>AUDIO,VIDEO,PPA</td>\n",
              "      <td>POSE,PROS,AFFECT</td>\n",
              "      <td>REG,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>PLOS</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3095923626</td>\n",
              "      <td>a multimodal analysis of making</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,SENSOR,PPA,INTER</td>\n",
              "      <td>GEST,PPA,EDA,ACT,PROS,QUAL,INTER</td>\n",
              "      <td>STATS,CLUST,QUAL,PATT</td>\n",
              "      <td>EARLY</td>\n",
              "      <td>IJAIED</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12, UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2456887548</td>\n",
              "      <td>an unobtrusive and multimodal approach for behavioral engagement detection of students</td>\n",
              "      <td>Nese Alyuz</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,VIDEO,SCREEN,PPA</td>\n",
              "      <td>AFFECT,POSE,LOGS,PPA</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>MIE</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1374035721</td>\n",
              "      <td>attentivelearner2: a multimodal approach for improving mooc learning on mobile devices</td>\n",
              "      <td>Phuong Pham</td>\n",
              "      <td>2017</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SURVEY</td>\n",
              "      <td>PULSE,AFFECT,SURVEY</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>AIED</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>85990093</td>\n",
              "      <td>multimodal markers of persuasive speech : designing a virtual debate coach</td>\n",
              "      <td>Volha Petukhova</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>PROS,GEST</td>\n",
              "      <td>CLS,QUAL,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>INTERSPEECH</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>957160695</td>\n",
              "      <td>virtual debate coach design: assessing multimodal argumentation performance</td>\n",
              "      <td>Volha Petukhova</td>\n",
              "      <td>2017</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>GEST,TRANS,PROS,SURVEY,GAZE</td>\n",
              "      <td>STATS,CLS,QUAL</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICMI</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1637690235</td>\n",
              "      <td>supervised machine learning in multimodal learning analytics for estimating success in project-based learning</td>\n",
              "      <td>Daniel Spikol</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,PPA,RPA</td>\n",
              "      <td>POSE,GEST,PROS,LOGS,PPA,RPA</td>\n",
              "      <td>REG,CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1886134458</td>\n",
              "      <td>personalizing computer science education by leveraging multimodal learning analytics</td>\n",
              "      <td>David Azcona</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,PPA</td>\n",
              "      <td>LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>FIE</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2181637610</td>\n",
              "      <td>toward using multi-modal learning analytics to support and measure collaboration in co-located dyads</td>\n",
              "      <td>Emma L. Starr</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA,RPA,LOGS</td>\n",
              "      <td>POSE,PROS,PPA,RPA,LOGS</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ICLS</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>483140962</td>\n",
              "      <td>investigating multimodal affect sensing in an affective tutoring system using unobtrusive sensors</td>\n",
              "      <td>Hua Leong Fwa</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS</td>\n",
              "      <td>POSE,ACT,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>PPIG</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3308658121</td>\n",
              "      <td>exploring collaboration using motion sensors and multi-modal learning analytics</td>\n",
              "      <td>Joseph M. Reilly</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>PPA,RPA,POSE,GEST</td>\n",
              "      <td>CLUST,PATT,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>EDM</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3135645357</td>\n",
              "      <td>multimodal teaching analytics: automated extraction of orchestration graphs from wearable sensor data</td>\n",
              "      <td>Luis P. Prieto</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,VIDEO,AUDIO,MOTION</td>\n",
              "      <td>GAZE,PROS,ACT,PIXEL</td>\n",
              "      <td>NET,CLS,STATS,PATT,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JCAL</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2836996318</td>\n",
              "      <td>predicting learners' emotions in mobile mooc learning via a multimodal intelligent tutor</td>\n",
              "      <td>Phuong Pham</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO</td>\n",
              "      <td>AFFECT,PULSE</td>\n",
              "      <td>REG</td>\n",
              "      <td>LATE</td>\n",
              "      <td>ITS</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3783339081</td>\n",
              "      <td>a novel method for the in-depth multimodal analysis of student learning trajectories in intelligent tutoring systems</td>\n",
              "      <td>Ran Liu</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,SCREEN,PPA</td>\n",
              "      <td>LOGS,TRANS,ACT,QUAL,PPA</td>\n",
              "      <td>STATS,REG,QUAL</td>\n",
              "      <td>MID</td>\n",
              "      <td>JLA</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2345021698</td>\n",
              "      <td>exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course</td>\n",
              "      <td>René Noël</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,PPA,RPA</td>\n",
              "      <td>RPA,PROS</td>\n",
              "      <td>QUAL,NET,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>Access</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>804659204</td>\n",
              "      <td>towards smart educational recommendations with reinforcement learning in classroom</td>\n",
              "      <td>Su Liu</td>\n",
              "      <td>2018</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SENSOR</td>\n",
              "      <td>PULSE,AFFECT,GAZE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>TALE</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2497456347</td>\n",
              "      <td>the rap system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2018</td>\n",
              "      <td>Training</td>\n",
              "      <td>AUDIO,VIDEO,PPA,SURVEY</td>\n",
              "      <td>PPA,GAZE,POSE,PROS</td>\n",
              "      <td>CLS,STATS,QUAL</td>\n",
              "      <td>LATE</td>\n",
              "      <td>LAK</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2070224207</td>\n",
              "      <td>detecting medical simulation errors with machine learning and multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,LOGS</td>\n",
              "      <td>POSE,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>CAIM</td>\n",
              "      <td>BLND</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1847468084</td>\n",
              "      <td>computationally augmented ethnography: emotion tracking and learning in museum games</td>\n",
              "      <td>Kit Martin</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,PPA,RPA</td>\n",
              "      <td>TRANS,AFFECT,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ICQE</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1326191931</td>\n",
              "      <td>multimodal learning analytics in a laboratory classroom</td>\n",
              "      <td>Man Ching Esther Chan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>POSE,GAZE,PROS</td>\n",
              "      <td>CLS,CLUST</td>\n",
              "      <td>LATE</td>\n",
              "      <td>MLPALA</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>4278392816</td>\n",
              "      <td>multimodal data as a means to understand the learning experience</td>\n",
              "      <td>Michail Giannakos</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>LOGS,EYE,SENSOR,VIDEO</td>\n",
              "      <td>EEG,GAZE,LOGS,PULSE,EDA,TEMP,BP,POSE</td>\n",
              "      <td>REG,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJIM</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1576545447</td>\n",
              "      <td>artificial intelligence and multimodal data in the service of human decision-making: a case study in debate tutoring</td>\n",
              "      <td>Mutlu Cukurova</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,SURVEY</td>\n",
              "      <td>AFFECT,LOGS</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>853680639</td>\n",
              "      <td>sensor-based data fusion for multimodal affect detection in game-based learning environments</td>\n",
              "      <td>Nathan Henderson</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,SENSOR,RPA</td>\n",
              "      <td>POSE,EDA,AFFECT</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>EDM</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>3398902089</td>\n",
              "      <td>what multimodal data can tell us about the students’ regulation of their learning process?</td>\n",
              "      <td>Sanna Järvelä</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SENSOR,VIDEO,AUDIO</td>\n",
              "      <td>EDA,AFFECT,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>LAI</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>3448122334</td>\n",
              "      <td>investigating the impact of a real-time, multimodal student engagement analytics technology in authentic classrooms</td>\n",
              "      <td>Sinem Aslan</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,SCREEN,AUDIO,SURVEY,RPA,PPA,INTER</td>\n",
              "      <td>AFFECT,LOGS,POSE,QUAL,INTER,SURVEY,RPA</td>\n",
              "      <td>QUAL,STATS,CLS</td>\n",
              "      <td>LATE</td>\n",
              "      <td>CHI</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1296637108</td>\n",
              "      <td>towards collaboration translucence: giving meaning to multimodal group data</td>\n",
              "      <td>Vanessa Echeverria</td>\n",
              "      <td>2019</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER</td>\n",
              "      <td>POSE,LOGS,TRANS,EDA,ACT,PROS,INTER</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>CHI</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1019093033</td>\n",
              "      <td>prime: block-wise missingness handling for multi-modalities in intelligent tutoring systems</td>\n",
              "      <td>Xi Yang</td>\n",
              "      <td>2019</td>\n",
              "      <td>Learning</td>\n",
              "      <td>PPA,VIDEO,EYE</td>\n",
              "      <td>LOGS,AFFECT,GAZE</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>MMM</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1581261659</td>\n",
              "      <td>early prediction of visitor engagement in science museums with multimodal learning analytics</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,LOGS</td>\n",
              "      <td>POSE,GEST,AFFECT,GAZE,LOGS</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>ICMI</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1598166515</td>\n",
              "      <td>multimodal learning analytics for game-based learning</td>\n",
              "      <td>Andrew Emerson</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,EYE</td>\n",
              "      <td>AFFECT,GAZE,LOGS,PPA</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>3009548670</td>\n",
              "      <td>real-time multimodal feedback with the cpr tutor</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>LOGS,VIDEO,SENSOR,MOTION</td>\n",
              "      <td>POSE,EMG,GEST</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>AIED</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1770989706</td>\n",
              "      <td>focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving</td>\n",
              "      <td>Hana Vrzakova</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,SCREEN,SURVEY,PPA</td>\n",
              "      <td>PROS,ACT,GEST,PPA</td>\n",
              "      <td>STATS,PATT</td>\n",
              "      <td>MID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>3051560548</td>\n",
              "      <td>temporal analysis of multimodal data to predict collaborative learning outcomes</td>\n",
              "      <td>Jennifer K. Olsen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,AUDIO,EYE</td>\n",
              "      <td>GAZE,LOGS,PROS,TRANS,QUAL</td>\n",
              "      <td>REG</td>\n",
              "      <td>MID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>147203129</td>\n",
              "      <td>multimodal learning analytics to inform learning design: lessons learned from computing education</td>\n",
              "      <td>Katerina Mangaroska</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR,LOGS</td>\n",
              "      <td>LOGS,GAZE,EDA,PULSE,AFFECT,TEMP</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JLA</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>2000036002</td>\n",
              "      <td>predicting learners’ effortful behaviour in adaptive assessment using multimodal data</td>\n",
              "      <td>Kshitij Sharma</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR</td>\n",
              "      <td>EDA,TEMP,PULSE,EEG,GAZE,AFFECT</td>\n",
              "      <td>CLUST,CLS,PATT</td>\n",
              "      <td>MID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>2055153191</td>\n",
              "      <td>round or rectangular tables for collaborative problem solving? a multimodal learning analytics study</td>\n",
              "      <td>Milica Vujovic</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,MOTION</td>\n",
              "      <td>POSE,GEST,ACT</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM, STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>3796643912</td>\n",
              "      <td>an evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities</td>\n",
              "      <td>Penelope J. Standen</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,RPA</td>\n",
              "      <td>AFFECT,POSE,LOGS,RPA,GAZE,PROS,GEST</td>\n",
              "      <td>CLS,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>BJET</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>HUM, OTH, STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>2879332689</td>\n",
              "      <td>from data to insights: a layered storytelling approach for multimodal learning analytics</td>\n",
              "      <td>Roberto Martinez-Maldonado</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>LOGS,MOTION,SENSOR,RPA,VIDEO</td>\n",
              "      <td>POSE,EDA,LOGS,RPA</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>CHI</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>1877483551</td>\n",
              "      <td>motion-based educational games: using multi-modal data to predict player’s performance</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,EYE,SENSOR</td>\n",
              "      <td>PULSE,TEMP,EDA,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>COG</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>3637456466</td>\n",
              "      <td>impact of inquiry interventions on students in e-learning and classroom environments using affective computing framework</td>\n",
              "      <td>T. S. Ashwin</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,PPA</td>\n",
              "      <td>AFFECT,POSE,GEST</td>\n",
              "      <td>CLS,STATS,PATT</td>\n",
              "      <td>MID</td>\n",
              "      <td>UMUAI</td>\n",
              "      <td>PHYS, VIRT</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND, MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>2936220551</td>\n",
              "      <td>multi-source and multimodal data fusion for predicting academic performance in blended learning university courses</td>\n",
              "      <td>Wilson Chango</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,LOGS,PPA</td>\n",
              "      <td>LOGS,POSE,RPA,ACT</td>\n",
              "      <td>CLS,QUAL,STATS</td>\n",
              "      <td>MID,LATE</td>\n",
              "      <td>CEE</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>2634033325</td>\n",
              "      <td>controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting</td>\n",
              "      <td>Xavier Ochoa</td>\n",
              "      <td>2020</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>POSE,PROS,PPA</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>BJET</td>\n",
              "      <td>BLND</td>\n",
              "      <td>HUM</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>123412197</td>\n",
              "      <td>utilizing multimodal data through fsqca to explain engagement in adaptive learning</td>\n",
              "      <td>Zacharoula Papamitsiou</td>\n",
              "      <td>2020</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,LOGS,EYE,SENSOR,VIDEO</td>\n",
              "      <td>PULSE,AFFECT,EEG,GAZE,LOGS,BP,TEMP,EDA,SURVEY</td>\n",
              "      <td>PATT</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>TLT</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>1763513559</td>\n",
              "      <td>keep me in the loop: real-time feedback with multimodal data</td>\n",
              "      <td>Daniele Di Mitri</td>\n",
              "      <td>2021</td>\n",
              "      <td>Training</td>\n",
              "      <td>SURVEY,LOGS,VIDEO,SENSOR,MOTION</td>\n",
              "      <td>POSE,EMG,GEST,SURVEY</td>\n",
              "      <td>CLS,QUAL,STATS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJAIED</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>4035649049</td>\n",
              "      <td>storytelling with learner data: guiding student reflection on multimodal team data</td>\n",
              "      <td>Gloria Fernández-Nieto</td>\n",
              "      <td>2021</td>\n",
              "      <td>Training</td>\n",
              "      <td>SENSOR,LOGS,RPA</td>\n",
              "      <td>EDA,LOGS,RPA,AFFECT</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>TLT</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>3625722965</td>\n",
              "      <td>table tennis tutor: forehand strokes classification based on multimodal data and neural networks</td>\n",
              "      <td>Khaleel Asyraaf Mat Sanusi</td>\n",
              "      <td>2021</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,MOTION,INTER</td>\n",
              "      <td>POSE,GEST,ACT,INTER</td>\n",
              "      <td>CLS,QUAL</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>Sensors</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>PSY</td>\n",
              "      <td>IND</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>1426267857</td>\n",
              "      <td>affect, support, and personal factors: multimodal causal models of one-on-one coaching</td>\n",
              "      <td>Lujie Karen Chen</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,SURVEY</td>\n",
              "      <td>PROS,GAZE,TRANS,AFFECT,SURVEY</td>\n",
              "      <td>STATS,NET</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>JEDM</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>666050348</td>\n",
              "      <td>multicraft: a multimodal interface for supporting and studying learning in minecraft</td>\n",
              "      <td>Marcelo Worsley</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS</td>\n",
              "      <td>PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>HCII</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INF</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>518268671</td>\n",
              "      <td>using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game</td>\n",
              "      <td>María Ximena López</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,LOGS,AUDIO,VIDEO</td>\n",
              "      <td>LOGS,SURVEY,GAZE,PROS</td>\n",
              "      <td>STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>ECGBL</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>3660066725</td>\n",
              "      <td>children's play and problem solving in motion-based educational games: synergies between human annotations and multi-modal data</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SENSOR,EYE,LOGS</td>\n",
              "      <td>ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP</td>\n",
              "      <td>STATS,QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>IDC</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>3856280479</td>\n",
              "      <td>children's play and problem-solving in motion-based learning technologies using a multi-modal mixed methods approach</td>\n",
              "      <td>Serena Lee-Cultura</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,SENSOR,EYE,LOGS</td>\n",
              "      <td>ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP,BP</td>\n",
              "      <td>STATS,QUAL,CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJCCI</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>4277812050</td>\n",
              "      <td>improving prediction of students' performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources</td>\n",
              "      <td>Wilson Chango</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>LOGS,VIDEO,EYE,PPA</td>\n",
              "      <td>AFFECT,LOGS,GAZE,PPA</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID,LATE</td>\n",
              "      <td>JCHE</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNI</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>566043228</td>\n",
              "      <td>automatic student engagement in online learning environment based on neural turing machine</td>\n",
              "      <td>Xiaoyang Ma</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO</td>\n",
              "      <td>POSE,GAZE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>MID</td>\n",
              "      <td>IJIET</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>UNSP</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>1315379489</td>\n",
              "      <td>multimodal engagement analysis from facial videos in the classroom</td>\n",
              "      <td>Ömer Sümer</td>\n",
              "      <td>2021</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO</td>\n",
              "      <td>POSE,AFFECT</td>\n",
              "      <td>CLS</td>\n",
              "      <td>EARLY,LATE</td>\n",
              "      <td>TAC</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM, STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>433919853</td>\n",
              "      <td>understanding fun in learning to code: a multi-modal data approach</td>\n",
              "      <td>Gabriella Tisza</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SENSOR,VIDEO,PPA</td>\n",
              "      <td>TEMP,PULSE,EDA,BP,AFFECT</td>\n",
              "      <td>REG,STATS</td>\n",
              "      <td>MID</td>\n",
              "      <td>IDC</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>2273914836</td>\n",
              "      <td>many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities</td>\n",
              "      <td>Jauwairia Nasir</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO,LOGS,PPA,SURVEY</td>\n",
              "      <td>PROS,AFFECT,GAZE,TRANS,LOGS</td>\n",
              "      <td>STATS,QUAL,CLUST,CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>IJCSCL</td>\n",
              "      <td>BLND</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>32184286</td>\n",
              "      <td>once more with feeling: emotions in multimodal learning analytics</td>\n",
              "      <td>Marcus Kubsch</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>SURVEY,PPA,AUDIO</td>\n",
              "      <td>INTER,SURVEY,TRANS,PROS,AFFECT</td>\n",
              "      <td>CLS,REG,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MMLA Handbook</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB, MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>2609260641</td>\n",
              "      <td>visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication</td>\n",
              "      <td>René Noël</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>AUDIO,VIDEO,RPA,INTER</td>\n",
              "      <td>PROS,POSE,RPA,INTER,QUAL</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>DAMLE</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>HUM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INF</td>\n",
              "      <td>PROF, UNI</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>1345598079</td>\n",
              "      <td>intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning</td>\n",
              "      <td>Sofia Tancredi</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>EYE,VIDEO,AUDIO,INTER</td>\n",
              "      <td>GAZE,GEST,TRANS,POSE,INTER</td>\n",
              "      <td>PATT,QUAL,STATS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MMLA Handbook</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>IND</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>2155422499</td>\n",
              "      <td>a multimodal analysis of pair work engagement episodes: implications for emi lecturer training</td>\n",
              "      <td>Teresa Morell</td>\n",
              "      <td>2022</td>\n",
              "      <td>Training</td>\n",
              "      <td>VIDEO,AUDIO,PPA</td>\n",
              "      <td>TRANS,PPA,QUAL,POSE,ACT</td>\n",
              "      <td>QUAL</td>\n",
              "      <td>OTH</td>\n",
              "      <td>JEAP</td>\n",
              "      <td>PHYS</td>\n",
              "      <td>OTH</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>PROF</td>\n",
              "      <td>MF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>3754172825</td>\n",
              "      <td>detecting impasse during collaborative problem solving with multimodal learning analytics</td>\n",
              "      <td>Yingbo Ma</td>\n",
              "      <td>2022</td>\n",
              "      <td>Learning</td>\n",
              "      <td>VIDEO,AUDIO</td>\n",
              "      <td>TRANS,PROS,SPECT,GAZE,POSE</td>\n",
              "      <td>CLS</td>\n",
              "      <td>HYBRID</td>\n",
              "      <td>LAK</td>\n",
              "      <td>VIRT</td>\n",
              "      <td>STEM</td>\n",
              "      <td>MULTI</td>\n",
              "      <td>INSTR</td>\n",
              "      <td>K12</td>\n",
              "      <td>MB</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ba9b298-2bc1-45c0-b02b-86593af0f9b7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4ba9b298-2bc1-45c0-b02b-86593af0f9b7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4ba9b298-2bc1-45c0-b02b-86593af0f9b7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-894bc3b0-eb56-491a-a5ea-b718d00950c7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-894bc3b0-eb56-491a-a5ea-b718d00950c7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-894bc3b0-eb56-491a-a5ea-b718d00950c7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_16341aa3-4820-4d56-be6d-7d228e068c28\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_no_common_mods')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_16341aa3-4820-4d56-be6d-7d228e068c28 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_no_common_mods');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Environment Subject."
      ],
      "metadata": {
        "id": "KYTlNFLHEPGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subjs = {}\n",
        "for _,row in df_nlp.iterrows():\n",
        "    subj = row[\"Environment Subject\"].split(\", \")\n",
        "    for s in subj:\n",
        "        subjs[s] = subjs.get(s, 0) + 1\n",
        "subjs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "982e462f-507a-4cf2-da91-7e340f1916d5",
        "id": "43Q42EbJ9K2x"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'STEM': 27, 'HUM': 8, 'OTH': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subjs_all = {}\n",
        "for _,row in df_all.iterrows():\n",
        "    subj = row[\"Environment Subject\"].split(\", \")\n",
        "    for s in subj:\n",
        "        subjs_all[s] = subjs_all.get(s, 0) + 1\n",
        "subjs_all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3c1f585-be2b-42ea-baf9-46f4c7568644",
        "id": "ykxdX6_T9K2x"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'STEM': 55, 'HUM': 11, 'UNSP': 4, 'PSY': 5, 'OTH': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}