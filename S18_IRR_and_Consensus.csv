UUID,Title,Mapped First Author,Year,Environment Type (learning or training),Mapped Data Collection Mediums,Mapped Modalities,Mapped Analysis Methods,Mapped Fusion Types,Mapped Publication Acronym,Mapped Full Publication,Sort Number,Environment Setting,Environment Subject,Participant Structure,Didactic Nature,Level of Instruction or Training,Analysis Approach,Analysis Results (w/ multimodal advantages),Full-Read 3 by Researcher,Reviewer,Reviewer Notes
1326191931,multimodal learning analytics in a laboratory classroom,Man Ching Esther Chan,2019,Learning,"VIDEO,AUDIO","POSE,GAZE,PROS","CLS,CLUST",LATE,MLPALA,Machine Learning Paradigms: Advances in Learning Analytics,3,PHYS,STEM,"IND, MULTI",INSTR,UNSP,MB,The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings,Joyce,1,
1326191931,multimodal learning analytics in a laboratory classroom,Man Ching Esther Chan,2019,Learning,"VIDEO,AUDIO","POSE,GAZE,PROS","CLS,CLUST",LATE,MLPALA,Machine Learning Paradigms: Advances in Learning Analytics,3,PHYS,STEM,"IND, MULTI",INSTR,UNSP,MB,"Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures.",Eduardo,2,
1326191931,multimodal learning analytics in a laboratory classroom,Man Ching Esther Chan,2019,Learning,"VIDEO,AUDIO","POSE,GAZE,PROS","CLS,CLUST",LATE,MLPALA,Machine Learning Paradigms: Advances in Learning Analytics,3,PHYS,STEM,"IND, MULTI",INSTR,UNSP,MB,,Joyce/Eduardo,1&2,
1469065963,examining socially shared regulation and shared physiological arousal events with multimodal learning analytics,Andy Nguyen,2022,Learning,"VIDEO,AUDIO,SENSOR","QUAL,EDA","PATT,CLS,CLUST",HYBRID,BJET,British Journal of Educational Technology,4,PHYS,STEM,MULTI,INSTR,K12,MB,"Results indicate that physiological arousal could indicate engagement in metacognitive interactions, provided evidence for physiological activities as triggers for adaptive loops of learning regulation in collaborative learning. They utilised the CNN model to classify the sequences of regulatory activities and Shared Physiological Arousal Events moments and results have provided a proof of concept and indicated the potential of using ML for predicting collaborative learning success.",Joyce,1,
1469065963,examining socially shared regulation and shared physiological arousal events with multimodal learning analytics,Andy Nguyen,2022,Learning,"VIDEO,AUDIO,SENSOR","QUAL,EDA","PATT,CLS,CLUST",HYBRID,BJET,British Journal of Educational Technology,4,PHYS,STEM,MULTI,INSTR,K12,MB,"The paper explores the correlation of physiological signals to metacognition interactions in a collaborative setting. Two researchers video coded sessions into sequences of interactions. With these sequences, Markov chains were made and a 1D CNN (input physiological signal, output interaction label) was trained, with preliminary results that suggest a possible future direction.",Eduardo,2,
1469065963,examining socially shared regulation and shared physiological arousal events with multimodal learning analytics,Andy Nguyen,2022,Learning,"VIDEO,AUDIO,SENSOR","QUAL,EDA","PATT,CLS,CLUST",HYBRID,BJET,British Journal of Educational Technology,4,PHYS,STEM,MULTI,INSTR,K12,MB,,Joyce/Eduardo,1&2,
1598166515,multimodal learning analytics for game-based learning,Andrew Emerson,2020,Learning,"VIDEO,LOGS,EYE","AFFECT,GAZE,LOGS,PPA","CLS,STATS",MID,BJET,British Journal of Educational Technology,5,VIRT,STEM,IND,INF,UNI,MB,"Results indicate that when predicting student posttest performance and interest, models utilizing multimodal data either perform equally well or outperform models utilizing unimodal data. The findings suggest that MMLA can accurately predict students’ posttest performance and interest during game-based learning and hold significant potential for guiding real-time adaptive scaffolding",Joyce,1,
1598166515,multimodal learning analytics for game-based learning,Andrew Emerson,2020,Learning,"VIDEO,LOGS,EYE","AFFECT,GAZE,LOGS,PPA","CLS,STATS",MID,BJET,British Journal of Educational Technology,5,VIRT,STEM,IND,INF,UNI,MB,"Common case of multimodal outperform unimodal models, through the addition of gaze to classify student's posttest performance and interest.",Eduardo,2,
1598166515,multimodal learning analytics for game-based learning,Andrew Emerson,2020,Learning,"VIDEO,LOGS,EYE","AFFECT,GAZE,LOGS,PPA","CLS,STATS",MID,BJET,British Journal of Educational Technology,5,VIRT,STEM,IND,INF,UNI,MB,,Joyce/Eduardo,1&2,
1877483551,motion-based educational games: using multi-modal data to predict player’s performance,Serena Lee-Cultura,2020,Learning,"VIDEO,EYE,SENSOR","PULSE,TEMP,EDA,GAZE,POSE",CLS,MID,COG,IEEE Conference on Games,6,BLND,STEM,IND,INSTR,K12,MB,Authors conclude that the feature combination of gaze and physiological MMD provide the most accurate predictions of correct answers. They also show the feasibility of early prediction of children's performance by using half (as oppose to full) data lengths to extract features and predict correctness.,Joyce,1,
1877483551,motion-based educational games: using multi-modal data to predict player’s performance,Serena Lee-Cultura,2020,Learning,"VIDEO,EYE,SENSOR","PULSE,TEMP,EDA,GAZE,POSE",CLS,MID,COG,IEEE Conference on Games,6,BLND,STEM,IND,INF,K12,MB,"Researchers show the ability to predict student's performance in embodied game using various data sources (gaze, physiological, skeleton). However, they note that using certain modalities (e.g., skeleton and physiological) reduce the predictive performance of the model.",Eduardo,2,
1877483551,motion-based educational games: using multi-modal data to predict player’s performance,Serena Lee-Cultura,2020,Learning,"VIDEO,EYE,SENSOR","PULSE,TEMP,EDA,GAZE,POSE",CLS,MID,COG,IEEE Conference on Games,6,BLND,STEM,IND,INSTR,K12,MB,,Joyce/Eduardo,1&2,
2000036002,predicting learners’ effortful behaviour in adaptive assessment using multimodal data,Kshitij Sharma,2020,Learning,"VIDEO,EYE,SENSOR","EDA,TEMP,PULSE,EEG,GAZE,AFFECT","CLUST,CLS,PATT",MID,LAK,International Conference on Learning Analytics & Knowledge,7,VIRT,STEM,IND,INSTR,UNI,MB,"Findings provide evidence that HMMs can encode the relationship between effort and behaviour (captured by the multimodal data) in a more efficient way than the other methods. A practical implication of the approach is that the derived HMMs also pinpoint the moments to provide preventive/prescriptive feedback to the learners in real-time, by building-upon the relationship between behavioural patterns and the effort the learners are putting in.",Joyce,1,
2000036002,predicting learners’ effortful behaviour in adaptive assessment using multimodal data,Kshitij Sharma,2020,Learning,"VIDEO,EYE,SENSOR","EDA,TEMP,PULSE,EEG,GAZE,AFFECT","CLUST,CLS,PATT",MID,LAK,International Conference on Learning Analytics & Knowledge,7,VIRT,STEM,IND,INSTR,UNI,MB,"The results show that the proposed method not only outperforms the contemporary classification algorithms but it also gives the educators several opportunities for providing (proactive) actionable feedback by pinpointing the exact moments in the learning activity
where feedback is needed.",Eduardo,2,
2000036002,predicting learners’ effortful behaviour in adaptive assessment using multimodal data,Kshitij Sharma,2020,Learning,"VIDEO,EYE,SENSOR","EDA,TEMP,PULSE,EEG,GAZE,AFFECT","CLUST,CLS,PATT",MID,LAK,International Conference on Learning Analytics & Knowledge,7,VIRT,STEM,IND,INSTR,UNI,MB,,Joyce/Eduardo,1&2,
2070224207,detecting medical simulation errors with machine learning and multimodal data,Daniele Di Mitri,2019,Training,"VIDEO,MOTION,LOGS","POSE,LOGS",CLS,MID,CAIM,Conference on Artificial Intelligence in Medicine,11,BLND,PSY,IND,TRAIN,UNI,MB,"Used each Chest Compression as training sample by masking/windowing of the original time series, then trained an LSTM network with all these samples and were able to classify accurately the target classes, however discarding the rest of the time-series they were not able to detect if a CC happened. Author asks Doctorial Consortium how, given the available data, could they train a classifier able to detect whether a CC happened or not.",Joyce,1,
2070224207,detecting medical simulation errors with machine learning and multimodal data,Daniele Di Mitri,2019,Training,"VIDEO,MOTION,LOGS","POSE,LOGS",CLS,MID,CAIM,Conference on Artificial Intelligence in Medicine,11,BLND,PSY,IND,TRAIN,UNI,MB,"Trained an LSTM to predict ['too slow', 'on-point', 'too fast'] for Chest compression training. Achieved 70-75% accuracy.",Eduardo,2,
2070224207,detecting medical simulation errors with machine learning and multimodal data,Daniele Di Mitri,2019,Training,"VIDEO,MOTION,LOGS","POSE,LOGS",CLS,MID,CAIM,Conference on Artificial Intelligence in Medicine,11,BLND,PSY,IND,TRAIN,UNI,MB,,Joyce/Eduardo,1&2,
2634033325,controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting,Xavier Ochoa,2020,Training,"VIDEO,AUDIO,PPA","POSE,PROS,PPA",STATS,OTH,BJET,British Journal of Educational Technology,12,BLND,HUM,IND,INF,UNSP,MF,"Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoidance of filled pauses)",Joyce,1,
2634033325,controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting,Xavier Ochoa,2020,Training,"VIDEO,AUDIO,PPA","POSE,PROS,PPA",STATS,OTH,BJET,British Journal of Educational Technology,12,BLND,HUM,IND,TRAIN,UNSP,MF,Authors showcase that the training tool improved manually defined scores between an initial and second use of the tool.,Eduardo,2,
2634033325,controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting,Xavier Ochoa,2020,Training,"VIDEO,AUDIO,PPA","POSE,PROS,PPA",STATS,OTH,BJET,British Journal of Educational Technology,12,BLND,HUM,IND,TRAIN,UNSP,MF,,Joyce/Eduardo,1&2,
3051560548,temporal analysis of multimodal data to predict collaborative learning outcomes,Jennifer K. Olsen,2020,Learning,"LOGS,AUDIO,EYE","GAZE,LOGS,PROS,TRANS,QUAL",REG,MID,BJET,British Journal of Educational Technology,13,VIRT,STEM,MULTI,INSTR,K12,MB,"Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.",Joyce,1,
3051560548,temporal analysis of multimodal data to predict collaborative learning outcomes,Jennifer K. Olsen,2020,Learning,"LOGS,AUDIO,EYE","GAZE,LOGS,PROS,TRANS,QUAL",REG,MID,BJET,British Journal of Educational Technology,13,VIRT,STEM,MULTI,INSTR,K12,MB,Evaluating how multimodal features contribute to a model's performance to predict learning gains. Audio features introduce noise that negatively impacted the error of the model.,Eduardo,2,
3051560548,temporal analysis of multimodal data to predict collaborative learning outcomes,Jennifer K. Olsen,2020,Learning,"LOGS,AUDIO,EYE","GAZE,LOGS,PROS,TRANS,QUAL",REG,MID,BJET,British Journal of Educational Technology,13,VIRT,STEM,MULTI,INSTR,K12,MB,,Joyce/Eduardo,1&2,
3339002981,estimation of success in collaborative learning based on multimodal learning analytics features,Daniel Spikol,2017,Learning,"EYE,LOGS,VIDEO,AUDIO","GAZE,LOGS,PROS,POSE",CLS,MID,ICALT,International Conference on Advanced Learning Technologies,14,VIRT,STEM,MULTI,INSTR,UNI,MB,"Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, authors found that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.",Joyce,1,
3339002981,estimation of success in collaborative learning based on multimodal learning analytics features,Daniel Spikol,2017,Learning,"EYE,LOGS,VIDEO,AUDIO","GAZE,LOGS,PROS,POSE",CLS,MID,ICALT,International Conference on Advanced Learning Technologies,14,VIRT,STEM,MULTI,INSTR,UNI,MB,"Predicting the learning gains via classification (bad, ok, good) through gaze, logs, audio, and dialog. Determined that distance measures between hands and gaze fixations was the key features to predict students' performance.",Eduardo,2,
3339002981,estimation of success in collaborative learning based on multimodal learning analytics features,Daniel Spikol,2017,Learning,"EYE,LOGS,VIDEO,AUDIO","GAZE,LOGS,PROS,POSE",CLS,MID,ICALT,International Conference on Advanced Learning Technologies,14,VIRT,STEM,MULTI,INSTR,UNI,MB,,Joyce/Eduardo,1&2,
3408664396,multimodal student engagement recognition in prosocial games,Athanasios Psaltis,2017,Learning,"VIDEO,LOGS","POSE,AFFECT,LOGS",CLS,LATE,T-CIAIG,Transactions on Computational Intelligence and AI in Games,15,BLND,HUM,IND,INF,K12,MB,"Presented a novel methodology for the automatic recognition of student engagement in prosocial games aiming to capture the different dimensions of engagement, i.e., behavioral, cognitive, and affective, by exploiting real-time engagement cues from different input modalities (body motion and facial expression analysis to identify the affective state of students, features related to their cognitive and behavioral engagement based on the analysis of their interaction with the game)",Joyce,1,
3408664396,multimodal student engagement recognition in prosocial games,Athanasios Psaltis,2017,Learning,"VIDEO,LOGS","POSE,AFFECT,LOGS",CLS,LATE,T-CIAIG,Transactions on Computational Intelligence and AI in Games,15,BLND,HUM,IND,INF,K12,MB,Same as Joyce (first paragraph in conclusion),Eduardo,2,
3408664396,multimodal student engagement recognition in prosocial games,Athanasios Psaltis,2017,Learning,"VIDEO,LOGS","POSE,AFFECT,LOGS",CLS,LATE,T-CIAIG,Transactions on Computational Intelligence and AI in Games,15,BLND,HUM,IND,INF,K12,MB,,Joyce/Eduardo,1&2,
804659204,towards smart educational recommendations with reinforcement learning in classroom,Su Liu,2018,Learning,"VIDEO,SENSOR","PULSE,AFFECT,GAZE",CLS,MID,TALE,"International Conference on Teaching, Assessment and Learning for Engineering",48,UNSP,UNSP,IND,INSTR,UNSP,MB,"By aplying reinforcement learning to select appropriate learning activities without being aware of the learning model of each student, simulation results showed that the proposed learning recommendation systems can promote students’ performance with higher average scores in the tests",Joyce,1,
804659204,towards smart educational recommendations with reinforcement learning in classroom,Su Liu,2018,Learning,"VIDEO,SENSOR","PULSE,AFFECT,GAZE",CLS,MID,TALE,"International Conference on Teaching, Assessment and Learning for Engineering",48,UNSP,UNSP,IND,INSTR,UNSP,MB,Determined that an RL activity recommendataiton system would assit (via simulated data) teachers in select the optimal learning activity to optimize collective learning gains in a classroom.,Eduardo,2,
804659204,towards smart educational recommendations with reinforcement learning in classroom,Su Liu,2018,Learning,"VIDEO,SENSOR","PULSE,AFFECT,GAZE",CLS,MID,TALE,"International Conference on Teaching, Assessment and Learning for Engineering",48,UNSP,UNSP,IND,INSTR,UNSP,MB,,Joyce/Eduardo,1&2,
1581261659,early prediction of visitor engagement in science museums with multimodal learning analytics,Andrew Emerson,2020,Learning,"VIDEO,EYE,LOGS","POSE,GEST,AFFECT,GAZE,LOGS",REG,MID,ICMI,International Conference on Multimodal Interaction,49,VIRT,STEM,IND,INF,K12,MB,"The evaluation revealed that predictive models of visitor dwell time can make improved predictions over time, and using additional modalities yields better performance as visitors near the end of their interactions. Random forest models outperformed competing models on three of the four modality combinations, and Lasso regression performed best on the unimodal configuration for predicting dwell time. Notably, overall predictive performance improves when removing eye gaze and interaction log modalities. However, removing facial expression results in a steep drop in performance.",Joyce,1,
1581261659,early prediction of visitor engagement in science museums with multimodal learning analytics,Andrew Emerson,2020,Learning,"VIDEO,EYE,LOGS","POSE,GEST,AFFECT,GAZE,LOGS",REG,MID,ICMI,International Conference on Multimodal Interaction,49,VIRT,STEM,IND,INF,K12,MB,Same as Joyce (first paragraph in conclusion),Eduardo,2,
1581261659,early prediction of visitor engagement in science museums with multimodal learning analytics,Andrew Emerson,2020,Learning,"VIDEO,EYE,LOGS","POSE,GEST,AFFECT,GAZE,LOGS",REG,MID,ICMI,International Conference on Multimodal Interaction,49,VIRT,STEM,IND,INF,K12,MB,,Joyce/Eduardo,1&2,
2836996318,predicting learners' emotions in mobile mooc learning via a multimodal intelligent tutor,Phuong Pham,2018,Learning,VIDEO,"AFFECT,PULSE",REG,LATE,ITS,International Conference on Intelligent Tutoring Systems,50,VIRT,STEM,IND,INSTR,UNI,MB,The tutor can detect 6 emotions in mobile MOOC learning reliably with high accuracy; it can also predict learning outcomes; work rpoves it is feasible to track both PPG signals and facial expressions in real time in a scalable manner on today’s unmodified smartphones.,Joyce,1,
2836996318,predicting learners' emotions in mobile mooc learning via a multimodal intelligent tutor,Phuong Pham,2018,Learning,VIDEO,"AFFECT,PULSE",REG,LATE,ITS,International Conference on Intelligent Tutoring Systems,50,VIRT,STEM,IND,INSTR,UNI,MB,"The study shows the feasibility of capturing rich and fine-grained physiological signals such as PPG signals and facial expressions in mobile learning contexts without introducing any addi‐
tional hardware. Experimental results show that PPG signals and facial expressions collected by AttentiveLearner2 in real time are complementary and can serve as fine-grained, rich signals to understand learners’ emotions. By capturing the temporal dynamics of both feature channels, AttentiveLearner2 can achieve higher performance by combining both PPG features and FEA features. Our approach is complementary to today’s existing technique such as clickstream analysis and is promising towards enabling personalized interventions for mobile MOOC learning.",Eduardo,2,
2836996318,predicting learners' emotions in mobile mooc learning via a multimodal intelligent tutor,Phuong Pham,2018,Learning,VIDEO,"AFFECT,PULSE",REG,LATE,ITS,International Conference on Intelligent Tutoring Systems,50,VIRT,STEM,IND,INSTR,UNI,MB,,Joyce/Eduardo,1&2,
32184286,once more with feeling: emotions in multimodal learning analytics,Marcus Kubsch,2022,Learning,"SURVEY,PPA,AUDIO","INTER,SURVEY,TRANS,PROS,AFFECT","CLS,REG,STATS",OTH,MMLA Handbook,The Multimodal Learning Analytics Handbook,51,PHYS,STEM,IND,INSTR,K12,MB,"Results demonstrate how NLP and ML techniques allow us to use different modalities of the same data, voice and transcript, and different modalities of different data sources, voice data from interviews, answers to a goal orientation questionnaire, and answers to open-ended questions about energy, in order to better understand individual differences in students’ performances",Joyce,1,
32184286,once more with feeling: emotions in multimodal learning analytics,Marcus Kubsch,2022,Learning,"SURVEY,PPA,AUDIO","INTER,SURVEY,TRANS,PROS,AFFECT","CLS,REG,STATS",OTH,MMLA Handbook,The Multimodal Learning Analytics Handbook,51,PHYS,STEM,IND,INSTR,K12,"MB, MF","Used text and audio to predict student's affect. With the affect, the authors' explored its statistical relation to student's knowledge. Results point that they need more data to improve performance in affect prediction but promising direction.",Eduardo,2,
32184286,once more with feeling: emotions in multimodal learning analytics,Marcus Kubsch,2022,Learning,"SURVEY,PPA,AUDIO","INTER,SURVEY,TRANS,PROS,AFFECT","CLS,REG,STATS",OTH,MMLA Handbook,The Multimodal Learning Analytics Handbook,51,PHYS,STEM,IND,INSTR,K12,"MB, MF",,Joyce/Eduardo,1&2,
433919853,understanding fun in learning to code: a multi-modal data approach,Gabriella Tisza,2022,Learning,"SENSOR,VIDEO,PPA","TEMP,PULSE,EDA,BP,AFFECT","REG,STATS",MID,IDC,Interaction Design and Children Conference,52,VIRT,STEM,IND,INSTR,K12,MB,"Sadness, anger and stress are negatively, and arousal is positively related to students’ relative learning gains; experienced fun is positively related to students’ RLG",Joyce,1,
433919853,understanding fun in learning to code: a multi-modal data approach,Gabriella Tisza,2022,Learning,"SENSOR,VIDEO,PPA","TEMP,PULSE,EDA,BP,AFFECT","REG,STATS",MID,IDC,Interaction Design and Children Conference,52,VIRT,STEM,IND,INSTR,K12,MF,"Our findings support endeavors of educators, designers, and re-
searchers to make learning to code a fun experience, as we found a positive relationship between those. Further research studies could aim to improve the applicability of physiological measure devices (e.g. wristbands) for children.",Eduardo,2,
433919853,understanding fun in learning to code: a multi-modal data approach,Gabriella Tisza,2022,Learning,"SENSOR,VIDEO,PPA","TEMP,PULSE,EDA,BP,AFFECT","REG,STATS",MID,IDC,Interaction Design and Children Conference,52,VIRT,STEM,IND,INSTR,K12,"MB, MF",,Joyce/Eduardo,1&2,
483140962,investigating multimodal affect sensing in an affective tutoring system using unobtrusive sensors,Hua Leong Fwa,2018,Learning,"VIDEO,LOGS","POSE,ACT,LOGS",CLS,MID,PPIG,Workshop Psychology of Programming Interest Group,53,VIRT,STEM,IND,INSTR,UNI,MB,"A multimodal approach offers higher accuracy and better robustness as compared to a unimodal approach (multimodal fusion leads to higher detection accuracy over unimodal model). In addition, the inclusion of keystrokes and mouse clicks makes up for the detection gap where video based sensing modes (facial and head postures) are not available.",Joyce,1,
483140962,investigating multimodal affect sensing in an affective tutoring system using unobtrusive sensors,Hua Leong Fwa,2018,Learning,"VIDEO,LOGS","POSE,ACT,LOGS",CLS,MID,PPIG,Workshop Psychology of Programming Interest Group,53,VIRT,STEM,IND,INSTR,UNI,MB,"The main goal of this study is to xplore automated techniques for the detection of frustration in a naturalistic learning environment. With adequate detection of frustration on a moment by moment
basis, hints and tutorial supports can be provided to the students to overcome learning barriers and
alleviate their frustration so as to sustain their engagement in learning.",Eduardo,2,
483140962,investigating multimodal affect sensing in an affective tutoring system using unobtrusive sensors,Hua Leong Fwa,2018,Learning,"VIDEO,LOGS","POSE,ACT,LOGS",CLS,MID,PPIG,Workshop Psychology of Programming Interest Group,53,VIRT,STEM,IND,INSTR,UNI,MB,,Joyce/Eduardo,1&2,
3308658121,exploring collaboration using motion sensors and multi-modal learning analytics,Joseph M. Reilly,2018,Learning,"VIDEO,AUDIO,PPA","PPA,RPA,POSE,GEST","CLUST,PATT,STATS",OTH,EDM,International Conference on Educational Data Mining,54,BLND,STEM,MULTI,INSTR,UNI,MF,Significant correlations found between average movement of points along the upper right side of participants’ bodies with outcome measures indicates the importance of gesturing and physical movement when communicating ideas.,Joyce,1,
3308658121,exploring collaboration using motion sensors and multi-modal learning analytics,Joseph M. Reilly,2018,Learning,"VIDEO,AUDIO,PPA","PPA,RPA,POSE,GEST","CLUST,PATT,STATS",OTH,EDM,International Conference on Educational Data Mining,54,BLND,STEM,MULTI,INF,UNI,"MB, MF","We plan to further identify productive micro-behaviors from the Kinect data to gain additional insights in the ways that dyads synchronized their actions. Future work with regards to prototypical postures would also explore both participants in a dyad at once, clustering on both joint angles simultaneously. This may reveal combinations of postures that are informative and could extend our exploration of physical synchrony within dyads.
The differences between dyads in different conditions will also be a main focus of analysis moving forward.",Eduardo,2,
3308658121,exploring collaboration using motion sensors and multi-modal learning analytics,Joseph M. Reilly,2018,Learning,"VIDEO,AUDIO,PPA","PPA,RPA,POSE,GEST","CLUST,PATT,STATS",OTH,EDM,International Conference on Educational Data Mining,54,BLND,STEM,MULTI,INSTR,UNI,"MB, MF",,Joyce/Eduardo,1&2,
1847468084,computationally augmented ethnography: emotion tracking and learning in museum games,Kit Martin,2019,Learning,"VIDEO,AUDIO,PPA,RPA","TRANS,AFFECT,QUAL",QUAL,OTH,ICQE,International Conference on Quantitative Ethnography,55,VIRT,STEM,MULTI,INF,UNSP,MB,"Authors present an initial pilot using these methods in concert to identify key moments in multiple modalities. While the use of constructivist dialogue mapping showed that the users learned during their interaction with Ant Adaption, emotional logging identified alternative moments of learning outside of their analytic framework.",Joyce,1,
1847468084,computationally augmented ethnography: emotion tracking and learning in museum games,Kit Martin,2019,Learning,"VIDEO,AUDIO,PPA,RPA","TRANS,AFFECT,QUAL",QUAL,OTH,ICQE,International Conference on Quantitative Ethnography,55,BLND,STEM,MULTI,INF,UNSP,MF,"This paper presented a preliminary approach to augment qualitative analysis of an
informal learning environment. Using techniques from multimodal learning analytics,
we were able to expand our analysis of learning while participants interacted with a
multitouch environment. Our methodological approach required us to extract emotions
from the low-level logs of facial action units using FACET and then revisit video
corresponding to particular FACET values to identify moments of high emotional stimulation theoretically implicated in learning.",Eduardo,2,
1847468084,computationally augmented ethnography: emotion tracking and learning in museum games,Kit Martin,2019,Learning,"VIDEO,AUDIO,PPA,RPA","TRANS,AFFECT,QUAL",QUAL,OTH,ICQE,International Conference on Quantitative Ethnography,55,VIRT,STEM,MULTI,INF,UNSP,"MB, MF",,Joyce/Eduardo,1&2,
2345021698,exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course,René Noël,2018,Learning,"AUDIO,PPA,RPA","RPA,PROS","QUAL,NET,STATS",OTH,Access,IEEE Access,56,PHYS,STEM,MULTI,INF,UNI,"MB, MF","``There was no difference between the proportion of the session spent being bored or frustrated or in the achievement scores for the two conditions. In order to determine whether the effect on achievement was different depending on length of exposure, participants with a duration less than 60 minutes were excluded from the analysis. However, there was still no significant difference between achievement scores from intervention and control sessions for the group as a whole or for each subgroup.``",Eduardo,1,"model-free for correlation study, mode-based for collaborative and non-collaborative classification"
2345021698,exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course,René Noël,2018,Learning,"AUDIO,PPA,RPA","RPA,PROS","QUAL,NET,STATS",OTH,Access,IEEE Access,56,PHYS,STEM,MULTI,INF,UNI,"MB, MF","Main findings of the case study are the relationships between prior experience in software requirements and the way the team members collaborate, and the lower productivity of low experienced groups. No evidence was found that performance of domain experts was superior from non-experts during collaborative problem-solving sessions. Although it was stated that low experience subjects produced more user stories, a greater productivity of top experience subjects was not statistically verified.",Joyce,2,
2345021698,exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course,René Noël,2018,Learning,"AUDIO,PPA,RPA","RPA,PROS","QUAL,NET,STATS",OTH,Access,IEEE Access,56,PHYS,STEM,MULTI,INF,UNI,"MB, MF",,Eduardo/Joyce,1&2,
3796643912,an evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities,Penelope J. Standen,2020,Learning,"VIDEO,AUDIO,LOGS,RPA","AFFECT,POSE,LOGS,RPA,GAZE,PROS,GEST","CLS,STATS",HYBRID,BJET,British Journal of Educational Technology,57,VIRT,UNSP,IND,INSTR,K12,MF,"This is the first study to evaluate an adaptive learning system for learners with ID based on multi-modal affect recognition. Three separate states were automatically identified, with lower levels of the state labelled “boredom” having the strongest link to learning achievement. Both those labelled “frustration” and “engagement” were positively related to achievement. Our results are in line with other studies showing that engagement increases when activities are tailored to the personal needs
and emotional states of learners (Athanasiadis et al., 2017), but no significant difference in learn-ing achievement was found (at least for the period of our study) when adaption was based on both the affective state and achievement of the learner, compared with achievement alone.",Eduardo,1,statistical model -> model-free
3796643912,an evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities,Penelope J. Standen,2020,Learning,"VIDEO,AUDIO,LOGS,RPA","AFFECT,POSE,LOGS,RPA,GAZE,PROS,GEST","CLS,STATS",HYBRID,BJET,British Journal of Educational Technology,57,VIRT,"HUM, OTH, STEM",IND,INSTR,K12,MB,"Results suggest that engagement does increase when activities are tailored to the personal needs and emotional state of the learner and that the system was promoting affective states that in turn promote learning. However, longer exposure is necessary to determine the effect on learning.",Joyce,2,
3796643912,an evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities,Penelope J. Standen,2020,Learning,"VIDEO,AUDIO,LOGS,RPA","AFFECT,POSE,LOGS,RPA,GAZE,PROS,GEST","CLS,STATS",HYBRID,BJET,British Journal of Educational Technology,57,VIRT,"HUM, OTH, STEM",IND,INSTR,K12,MB,,Eduardo/Joyce,1&2,
205660768,multimodal learning analytics to investigate cognitive load during online problem solving,Charlotte Larmuseau,2020,Learning,"PPA,SENSOR","PPA,PULSE,EDA","STATS,CLS",EARLY,BJET,British Journal of Educational Technology,58,VIRT,OTH,IND,UNSP,K12,MF,"Against our expectations, results revealed that physiological data could not be used to detect differences in CL based on intrinsic and extraneous manipulations. By contrast, most of the significant results are related to OSPAN and the baseline measurement. Based on our findings related to OSPAN, we might be able to conclude that HR, HRV and ST is more sensitive to high CL, namely, exceeding the learner’s cognitive capacity and the related mental states (ie, stress). In this respect, as high CL can also provoke stress, it is not always clear what exactly is measured via physiological data.",Eduardo,1,Relating to cognitive load theory as their model
205660768,multimodal learning analytics to investigate cognitive load during online problem solving,Charlotte Larmuseau,2020,Learning,"PPA,SENSOR","PPA,PULSE,EDA","STATS,CLS",EARLY,BJET,British Journal of Educational Technology,58,VIRT,STEM,IND,INSTR,"K12, UNI","MB, MF","This study manipulated intrinsic and extraneous load to investigate how physiological features, namely, GSR, ST and HR(V) vary as a result of changes in CL. Results revealed no significant differences between the manipulated conditions in terms of physiological data. Nonetheless, HR and ST were significantly related to self-reported CL, whereas ST to task performance. Additionally, this study revealed the potential of ST and HR to assess high CL.",Joyce,2,
205660768,multimodal learning analytics to investigate cognitive load during online problem solving,Charlotte Larmuseau,2020,Learning,"PPA,SENSOR","PPA,PULSE,EDA","STATS,CLS",EARLY,BJET,British Journal of Educational Technology,58,VIRT,STEM,IND,INSTR,"K12, UNI","MB, MF",,Eduardo/Joyce,1&2,
2181637610,toward using multi-modal learning analytics to support and measure collaboration in co-located dyads,Emma L. Starr,2018,Learning,"VIDEO,AUDIO,PPA,RPA,LOGS","POSE,PROS,PPA,RPA,LOGS","STATS,QUAL",OTH,ICLS,International Conference of the Learning Sciences,59,BLND,STEM,MULTI,INF,UNI,MF,"While this study was not able to show a clear effect of providing a real-time visualization to support
collaboration, it made many other valuable contributions. First, it showed that simple verbal interventions can
help participants pay attention to particular aspects of their collaborative behavior (i.e., how much they are
talking and how much space they are providing to their partner). Second, it suggested that awareness tools such
as the one developed for this study have to be designed differently to impact social interactions (e.g., by being
more salient or be used in a setting where users have the mental bandwidth to reflect on their collaborative
style). Third, we collected a rich multi-modal dataset that can be used to build proxies for measuring effective
collaborations.",Eduardo,1,Relating their research to Roschelle's 1992 framework of convergent conceptual change for collaboration
2181637610,toward using multi-modal learning analytics to support and measure collaboration in co-located dyads,Emma L. Starr,2018,Learning,"VIDEO,AUDIO,PPA,RPA,LOGS","POSE,PROS,PPA,RPA,LOGS","STATS,QUAL",OTH,ICLS,International Conference of the Learning Sciences,59,BLND,STEM,MULTI,INF,UNI,MF,"The purpose of this paper was to explore the effect of two collaboration interventions and the relationship between collaboration quality, task performance and learning gains, however this study was not able to show a clear effect of providing a real-time visualization to support collaboration. It did show that simple verbal interventions can help participants pay attention to particular aspects of their collaborative behavior, and suggested that awareness tools such as the one developed for this study have to be designed differently to impact social interactions. Authors built a rich multi-modal dataset that can be used to build proxies for measuring effective collaborations. As a preliminary analysis, they found that various indicators captured by the Kinect sensor were correlated with participants’ quality of collaboration.",Joyce,2,
2181637610,toward using multi-modal learning analytics to support and measure collaboration in co-located dyads,Emma L. Starr,2018,Learning,"VIDEO,AUDIO,PPA,RPA,LOGS","POSE,PROS,PPA,RPA,LOGS","STATS,QUAL",OTH,ICLS,International Conference of the Learning Sciences,59,BLND,STEM,MULTI,INF,UNI,MF,,Eduardo/Joyce,1&2,
4035649049,storytelling with learner data: guiding student reflection on multimodal team data,Gloria Fernández-Nieto,2021,Training,"SENSOR,LOGS,RPA","EDA,LOGS,RPA,AFFECT",QUAL,OTH,TLT,Transactions on Learning Technologies,60,BLND,STEM,MULTI,INF,UNI,MF,"This article presented two qualitative studies conducted in authentic nursing simulation classrooms with the purpose of communicating insights to students through data stories. Given the limitations of current visual analytics, we anticipate that approaches such as DS will grow in importance to help students make the most
of the new forms of feedback that are becoming possible.",Eduardo,1,Data stories
4035649049,storytelling with learner data: guiding student reflection on multimodal team data,Gloria Fernández-Nieto,2021,Training,"SENSOR,LOGS,RPA","EDA,LOGS,RPA,AFFECT",QUAL,OTH,TLT,Transactions on Learning Technologies,60,BLND,STEM,MULTI,"INSTR, TRAIN",UNI,MF,"This article results show that the enhancements using DS principles helped students identify misconceptions, think about strategies to address errors they made, and reflect on the arousal levels they may have experienced during the simulations. Although the studies presented in this article were conducted in the context of complex, multimodal learning situations, there is no reason why a storytelling approach could not be implemented to aid in the interpretation of more conventional LA visualizations supporting noncollocated teamwork.",Joyce,2,
4035649049,storytelling with learner data: guiding student reflection on multimodal team data,Gloria Fernández-Nieto,2021,Training,"SENSOR,LOGS,RPA","EDA,LOGS,RPA,AFFECT",QUAL,OTH,TLT,Transactions on Learning Technologies,60,BLND,STEM,MULTI,TRAIN,UNI,MF,,Eduardo/Joyce,1&2,
1019093033,prime: block-wise missingness handling for multi-modalities in intelligent tutoring systems,Xi Yang,2019,Learning,"PPA,VIDEO,EYE","LOGS,AFFECT,GAZE","CLS,STATS",HYBRID,MMM,International Conference on Multimedia Modeling,61,VIRT,STEM,IND,UNSP,UNI,MB,"In this work, we proposed a data imputation method called PRIME for blockwise missingness handling in multimodal data and measured its effectiveness in a student modeling task to predict students’ learning gain in an ITS. Through experiments, we demonstrated that: (1) the multimodal data is more effective than the single-modal data; (2) compared to competitive baseline missing data handling methods, the PRIME can not only improve the prediction performance, but also achieve more accurate reconstruction results.",Eduardo,1,Data imputation
1019093033,prime: block-wise missingness handling for multi-modalities in intelligent tutoring systems,Xi Yang,2019,Learning,"PPA,VIDEO,EYE","LOGS,AFFECT,GAZE","CLS,STATS",HYBRID,MMM,International Conference on Multimedia Modeling,61,VIRT,STEM,IND,INSTR,UNI,MB,"Results show that using multimodal data as a result of missing data handling yields better prediction performance than using logfiles only, and PRIME outperforms other baseline methods for both learning gain prediction and data reconstruction tasks",Joyce,2,
1019093033,prime: block-wise missingness handling for multi-modalities in intelligent tutoring systems,Xi Yang,2019,Learning,"PPA,VIDEO,EYE","LOGS,AFFECT,GAZE","CLS,STATS",HYBRID,MMM,International Conference on Multimedia Modeling,61,VIRT,STEM,IND,INSTR,UNI,MB,,Eduardo/Joyce,1&2,
2936220551,multi-source and multimodal data fusion for predicting academic performance in blended learning university courses,Wilson Chango,2020,Learning,"VIDEO,LOGS,PPA","LOGS,POSE,RPA,ACT","CLS,QUAL,STATS","MID,LATE",CEE,Elsevier Computers and Electrical Engineering,62,BLND,STEM,IND,INSTR,UNI,MB,"Which data fusion approach and classification algorithms produce the best results from our data? The use of ensembles and selecting the best attributes approach from discretized summary data produced our highest/best results in Accuracy and AUC values. The REPTree classification algorithm obtained the highest/best results in this approach from discretized summary data. 

• How useful are the prediction models we produce to help teachers detect students who are at risk of drop out or fail the course? The white-box models we produced give teachers very understandable explanations (IF-THEN rules) of how they classified the students’ final performance or classification. They showed that the attributes that appear most in these rules were attention in theory classes,
scores in Moodle quizzes, and the level of activity in the Moodle forum.",Eduardo,1,Predicting student drop-out from LMS data
2936220551,multi-source and multimodal data fusion for predicting academic performance in blended learning university courses,Wilson Chango,2020,Learning,"VIDEO,LOGS,PPA","LOGS,POSE,RPA,ACT","CLS,QUAL,STATS","MID,LATE",CEE,Elsevier Computers and Electrical Engineering,62,BLND,STEM,IND,INSTR,UNI,MB,"The results show that the best predictions are produced using ensembles and selecting the best attributes approach with discretized data. The best prediction models show that the level of attention in theory classes, scores in Moodle quizzes, and the level of activity in Moodle forums are the best set of attributes for predicting students’ final performance in their courses.",Joyce,2,
2936220551,multi-source and multimodal data fusion for predicting academic performance in blended learning university courses,Wilson Chango,2020,Learning,"VIDEO,LOGS,PPA","LOGS,POSE,RPA,ACT","CLS,QUAL,STATS","MID,LATE",CEE,Elsevier Computers and Electrical Engineering,62,BLND,STEM,IND,INSTR,UNI,MB,,Eduardo/Joyce,1&2,
1886134458,personalizing computer science education by leveraging multimodal learning analytics,David Azcona,2018,Learning,"LOGS,PPA","LOGS,PPA","CLS,STATS",MID,FIE,Frontiers in Education Conference,63,VIRT,STEM,IND,INSTR,UNI,MB,"Overall, feedback was very positive and responses can be found in Table IX. Most students would recommend this system to students attending the same course next year or would like to see this system included in other courses as shown in questions 5 and 6 respectively. In terms of the last question to improve the system, students who were doing well or very well, were getting an increasingly similar response each week and were demanding a more personalised notification and some other additional learning resources.",Eduardo,1,
1886134458,personalizing computer science education by leveraging multimodal learning analytics,David Azcona,2018,Learning,"LOGS,PPA","LOGS,PPA","CLS,STATS",MID,FIE,Frontiers in Education Conference,63,VIRT,STEM,IND,INSTR,UNI,MB,"Predictive models built using student characteristics, prior academic history, logged interactions between students and online resources, and students’ progress in programming laboratory work were used to give weekly predictions to students. Predictions worked relatively well with one year of training data for the three courses. Authors noted that CS2 and SH1’s models were based on 2015/16’s previous student data and PF3’s was based on PF2’s student data from the first semester of the academic year, thus they did not expect it to work as well as the other models as the courseware was not the same.",Joyce,2,
1886134458,personalizing computer science education by leveraging multimodal learning analytics,David Azcona,2018,Learning,"LOGS,PPA","LOGS,PPA","CLS,STATS",MID,FIE,Frontiers in Education Conference,63,VIRT,STEM,IND,INSTR,UNI,MB,,Eduardo/Joyce,1&2,
2879332689,from data to insights: a layered storytelling approach for multimodal learning analytics,Roberto Martinez-Maldonado,2020,Training,"LOGS,MOTION,SENSOR,RPA,VIDEO","POSE,EDA,LOGS,RPA",QUAL,OTH,CHI,Conference on Human Factors in Computing Systems,64,BLND,STEM,MULTI,INF,UNI,MF,"This paper documents how we have wrestled with the challenge of designing activity-based feedback visualisations which draw the attention of non-technical users to key insights in the data. We argue, supported by user studies, that this work advances the state of the art in making multimodal data streams intelligible to non-data experts. The approach should enable similar collocated activities to benefit from these novel collaboration analytics.",Eduardo,1,
2879332689,from data to insights: a layered storytelling approach for multimodal learning analytics,Roberto Martinez-Maldonado,2020,Training,"LOGS,MOTION,SENSOR,RPA,VIDEO","POSE,EDA,LOGS,RPA",QUAL,OTH,CHI,Conference on Human Factors in Computing Systems,64,BLND,STEM,MULTI,"INSTR, TRAIN",UNI,MF,"This paper documents how authors tackled the challenge of designing activity-based feedback visualisations which draw the attention of non-technical users to key insights in the data. We argue, supported by user studies, that this work advances the state of the art in making multimodal data streams intelligible to non-data experts. The approach should enable similar collocated activities to benefit from these novel collaboration analytics.",Joyce,2,
2879332689,from data to insights: a layered storytelling approach for multimodal learning analytics,Roberto Martinez-Maldonado,2020,Training,"LOGS,MOTION,SENSOR,RPA,VIDEO","POSE,EDA,LOGS,RPA",QUAL,OTH,CHI,Conference on Human Factors in Computing Systems,64,BLND,STEM,MULTI,TRAIN,UNI,MF,,Eduardo/Joyce,1&2,
3146393211,mobile mixed reality for experiential learning and simulation in medical and health sciences education,James Birt,2018,Learning,"PPA,INTER","PPA,TRANS","QUAL,STATS",OTH,Information,MDPI Information,65,BLND,STEM,IND,INSTR,UNI,MF,"Finally, it was clear from the two intervention studies that the simulation provided by mobile devices, although not as powerful as the desktop-computer-powered Oculus Rift mixed reality or physical simulators, has no negative impacts on the learners. In fact, the simulation improves learners’ competence and skills with the skills intervention showing statistically significant improvements in the learners that received the mobile mixed reality simulation tools prior to residential school.",Eduardo,1,
3146393211,mobile mixed reality for experiential learning and simulation in medical and health sciences education,James Birt,2018,Learning,"PPA,INTER","PPA,TRANS","QUAL,STATS",OTH,Information,MDPI Information,65,BLND,STEM,IND,INSTR,UNI,MF,"This study validates the use of mobile devices in university undergraduate health sciences curricula, and shows that not only are these modes (game engines, free AR/VR SDKs and mobile-based devices with GPU-enabled processors and high-quality screens) useful for enhancing the development of physical skills in students, but they are also received favorably.",Joyce,2,
3146393211,mobile mixed reality for experiential learning and simulation in medical and health sciences education,James Birt,2018,Learning,"PPA,INTER","PPA,TRANS","QUAL,STATS",OTH,Information,MDPI Information,65,BLND,STEM,IND,INSTR,UNI,MF,,Eduardo/Joyce,1&2,
3809293172,"blending learning analytics and embodied design to model students' comprehension of measurement using their actions, speech, and gestures",Avery H. Closser,2021,Learning,"VIDEO,AUDIO",RPA,"CLUST,REG",MID,IJCCI,International Journal of Child-Computer Interaction,66,PHYS,STEM,IND,INF,"K12, UNI","MB, MF","In this paper, we applied clustering, natural language processing, and general linear modeling to a small yet rich dataset detailing student behaviors and speech during measurement tasks to identify successful measurement strategies. Our findings revealed profiles of student behavior and speech that may indicate different levels of conceptual knowledge as well as evidence that spatial and kinetographic gestures predict performance on mea-
surement tasks.",Eduardo,1,
3809293172,"blending learning analytics and embodied design to model students' comprehension of measurement using their actions, speech, and gestures",Avery H. Closser,2021,Learning,"VIDEO,AUDIO",RPA,"CLUST,REG",MID,IJCCI,International Journal of Child-Computer Interaction,66,PHYS,STEM,IND,INF,"K12, UNI",MB,"Authors explored students’ conceptual understanding of measurement to indentify measurement estimation strategies that should be emphasized in classroom instruction. By applying machine-learning methods to a small, multimodal dataset from a study on student behavior in mathematics, we identified behavioral profiles, patterns in speech, and specific actions and gestures that are predictive of performance.",Joyce,2,
3809293172,"blending learning analytics and embodied design to model students' comprehension of measurement using their actions, speech, and gestures",Avery H. Closser,2021,Learning,"VIDEO,AUDIO",RPA,"CLUST,REG",MID,IJCCI,International Journal of Child-Computer Interaction,66,PHYS,STEM,IND,INF,"K12, UNI","MB, MF",,Eduardo/Joyce,1&2,
4019205162,introducing low-cost sensors into the classroom settings: improving the assessment in agile practices with multimodal learning analytics,Hector Cornide-Reyes,2019,Learning,"AUDIO,SURVEY,PPA,RPA","SURVEY,TRANS,PPA,RPA","NET,STATS","MID,OTH",Sensors,MDPI Sensors,67,PHYS,STEM,MULTI,INF,UNI,"MB, MF","RQ1: Better communication, better collaboration
RQ2: Collaborative teams showed lower variability in the estimates of story points (same page)
RQ3: Democratic leadership in collaborative groups",Eduardo,1,
4019205162,introducing low-cost sensors into the classroom settings: improving the assessment in agile practices with multimodal learning analytics,Hector Cornide-Reyes,2019,Learning,"AUDIO,SURVEY,PPA,RPA","SURVEY,TRANS,PPA,RPA","NET,STATS","MID,OTH",Sensors,MDPI Sensors,67,PHYS,STEM,MULTI,INSTR,UNI,MF,"The results obtained allowed the detection of important relationships and characteristics of the collaborative and Non-Collaborative groups, with productivity, effort, and predominant personality styles in the groups. Authors conclude that the Multimodal Learning Analysis techniques offer considerable feasibilities to support the process of skills development in students.",Joyce,2,
4019205162,introducing low-cost sensors into the classroom settings: improving the assessment in agile practices with multimodal learning analytics,Hector Cornide-Reyes,2019,Learning,"AUDIO,SURVEY,PPA,RPA","SURVEY,TRANS,PPA,RPA","NET,STATS","MID,OTH",Sensors,MDPI Sensors,67,PHYS,STEM,MULTI,INSTR,UNI,"MB, MF",,Eduardo/Joyce,1&2,
4277812050,improving prediction of students' performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources,Wilson Chango,2021,Learning,"LOGS,VIDEO,EYE,PPA","AFFECT,LOGS,GAZE,PPA",CLS,"HYBRID,LATE",JCHE,Journal of Computing in Higher Education,68,VIRT,STEM,IND,INF,UNI,MB,"The implications of the current study point to Web ITS and Web-based Adaptive Educational Systems. If data is captured from diferent data sources, the classifer ensemble methodology proposed in this study could make better, earlier performance predictions than the single data source models that are commonly used at present.",Eduardo,1,
4277812050,improving prediction of students' performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources,Wilson Chango,2021,Learning,"LOGS,VIDEO,EYE,PPA","AFFECT,LOGS,GAZE,PPA",CLS,"HYBRID,LATE",JCHE,Journal of Computing in Higher Education,68,VIRT,STEM,IND,INSTR,UNI,MB,"Authors tested whether prediction of learning performance could be improved by using attribute selection and classification ensembles. By carrying out three experiments and applying six classifcation algorithms to numerical and discretized preprocessed multimodal data, results show that the best predictions were produced using ensembles and selecting the best attributes approach with numerical data.",Joyce,2,
4277812050,improving prediction of students' performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources,Wilson Chango,2021,Learning,"LOGS,VIDEO,EYE,PPA","AFFECT,LOGS,GAZE,PPA",CLS,"HYBRID,LATE",JCHE,Journal of Computing in Higher Education,68,VIRT,STEM,IND,INSTR,UNI,MB,,Eduardo/Joyce,1&2,
1609706685,learning pulse: a machine learning approach for predicting performance in self-regulated learning using multimodal data,Daniele Di Mitri,2017,Training,"SENSOR,LOGS,MOTION,PPA","PULSE,ACT,AFFECT",REG,MID,LAK,International Conference on Learning Analytics & Knowledge,69,BLND,UNSP,IND,UNSP,UNI,MB,"This paper described Learning Pulse, an exploratory study whose aim was to use predictive modelling to generate timely predictions about learners’ performance during self-regulated learning by collecting multimodal data about their body, activity and context. Although the prediction accuracy with the data sources and experimental setup chosen in Learning Pulse led to modest results, all the research questions have been answered positively and have lead towards new insights on the storing, modelling and processing multimodal data.",Eduardo,1,
1609706685,learning pulse: a machine learning approach for predicting performance in self-regulated learning using multimodal data,Daniele Di Mitri,2017,Training,"SENSOR,LOGS,MOTION,PPA","PULSE,ACT,AFFECT",REG,MID,LAK,International Conference on Learning Analytics & Knowledge,69,BLND,UNSP,IND,UNSP,UNI,MB,"This paper described an exploratory study whose aim was to use predictive modelling to generate timely predictions about learners’ performance during self-regulated learning by collecting multimodal data about their body, activity and context. The limited significance of the prediction results did not allow authors to assert that accurate and learner-specific predictions can be generated, however all the research questions have been answered positively and have lead towards new insights on the storing, modelling and processing multimodal data.",Joyce,2,
1609706685,learning pulse: a machine learning approach for predicting performance in self-regulated learning using multimodal data,Daniele Di Mitri,2017,Training,"SENSOR,LOGS,MOTION,PPA","PULSE,ACT,AFFECT",REG,MID,LAK,International Conference on Learning Analytics & Knowledge,69,BLND,UNSP,IND,UNSP,UNI,MB,,Eduardo/Joyce,1&2,
3398902089,what multimodal data can tell us about the students’ regulation of their learning process?,Sanna Järvelä,2019,Learning,"SENSOR,VIDEO,AUDIO","EDA,AFFECT,QUAL",QUAL,OTH,LAI,Elsevier Learning and Instruction,70,PHYS,STEM,MULTI,INF,K12,MF,"Our major claim is that multichannel data can be potential for understanding regulatory processes in collaboration. With our five empirical case examples, we illustrate how triangulating multiple sources of data has potential to advance the theoretical and conceptual progress in social aspects of SRL theory.",Eduardo,1,
3398902089,what multimodal data can tell us about the students’ regulation of their learning process?,Sanna Järvelä,2019,Learning,"SENSOR,VIDEO,AUDIO","EDA,AFFECT,QUAL",QUAL,OTH,LAI,Elsevier Learning and Instruction,70,BLND,STEM,MULTI,INSTR,K12,MB,"Authors show with five empirical cases that multichannel data can be potential for understanding regulatory processes in collaboration, illustrating how triangulating multiple sources of data has potential to advance the theoretical and conceptual progress in social aspects of SRL theory: (1) understanding how interactions between different facets of regulation, such as cognition, motivation and emotion interact with cognitive strategic action by using video and EDA data; (2) visualizing how physiological synchrony measured from the heart rate can reveal or backup the interpretation of socially shared regulation of learning or co-regulation of learning located from the video; (3) visualizing temporality and cyclical processes (i.e., planning, enacting strategies, reflecting, adapting) of regulation by using video, EDA and facial expression recognition data; (5)) illustrating how combining not only physiological measures, but also facial expression data can lead even more accurate interpretations of the situations where regulation of learning is needed.",Joyce,2,
3398902089,what multimodal data can tell us about the students’ regulation of their learning process?,Sanna Järvelä,2019,Learning,"SENSOR,VIDEO,AUDIO","EDA,AFFECT,QUAL",QUAL,OTH,LAI,Elsevier Learning and Instruction,70,BLND,STEM,MULTI,INSTR,K12,MF,,Eduardo/Joyce,1&2,
3093310941,embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders,Hiroki Tanaka,2017,Training,"AUDIO,VIDEO,PPA","POSE,PROS,AFFECT","REG,STATS",MID,PLOS,PLOS ONE,71,VIRT,HUM,IND,TRAIN,"K12, UNI",MF,"We identified a positive correlation between overall narrative skills and other non-verbal behaviors. This result indicates that the overall narrative skills were subjectively decided based on other non-verbal behavior skills [22]. Although the automated social skills trainer did not provide feedback regarding narrative structure or eye gaze, these elements were also improved after the training.",Eduardo,1,
3093310941,embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders,Hiroki Tanaka,2017,Training,"AUDIO,VIDEO,PPA","POSE,PROS,AFFECT","REG,STATS",MID,PLOS,PLOS ONE,71,VIRT,HUM,IND,TRAIN,"K12, UNI",MF,"The focus of this study assessed the effectiveness of an automated social skills trainer with multimodal information that adheres to the basic human-based SST as closely as possible. Authors extended a previous method for automatic social skills training by adding audiovisual information regarding smiling ratio and head pose that improved the training effect. 
Multimodal feedback is also useful for both members of the general population with social difficulties and people with ASD because it helps such people understand and improve their narrative skills, as was previously reported in human-based SST [2, 3].",Joyce,2,
3093310941,embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders,Hiroki Tanaka,2017,Training,"AUDIO,VIDEO,PPA","POSE,PROS,AFFECT","REG,STATS",MID,PLOS,PLOS ONE,71,VIRT,HUM,IND,TRAIN,"K12, UNI",MF,,Eduardo/Joyce,1&2,
1576545447,artificial intelligence and multimodal data in the service of human decision-making: a case study in debate tutoring,Mutlu Cukurova,2019,Learning,"AUDIO,SURVEY","AFFECT,LOGS",CLS,MID,BJET,British Journal of Educational Technology,72,UNSP,HUM,IND,TRAIN,UNSP,MF,"In this study, we used non-transparent prediction models of openSMILE to predict the emotional traits of tutor candidates based on their audio data, and we used transparent logistic regression models to identify exactly what personality, emotion, and experience traits lead to effective debate tutoring skills. Predictive models were very powerful to make sense of complex and nonlinear audio data, whereas the transparent regression models were valuable to identify key aspects for tutors to reflect upon their own decisions and provide tutor candidates with feedback on their performance.",Eduardo,1,
1576545447,artificial intelligence and multimodal data in the service of human decision-making: a case study in debate tutoring,Mutlu Cukurova,2019,Learning,"AUDIO,SURVEY","AFFECT,LOGS",CLS,MID,BJET,British Journal of Educational Technology,72,PHYS,HUM,IND,TRAIN,UNSP,MB,Authors combined predictive and transparent models to support the human decision-making processes involved in tutor trainee evaluations and results showed that models with multimodal data can accurately classify tutors and have the potential to support the intuitive decision-making of expert tutors in the context of evaluating trainee applicants.,Joyce,2,
1576545447,artificial intelligence and multimodal data in the service of human decision-making: a case study in debate tutoring,Mutlu Cukurova,2019,Learning,"AUDIO,SURVEY","AFFECT,LOGS",CLS,MID,BJET,British Journal of Educational Technology,72,UNSP,HUM,IND,TRAIN,UNSP,"MB, MF",,Eduardo/Joyce,1&2,
3796180663,learning linkages: integrating data streams of multiple modalities and timescales,Ran Liu,2018,Learning,"VIDEO,AUDIO,LOGS,SCREEN,PPA","TRANS,QUAL,PPA","CLS,STATS",MID,JCAL,Journal of Computer Assisted Learning,73,VIRT,STEM,IND,INF,K12,MF,"We presented two empirical studies, collected in classroom studies with two distinct learning technology systems in different contexts (individual and collaborative). Our analyses and findings showcase a few different ways, in which multimodal data sources can enrich our understanding of student learning and provide a more holistic picture.",Eduardo,1,
3796180663,learning linkages: integrating data streams of multiple modalities and timescales,Ran Liu,2018,Learning,"VIDEO,AUDIO,LOGS,SCREEN,PPA","TRANS,QUAL,PPA","CLS,STATS",MID,JCAL,Journal of Computer Assisted Learning,73,VIRT,STEM,"IND, MULTI",INSTR,K12,"MB, MF","Authors collected student‐focused screen and webcam video which were useful for understanding students' learning processes and approaches based on detailed analyses of their interactions with the tutor interface, mouse movements, and out‐of‐tutor (in person) help‐seeking. High‐fidelity audio of students' collaborative dialogue was collected to generate high‐quality transcriptions of students' dialogue and apply an NLP approach to make use of the large quantity of audio dialogue. The verbal data allowed authors to identify linguistic features in students' collaborative dialogue that were highly predictive of math performance on pretest and posttest assessments, above and beyond any nonlinguistic variables.",Joyce,2,
3796180663,learning linkages: integrating data streams of multiple modalities and timescales,Ran Liu,2018,Learning,"VIDEO,AUDIO,LOGS,SCREEN,PPA","TRANS,QUAL,PPA","CLS,STATS",MID,JCAL,Journal of Computer Assisted Learning,73,VIRT,STEM,"IND, MULTI",INSTR,K12,"MB, MF",,Eduardo/Joyce,1&2,NLP
1770989706,focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving,Hana Vrzakova,2020,Learning,"AUDIO,VIDEO,SCREEN,SURVEY,PPA","PROS,ACT,GEST,PPA","STATS,PATT",MID,LAK,International Conference on Learning Analytics & Knowledge,1,VIRT,STEM,MULTI,INF,UNI,MB,"We found that idling with limited speech (i.e., silence or backchannel feedback only) and without movement was negatively correlated with task performance and with participants’ subjective perceptions of the collaboration. However, being silent and focused during solution execution was positively correlated with task performance. Results illustrate that in some cases, multimodal patterns improved the predictions and improved explanatory power over the unimodal primitives.",Caleb,1,
1770989706,focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving,Hana Vrzakova,2020,Learning,"AUDIO,VIDEO,SCREEN,SURVEY,PPA","PROS,ACT,GEST,PPA","STATS,PATT",MID,LAK,International Conference on Learning Analytics & Knowledge,1,VIRT,STEM,MULTI,INSTR,UNI,MF,"""Here, we examine unimodal primitives (activity on the screen, speech, and body movements), and their multimodal combinations during remote CPS.""

Mixed findings for uni- versus multi-modal:

""These results lead us to question: are the multimodal patterns better than the unimodal primitives? As illustrated above, we found evidence for both sides of the argument. In the case of code execution, the answer is no, but it is a yes in the case of idling. However, it is important to go beyond the significant correlations as there is an informative signal in the non-significant ones as well. For example, consider idling once again. By itself, this pattern is negatively correlated with the task score (r = -.21) and the correlation is even more negative when idling is accompanied by silence/back channeling and little movement (r = -.35). However, there are many other configurations where idling is weak or negligible predictor of task score. For example, idling occurring in the context of the contributors speaking with some movement is more weakly correlated with task score (r = -.11) and the correlation is essentially null when idling is accompanied with the controller speaking and some movement (r = -.06). Thus, even when they do not improve predictive power, multimodal patterns help contextualize and reveal nuances in the unimodal primitives. This supports the overall idea of multimodal learning analytics in which the additional modalities (speech and body movement in our case) help to understand unclear patterns such as idling. This finding is interesting from two perspectives.""",Clayton,2,"CPS study, Minecraft playing via Blockly. Model-free because only stats methods/pattern extraction and no AI/ML model or formal theoretical model. Compared unimodal to multimodal, and made arguments for both given the context."
1770989706,focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving,Hana Vrzakova,2020,Learning,"AUDIO,VIDEO,SCREEN,SURVEY,PPA","PROS,ACT,GEST,PPA","STATS,PATT",MID,LAK,International Conference on Learning Analytics & Knowledge,1,VIRT,STEM,MULTI,INSTR,UNI,MF,,Caleb/Clayton,1&2,
2456887548,an unobtrusive and multimodal approach for behavioral engagement detection of students,Nese Alyuz,2017,Learning,"LOGS,VIDEO,SCREEN,PPA","AFFECT,POSE,LOGS,PPA",CLS,HYBRID,MIE,International Workshop on Multimodal Interaction for Education,2,VIRT,STEM,IND,INSTR,K12,MB,"The experiments on authentically collected student dataset showed that it is beneficial to have separate classification pipelines for different learning sections of Instructional and Assessment. For the Instructional section, using only Appearance modality yields an F1-measure of 0.74, compared to fused performance of 0.70. For Assessment, fusing all three modalities (F1-measure of 0.89) provide a prominent improvement over the best performing unimodality (0.81 for Appearance).",Caleb,1,
2456887548,an unobtrusive and multimodal approach for behavioral engagement detection of students,Nese Alyuz,2017,Learning,"LOGS,VIDEO,SCREEN,PPA","AFFECT,POSE,LOGS,PPA",CLS,HYBRID,MIE,International Workshop on Multimodal Interaction for Education,2,VIRT,STEM,IND,INSTR,K12,MB,"Mixed findings for uni- versus multi-modal:

""The experiments on authentically collected student dataset showed that it is beneficial to have separate classification pipelines for different learning sections of Instructional and Assessment. For the Instructional section, using only Appearance modality yields an F1-measure of 0.74, compared to fused performance of 0.70. For Assessment, fusing all three modalities (F1-measure of 0.89) provide a prominent improvement over the best performing unimodality (0.81 for Appearance). Interestingly, although Context-Performance modality performs poorly for the Off-Task class when considered alone, it helps to eliminate false positives (especially for the Off-Task class) when incorporated into the other modalities. In summary, we can say that for Instructional section types, Appearance modality provides acceptable results; whereas for Assessment sections, all available information should be fused to achieve best performance.""",Clayton,2,Math virtual environment with 9th graders for engagement detection. Random Forest model.
2456887548,an unobtrusive and multimodal approach for behavioral engagement detection of students,Nese Alyuz,2017,Learning,"LOGS,VIDEO,SCREEN,PPA","AFFECT,POSE,LOGS,PPA",CLS,HYBRID,MIE,International Workshop on Multimodal Interaction for Education,2,VIRT,STEM,IND,INSTR,K12,MB,,Caleb/Clayton,1&2,
518268671,using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game,María Ximena López,2021,Learning,"SURVEY,LOGS,AUDIO,VIDEO","LOGS,SURVEY,GAZE,PROS",STATS,OTH,ECGBL,European Conference on Games Based Learning,8,BLND,STEM,MULTI,INSTR,UNI,MF,"Overall, our study suggests that coupling style can be operationalised as a multimodal indicator suitable to investigate complex collaboration dynamics in games, thus confirming what has been found in other domains",Caleb,1,
518268671,using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game,María Ximena López,2021,Learning,"SURVEY,LOGS,AUDIO,VIDEO","LOGS,SURVEY,GAZE,PROS",STATS,OTH,ECGBL,European Conference on Games Based Learning,8,BLND,STEM,MULTI,INSTR,UNI,MF,"""Firstly, we found that coupling styles can be used to characterise collaboration in a co-located SG. Consistently with previous research (Isenberg et al, 2010; Niu et al, 2018), our results showed that teams displayed both close and loose coupling styles while performing individual actions to accomplish shared goals. Interestingly, we found a positive association between the time spent working closely coupled and the individual interactions with the technology. This suggests that the pursuit of collective goals requires players to continuously alternate and integrate individual planning and action with closely-coupled, likely to verify and synchronise their own actions with others. Secondly, we found that the perceived quality of collaboration does not appear to be an effective indicator of collaboration quality by itself. However, its small association with close-coupling style suggests a conscious, continuous, and proactive approach to collaboration, since players who appreciate the value of collaboration also seem to actively engage in closely-coupled interactions with others. Thirdly, our findings suggest that better-performing teams do work more closely-coupled and alternate their interactions with individual work. This result indicates that freely alternating individual work with closely-coupled interaction is an effective collaboration strategy, and that collaborative SGs should afford this opportunity. Overall, our study suggests that coupling style can be operationalised as a multimodal indicator suitable to investigate complex collaboration dynamics in games, thus confirming what has been found in other domains (Isenberg et al, 2010; Niu et al, 2018).""",Clayton,2,"Blended game environment for sustainable development (collaborative). Only stats methods applied, and no formal model presented or addressed via RQs."
518268671,using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game,María Ximena López,2021,Learning,"SURVEY,LOGS,AUDIO,VIDEO","LOGS,SURVEY,GAZE,PROS",STATS,OTH,ECGBL,European Conference on Games Based Learning,8,BLND,STEM,MULTI,INSTR,UNI,MF,,Caleb/Clayton,1&2,
957160695,virtual debate coach design: assessing multimodal argumentation performance,Volha Petukhova,2017,Training,"VIDEO,AUDIO","GEST,TRANS,PROS,SURVEY,GAZE","STATS,CLS,QUAL",MID,ICMI,International Conference on Multimodal Interaction,9,PHYS,HUM,MULTI,INF,K12,MF,"""We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain corpora and classification procedures resulted in the best performance on an argument structure mining task.""",Caleb,1,
957160695,virtual debate coach design: assessing multimodal argumentation performance,Volha Petukhova,2017,Training,"VIDEO,AUDIO","GEST,TRANS,PROS,SURVEY,GAZE","STATS,CLS,QUAL",MID,ICMI,International Conference on Multimodal Interaction,9,PHYS,HUM,MULTI,TRAIN,K12,MB,"""We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain corpora and classification procedures resulted in the best performance on an argument structure mining task. Results of the argument quality experiments showed that argument com- prehensibility is affected by the number of referring expressions, information complexity, and presentation fluency. Presence of intensification and segmentation markers, position and movements of hands/ams and certain postures may affect the perception of the clarity, persuasiveness, and credibility of debaters.""",Clayton,2,"Real-time and post hoc vitual debate coach. SVM model for classification. Could make argument for blended environment with virtual coach, but I would consider the environment to be in person, as that is where the students interact and debate."
957160695,virtual debate coach design: assessing multimodal argumentation performance,Volha Petukhova,2017,Training,"VIDEO,AUDIO","GEST,TRANS,PROS,SURVEY,GAZE","STATS,CLS,QUAL",MID,ICMI,International Conference on Multimodal Interaction,9,PHYS,HUM,MULTI,TRAIN,K12,MB,,Caleb/Clayton,1&2,
3009548670,real-time multimodal feedback with the cpr tutor,Daniele Di Mitri,2020,Training,"LOGS,VIDEO,SENSOR,MOTION","POSE,EMG,GEST",CLS,HYBRID,AIED,International Conference on Artificial Intelligence in Education,10,PHYS,PSY,IND,TRAIN,PROF,MB,System architecture is functional in predicting novice and expert compressions with low error rate,Caleb,1,
3009548670,real-time multimodal feedback with the cpr tutor,Daniele Di Mitri,2020,Training,"LOGS,VIDEO,SENSOR,MOTION","POSE,EMG,GEST",CLS,HYBRID,AIED,International Conference on Artificial Intelligence in Education,10,PHYS,PSY,IND,TRAIN,PROF,MB,"""...we collected observations that, while cannot be generalised, provide some indication that the feedback of the CPR tutor had a positive influence on the CPR performance on the target classes. To sum up, the architecture used for the CPR Tutor allowed for provision of real-time multimodal feedback (H1) and the generated feedback seem to have a short-term positive influence on the CPR performance on the target classes considered.""",Clayton,2,"CPR tutor. Model-based via RNN (LSTM) classification. Environment is ""European University Hospital."" No mention of undergraduats, and I got the impression this was professional development given the students needed to have recurring CPR qualifications."
3009548670,real-time multimodal feedback with the cpr tutor,Daniele Di Mitri,2020,Training,"LOGS,VIDEO,SENSOR,MOTION","POSE,EMG,GEST",CLS,HYBRID,AIED,International Conference on Artificial Intelligence in Education,10,PHYS,PSY,IND,TRAIN,PROF,MB,,Caleb/Clayton,1&2,
3448122334,"investigating the impact of a real-time, multimodal student engagement analytics technology in authentic classrooms",Sinem Aslan,2019,Learning,"VIDEO,LOGS,SCREEN,AUDIO,SURVEY,RPA,PPA,INTER","AFFECT,LOGS,POSE,QUAL,INTER,SURVEY,RPA","QUAL,STATS,CLS",LATE,CHI,Conference on Human Factors in Computing Systems,16,VIRT,STEM,IND,INSTR,K12,MB,Significant impact on the teacher's scaffolding behavior and student engagement (less bordem),Caleb,1,
3448122334,"investigating the impact of a real-time, multimodal student engagement analytics technology in authentic classrooms",Sinem Aslan,2019,Learning,"VIDEO,LOGS,SCREEN,AUDIO,SURVEY,RPA,PPA,INTER","AFFECT,LOGS,POSE,QUAL,INTER,SURVEY,RPA","QUAL,STATS,CLS",LATE,CHI,Conference on Human Factors in Computing Systems,16,VIRT,STEM,IND,INSTR,K12,MB,SEAT had positive impact on student engagement and was also helpful to teachers.,Clayton,2,"Real-time, multimodal Student Engagement Analytics Technology. Model-based via ML models (RF) referenced in previous works."
3448122334,"investigating the impact of a real-time, multimodal student engagement analytics technology in authentic classrooms",Sinem Aslan,2019,Learning,"VIDEO,LOGS,SCREEN,AUDIO,SURVEY,RPA,PPA,INTER","AFFECT,LOGS,POSE,QUAL,INTER,SURVEY,RPA","QUAL,STATS,CLS",LATE,CHI,Conference on Human Factors in Computing Systems,16,VIRT,STEM,IND,INSTR,K12,MB,,Caleb/Clayton,1&2,
2497456347,the rap system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors,Xavier Ochoa,2018,Training,"AUDIO,VIDEO,PPA,SURVEY","PPA,GAZE,POSE,PROS","CLS,STATS,QUAL",LATE,LAK,International Conference on Learning Analytics & Knowledge,17,BLND,HUM,IND,INF,UNI,MB,Feedback generated by RAP is similar to human feedback across several dimensions and similar to other more complex systems,Caleb,1,
2497456347,the rap system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors,Xavier Ochoa,2018,Training,"AUDIO,VIDEO,PPA,SURVEY","PPA,GAZE,POSE,PROS","CLS,STATS,QUAL",LATE,LAK,International Conference on Learning Analytics & Knowledge,17,BLND,HUM,IND,TRAIN,UNI,MB,"""It revealed an overwhelmingly positive perception of the system especially in the dimensions of perceived usefulness and feedback which were rated as excellent by 65% and 58% of the students respectively. The qualitative analysis helped discover specific issues, on the positive side, students commented on the potential of the system to quickly learn some basic presentation skills: ""I would like to see this system used in our Communications class"". On the negative side, students commented that they sometimes were aware that they were being recorded and that the environment was too small. Also, some students felt uncomfortable with a pre-recorded audience because it didn’t seem to react to their presentation: ""the audience had always the same expressions"". Overall, the students agreed that the system was useful and that they learned about their own presentation skills while using it.""",Clayton,2,RAP system evaluation. Blended because interactions with screen crowd and presentation but IRL presenting. Model-based due to AI/ML methods like RF.
2497456347,the rap system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors,Xavier Ochoa,2018,Training,"AUDIO,VIDEO,PPA,SURVEY","PPA,GAZE,POSE,PROS","CLS,STATS,QUAL",LATE,LAK,International Conference on Learning Analytics & Knowledge,17,BLND,HUM,IND,TRAIN,UNI,MB,,Caleb/Clayton,1&2,
3660066725,children's play and problem solving in motion-based educational games: synergies between human annotations and multi-modal data,Serena Lee-Cultura,2021,Learning,"VIDEO,SENSOR,EYE,LOGS","ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP","STATS,QUAL",OTH,IDC,Interaction Design and Children Conference,18,BLND,STEM,MULTI,INSTR,K12,"MB, MF",The use of MMD can help to triangulate with traditional research methods and explore hidden cognitive states,Caleb,1,
3660066725,children's play and problem solving in motion-based educational games: synergies between human annotations and multi-modal data,Serena Lee-Cultura,2021,Learning,"VIDEO,SENSOR,EYE,LOGS","ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP","STATS,QUAL",OTH,IDC,Interaction Design and Children Conference,18,BLND,STEM,IND,INSTR,K12,MF,"""During informed problem solving episodes, we observed that children’s physiological stress, cognitive load, and emotional regu- lation were the highest. When children are presented a problem to solve, they may feel under pressure (external or self-imposed [60]) to answer the question correctly.""

""During our study, children interacted with a MBEG; however, despite the intended “fun factor” that typically accompanies games, the pressure to academically perform (i.e., correctly match a card-box pair) may have elevated children’s stress levels [38, 87]. This may explain why children’s stress levels peaked during episodes of informed problem solving. In a similar vein, increased levels of cognitive load during informed problem solving may be directly linked to the mental effort that children expended as they reasoned through problems [88].""

""Lastly, emotional regulation relates to children’s HRV [9, 97]. A plausible reason for observing the highest levels of emotional regulation during informed problem solving might be due to the immediate feedback that children received directly after they attempted to make a card-box match. The anticipation of the MBEG assessment/evaluation may have influenced children’s heart rate, causing high levels of variability as children invested themselves in informed problem solving. Thus, in accordance with prior research [32], we hypothesise that the feedback in general, may have triggered cognitive and affective responses which affected learning, particularly during this ongoing tasks (i.e., a collection of questions asked in series).""

""Contrary to previous research [69, 84], our results did not indicate a connection between guessing behaviour and children’s lack of engagement during their interactions with the MBEG (Figure 6, top right). As such, we propose that during episodes of guessing, children experienced some degree of external and/or self-imposed pressures to determine answers correctly (as during informed problem solving).""",Clayton,2,"Marvy learns. Embodied...child must move physicall to get on-screen ""monster"" Marvy to place shapes in the right buckets. No AI/ML model (stats via ANOVA). Lots of references to ""groundwork"" that directs research, but no formal model."
3660066725,children's play and problem solving in motion-based educational games: synergies between human annotations and multi-modal data,Serena Lee-Cultura,2021,Learning,"VIDEO,SENSOR,EYE,LOGS","ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP","STATS,QUAL",OTH,IDC,Interaction Design and Children Conference,18,BLND,STEM,IND,INSTR,K12,MF,,Caleb/Clayton,1&2,
3783339081,a novel method for the in-depth multimodal analysis of student learning trajectories in intelligent tutoring systems,Ran Liu,2018,Learning,"LOGS,AUDIO,SCREEN,PPA","LOGS,TRANS,ACT,QUAL,PPA","STATS,REG,QUAL",MID,JLA,Journal of Learning Analytics,19,VIRT,STEM,IND,INSTR,K12,"MB, MF",Corse-grained learning trajectories were able to identify key moments during learning that warrented futher analysis with other MMD,Caleb,1,
3783339081,a novel method for the in-depth multimodal analysis of student learning trajectories in intelligent tutoring systems,Ran Liu,2018,Learning,"LOGS,AUDIO,SCREEN,PPA","LOGS,TRANS,ACT,QUAL,PPA","STATS,REG,QUAL",MID,JLA,Journal of Learning Analytics,19,VIRT,STEM,IND,INSTR,K12,MB,"""Our results from this analysis showed how students’ early experiences struggling with a novel concept could significantly affect both their entire learning trajectories within an activity and pre-test–post-test measurements of learning gains related to that concept. It provided evidence of an important “moment” for early instructional intervention.

The analysis we conducted on the Concentration knowledge component was an example of how a knowledge component-centred analysis can benefit from this multi-step approach as well. Our results led to a modification in the knowledge component assignment to problem steps within a ChemVLab+ activity as well as instructional implications for promoting better learning of a previously hidden conceptual difficulty.""",Clayton,2,Chemistry Virtual Lab. ChemVLab+ tutor. Model-based via regressing post-test scores and calculating probability individual student will get problem correct.
3783339081,a novel method for the in-depth multimodal analysis of student learning trajectories in intelligent tutoring systems,Ran Liu,2018,Learning,"LOGS,AUDIO,SCREEN,PPA","LOGS,TRANS,ACT,QUAL,PPA","STATS,REG,QUAL",MID,JLA,Journal of Learning Analytics,19,VIRT,STEM,IND,INSTR,K12,MB,,Caleb/Clayton,1&2,
3095923626,a multimodal analysis of making,Marcelo Worsley,2017,Learning,"VIDEO,AUDIO,SENSOR,PPA,INTER","GEST,PPA,EDA,ACT,PROS,QUAL,INTER","STATS,CLUST,QUAL,PATT",EARLY,IJAIED,International Journal of Artificial Intelligence in Education,20,PHYS,STEM,MULTI,INF,K12,MB,"""each approach provides different affordances depending on the similarity metric and the dependent variable.""
 ""The take-away is that there are several strategies for doing multimodal learning analytics, and that many of these approaches can provide a meaningful glimpse into a complex data set, glimpses that may be difficult to identify using traditional approaches.""",Caleb,1,
3095923626,a multimodal analysis of making,Marcelo Worsley,2017,Learning,"VIDEO,AUDIO,SENSOR,PPA,INTER","GEST,PPA,EDA,ACT,PROS,QUAL,INTER","STATS,CLUST,QUAL,PATT",EARLY,IJAIED,International Journal of Artificial Intelligence in Education,20,PHYS,STEM,MULTI,INSTR,"K12, UNI",MB,"""Looking across analyses, there are clear instances where each provided some novel insights. In this sense, the overall algorithm appears to have relevance for studying learning, success and experimental condition; but honing in on these correlations requires different modes of analysis.

As a whole this article has shown that success, learning and process are not equivalent, though they may occasionally overlap. Thus, when thinking about measuring the effectiveness of a given learning environment it is important to be clear about which metrics one hopes to optimize. At the same time, this article has provided additional evidence that experimental condition can have an impact on learning, success and process. Because of this, one has to be cognizant about how to develop learning and reasoning approaches that allow the environment to realize the desired outcomes.""",Clayton,2,"Dyads working with materials like paper on engineering task. High school and undergraduate students. No AI/ML supervised model, no formal theorhetical model. Does clustering count as model? YES"
3095923626,a multimodal analysis of making,Marcelo Worsley,2017,Learning,"VIDEO,AUDIO,SENSOR,PPA,INTER","GEST,PPA,EDA,ACT,PROS,QUAL,INTER","STATS,CLUST,QUAL,PATT",EARLY,IJAIED,International Journal of Artificial Intelligence in Education,20,PHYS,STEM,MULTI,INF,"K12, UNI",MB,,Caleb/Clayton,1&2,
85990093,multimodal markers of persuasive speech : designing a virtual debate coach,Volha Petukhova,2017,Training,"VIDEO,AUDIO","PROS,GEST","CLS,QUAL,STATS",MID,INTERSPEECH,INTERSPEECH Conference,21,PHYS,HUM,MULTI,TRAIN,K12,MF,"""Experiments of different types supported fairly reliable identification of markers from multimodal data, and linking these to assessments of debater confidence level and intensification behaviour.""",Caleb,1,
85990093,multimodal markers of persuasive speech : designing a virtual debate coach,Volha Petukhova,2017,Training,"VIDEO,AUDIO","PROS,GEST","CLS,QUAL,STATS",MID,INTERSPEECH,INTERSPEECH Conference,21,PHYS,HUM,MULTI,TRAIN,K12,MB,"""In line with previous empirical findings, we acknowledge that persuasive speech is rather difficult to characterize. Neverthe- less, based on theoretical and empirical frameworks set up by Grice (1975), Gussenhoven (2002) and Hirschberg (2002), we were able to define a set of criteria which help us to explain observed regularities and define rules, strategies and constraints for the generation, assessment and correction of trainees’ debate performance. Experiments of different types supported fairly reliable identification of markers from multimodal data, and linking these to assessments of debater confidence level and intensification behaviour.""",Clayton,2,Designing a virtual debate coach. Model-based via SVM classification and also three different theorhetical frameworks.
85990093,multimodal markers of persuasive speech : designing a virtual debate coach,Volha Petukhova,2017,Training,"VIDEO,AUDIO","PROS,GEST","CLS,QUAL,STATS",MID,INTERSPEECH,INTERSPEECH Conference,21,PHYS,HUM,MULTI,TRAIN,K12,MB,,Caleb/Clayton,1&2,
86191824,examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes,Shiyan Jiang,2019,Learning,"SCREEN,INTER,PPA,AUDIO","INTER,QUAL,TRANS",QUAL,OTH,ILE,Interactive Learning Environments,22,VIRT,STEM,MULTI,INSTR,K12,MB,"""This study contributes an initial understanding into how different modalities mediate students’ interactions and offers implications for scaffolding peer interactions during multimodal composing processes.""",Caleb,1,
86191824,examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes,Shiyan Jiang,2019,Learning,"SCREEN,INTER,PPA,AUDIO","INTER,QUAL,TRANS",QUAL,OTH,ILE,Interactive Learning Environments,22,PHYS,STEM,MULTI,INSTR,K12,MB,"""Overall, students were more likely to share ideas and ask questions, and they tended to use quick- response strategies while multimodal composing.""

""Students’ interview responses also suggested that providing short responses was a typical strategy during multimodal composing.""

""When examining interactions across sessions, the group was more engaged in discussions at the beginning and the end of the project while fewer interactions occurred during the middle of their composing process.""

""Giving commands occurred much less frequently than other interaction types (Figure 3).""

""Students discussed more often about comics that combined visuals and text than other modal elements.""

""Making learning visible in different modes was critical to foster peer interaction (Jahnke, Norqvist, &
Olsson, 2013).""

""Results showed that there were interactional differences based on different modes.""

""While comparing discussions on static visual modes, namely images and multimodal comics, we found that images involved more self-oriented and less group-oriented contributions.""

""Discussions on animations included more elaborated feedback.""

""Written narrative provided the least opportunity for group-oriented contributions.""",Clayton,2,"Focus on how different modes influence student interactions over
time during science multimodal composing. Provides model via theoretical framework."
86191824,examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes,Shiyan Jiang,2019,Learning,"SCREEN,INTER,PPA,AUDIO","INTER,QUAL,TRANS",QUAL,OTH,ILE,Interactive Learning Environments,22,VIRT,STEM,MULTI,INSTR,K12,MB,,Caleb/Clayton,1&2,
818492192,understanding student learning trajectories using multimodal learning analytics within an embodied-interaction learning environment,Alejandro Andrade,2017,Learning,"VIDEO,LOGS,INTER,PPA","GAZE,LOGS,INTER,PPA,GEST","CLUST,QUAL",HYBRID,LAK,International Conference on Learning Analytics & Knowledge,23,BLND,STEM,IND,INSTR,K12,MF,"""Using MMLA techniques, we were able to spot differences in students’ motion sequences while students interacted with our embodied simulation.""",Caleb,1,
818492192,understanding student learning trajectories using multimodal learning analytics within an embodied-interaction learning environment,Alejandro Andrade,2017,Learning,"VIDEO,LOGS,INTER,PPA","GAZE,LOGS,INTER,PPA,GEST","CLUST,QUAL",HYBRID,LAK,International Conference on Learning Analytics & Knowledge,23,BLND,STEM,IND,INSTR,K12,MB,"""Using MMLA techniques, we were able to spot differences in students’ motion sequences while students interacted with our embodied simulation.""",Clayton,2,"Embodied predator-prey ecosystem environment. No AI/ML or formal theoretical model. Does HMM count as model? YES

""The aim of this paper is to show how multimodal learning analytics (MMLA) can help understand how elementary students explore the concept of feedback loops while controlling an embodied simulation of a predator-prey ecosystem using hand movements as an interface with the computer simulation."""
818492192,understanding student learning trajectories using multimodal learning analytics within an embodied-interaction learning environment,Alejandro Andrade,2017,Learning,"VIDEO,LOGS,INTER,PPA","GAZE,LOGS,INTER,PPA,GEST","CLUST,QUAL",HYBRID,LAK,International Conference on Learning Analytics & Knowledge,23,BLND,STEM,IND,INSTR,K12,MB,,Caleb/Clayton,1&2,
147203129,multimodal learning analytics to inform learning design: lessons learned from computing education,Katerina Mangaroska,2020,Learning,"VIDEO,EYE,SENSOR,LOGS","LOGS,GAZE,EDA,PULSE,AFFECT,TEMP",CLS,HYBRID,JLA,Journal of Learning Analytics,24,VIRT,STEM,IND,INSTR,UNI,MB,"""The findings emphasize the need for learning analytics that are consequential for learning, rather than easy and convenient to collect.""",Caleb,1,
147203129,multimodal learning analytics to inform learning design: lessons learned from computing education,Katerina Mangaroska,2020,Learning,"VIDEO,EYE,SENSOR,LOGS","LOGS,GAZE,EDA,PULSE,AFFECT,TEMP",CLS,HYBRID,JLA,Journal of Learning Analytics,24,VIRT,STEM,IND,INSTR,UNI,MB,"""The models from M2 to M8 significantly outperformed M1 (see Figure 3), with M1 exhibiting a significantly lower adjusted
R2 value of 0.42 (std.dev. = 0.116).""

i.e., multimodal much better than logs alone for predicting debugging performance. Best model had every single modality.

""Using machine learning, we looked at the overall patterns in the data and performed feature importance among the 72 measures extracted from the multimodal data. Findings like ours combined with pedagogical intent from educators and theories from the LS, can advance the synergy between LA and LD by translating results in applicable design guidelines that can lead to improvements in the design of learning activities, instructional methods for teaching particular skills, and even the overall course (re)design. The complexity of the MMLA approach is congruous with learning theories because it can be used to understand how effectively students use the opportunities for learning as given in the LD. Such understanding promises to support versatile improvements in the LD in digital environments, from setting the right feedback loop (e.g., explaining misconceptions vs. challenging the student), to the design of personalized interventions, and modelling effective learning strategies considering skills and knowledge proficiency.""

""The findings emphasize the need for learning analytics that are consequential for learning, rather than easy and convenient to collect.""",Clayton,2,Java debugging env looking at just logs as a baseline then compared to multimodal. RF model-based.
147203129,multimodal learning analytics to inform learning design: lessons learned from computing education,Katerina Mangaroska,2020,Learning,"VIDEO,EYE,SENSOR,LOGS","LOGS,GAZE,EDA,PULSE,AFFECT,TEMP",CLS,HYBRID,JLA,Journal of Learning Analytics,24,VIRT,STEM,IND,INSTR,UNI,MB,,Caleb/Clayton,1&2,
123412197,utilizing multimodal data through fsqca to explain engagement in adaptive learning,Zacharoula Papamitsiou,2020,Learning,"SURVEY,LOGS,EYE,SENSOR,VIDEO","PULSE,AFFECT,EEG,GAZE,LOGS,BP,TEMP,EDA,SURVEY",PATT,HYBRID,TLT,Transactions on Learning Technologies,25,VIRT,STEM,IND,INF,UNI,MB,"The analysis revealed six configurations that explain learners’ high performance and three that explain learners’ medium/low performance, driven by engagement measures coming from the multimodal data.",Caleb,1,
123412197,utilizing multimodal data through fsqca to explain engagement in adaptive learning,Zacharoula Papamitsiou,2020,Learning,"SURVEY,LOGS,EYE,SENSOR,VIDEO","PULSE,AFFECT,EEG,GAZE,LOGS,BP,TEMP,EDA,SURVEY",PATT,HYBRID,TLT,Transactions on Learning Technologies,25,VIRT,STEM,IND,INSTR,UNI,MF,"""This study demonstrated a consolidated analysis of multimodal data collected during an adaptive self-assessment activity, utilizing fsQCA for deeper understanding engagement in this setting. What this study adds to engagement literature is that when the learning tasks facilitate one’s own learning needs (motivation), it is likely that one will be deeper and more substantially involved with those tasks, yet the thorough analysis showcased that multimodal data can provide more than one engagement patterns to facilitate this objective.""",Clayton,2,Fuzzy set qualitative comparative analysis (fsQCA) approach to shed light to learners’ engagement patterns. Model free because no formal theoretical model or AI/ML model.
123412197,utilizing multimodal data through fsqca to explain engagement in adaptive learning,Zacharoula Papamitsiou,2020,Learning,"SURVEY,LOGS,EYE,SENSOR,VIDEO","PULSE,AFFECT,EEG,GAZE,LOGS,BP,TEMP,EDA,SURVEY",PATT,HYBRID,TLT,Transactions on Learning Technologies,25,VIRT,STEM,IND,INF,UNI,MF,,Caleb/Clayton,1&2,
1118315889,using multimodal learning analytics to identify aspects of collaboration in project-based learning,Daniel Spikol,2017,Learning,"VIDEO,AUDIO,LOGS","POSE,PROS",REG,MID,CSCL,Conference on Computer Supported Collaborative Learning,26,PHYS,STEM,MULTI,INF,UNI,MF,"""physical aspect of collaborative is an important part of this type of learning and that learning analytics systems can identify features that are relevant for helping researchers, teachers, and learners unpack what is happening.""",Caleb,1,
1118315889,using multimodal learning analytics to identify aspects of collaboration in project-based learning,Daniel Spikol,2017,Learning,"VIDEO,AUDIO,LOGS","POSE,PROS",REG,MID,CSCL,Conference on Computer Supported Collaborative Learning,26,BLND,STEM,MULTI,INSTR,UNI,MB,"""In this research study, we presented that where the students are looking, the distance between them, the motion of their hands our key features for a learning analytics system to be effectively used to identify collaboration in small groups of Engineering students.""",Clayton,2,Determine best MMLA features for CPS. Blened Arduino+IDE. Model-based (regression).
1118315889,using multimodal learning analytics to identify aspects of collaboration in project-based learning,Daniel Spikol,2017,Learning,"VIDEO,AUDIO,LOGS","POSE,PROS",REG,MID,CSCL,Conference on Computer Supported Collaborative Learning,26,PHYS,STEM,MULTI,INSTR,UNI,MB,,Caleb/Clayton,1&2,
1315379489,multimodal engagement analysis from facial videos in the classroom,Ömer Sümer,2021,Learning,VIDEO,"POSE,AFFECT",CLS,"EARLY,LATE",TAC,Transactions on Affective Computing,27,PHYS,"HUM, STEM",IND,INSTR,K12,MB,"""The best performing engagement classifiers achieved AUCs of .620 and .720 in Grades 8 and 12, respectively. We further investigated fusion strategies and found score-level fusion either improves the engagement classifiers or is on par with the best performing modality. We also investigated the effect of personalization and found that using only 60-seconds of person-specific data selected by margin uncertainty of the base classifier yielded an average AUC improvement of .084.""",Caleb,1,
1315379489,multimodal engagement analysis from facial videos in the classroom,Ömer Sümer,2021,Learning,VIDEO,"POSE,AFFECT",CLS,"EARLY,LATE",TAC,Transactions on Affective Computing,27,PHYS,"HUM, STEM",MULTI,INSTR,K12,MB,"""In contrast to the previous works that used mainly handcrafted local (i.e., local binary patterns, Gabor filters) and precomputed features such as head pose or estimated facial action units, we showed that engagement as a 3-class classification problem can be predicted in the classroom. We gathered a large-scale classroom observation dataset and collected the observer ratings of student engagement for Grades 8 and 12 (N=15). In contrast to the limited training and testing protocols in the literature, our study is the first to validate the use of automated engagement analysis in the classroom.

Our work proves that even a small amount of person-specific data could considerably enhance the performance of engagement classifiers. In comparison to the person-independent settings of many machine learning and computer vision tasks, personalization in engagement analysis significantly impacts performance. We find this to be the case because of personal differences in visible behaviors during levels of low and high engagement. Furthermore, engagement can even reveal variation in time (for instance, the indicators of engagement are not the same in different classes, i.e., math and history).""",Clayton,2,"AV facial detection. Model-based: RF, SVM, MLP, LSTM. Classroon setting (instructional, multi-person, physical). Many subjects stemming across STEM and humanities."
1315379489,multimodal engagement analysis from facial videos in the classroom,Ömer Sümer,2021,Learning,VIDEO,"POSE,AFFECT",CLS,"EARLY,LATE",TAC,Transactions on Affective Computing,27,PHYS,"HUM, STEM",MULTI,INSTR,K12,MB,,Caleb/Clayton,1&2,
1374035721,attentivelearner2: a multimodal approach for improving mooc learning on mobile devices,Phuong Pham,2017,Learning,"VIDEO,SURVEY","PULSE,AFFECT,SURVEY",CLS,MID,AIED,International Conference on Artificial Intelligence in Education,28,VIRT,STEM,IND,INSTR,UNSP,MB,"""In a 26-participant user study, we found that it is feasible to detect 6 types of emotion during learning via collected PPG signals and facial expressions and these modalities are complement with each other.""",Caleb,1,
1374035721,attentivelearner2: a multimodal approach for improving mooc learning on mobile devices,Phuong Pham,2017,Learning,"VIDEO,SURVEY","PULSE,AFFECT,SURVEY",CLS,MID,AIED,International Conference on Artificial Intelligence in Education,28,VIRT,STEM,IND,INSTR,UNSP,MB,"""In a 26-participant user study, we found that it is feasible to detect 6 types of emotion during learning via collected PPG signals and facial expressions and these modalities are complement with each other.""

""AttentiveLearner2 achieved high performance as all our models outperformed the baseline. Moreover, we found PPG signals and facial expressions are complement each other. If FEA features can win in 3 emotions (Confusion, Happiness, and Self-efficacy), PPG features are the best solution for Curiosity, and feature fusion can improve detection performance for Boredom and Frustration.""

""In a 26-participant user study, we found that by taking advantages from two modalities, AttentiveLearner2 achieved higher detection accuracy than models using only one modality across 6 different emotions. More importantly, these results were achieved on unmodified smartphones which supports the scalable deployment of AttentiveLearner2""",Clayton,2,"AttentiveLearner2 implicitly infers learners’ affective and cognitive states during learning by analyzing learners’ PPG signals and facial expressions to improve mobile MOOC learning. 

Model-based via multimodal emotion detection, but it's a short paper and the exact classification model is not provided."
1374035721,attentivelearner2: a multimodal approach for improving mooc learning on mobile devices,Phuong Pham,2017,Learning,"VIDEO,SURVEY","PULSE,AFFECT,SURVEY",CLS,MID,AIED,International Conference on Artificial Intelligence in Education,28,VIRT,STEM,IND,INSTR,UNSP,MB,,Caleb/Clayton,1&2,
1426267857,"affect, support, and personal factors: multimodal causal models of one-on-one coaching",Lujie Karen Chen,2021,Learning,"AUDIO,VIDEO,SURVEY","PROS,GAZE,TRANS,AFFECT,SURVEY","STATS,NET",HYBRID,JEDM,Journal of Educational Data Mining,29,PHYS,STEM,IND,INSTR,K12,MB,"""this research contributes to the relatively sparse literature in multimodal learning analytics by providing a balanced view of the teacher and student interactions with a data set collected in naturalistic home environments.""",Caleb,1,
1426267857,"affect, support, and personal factors: multimodal causal models of one-on-one coaching",Lujie Karen Chen,2021,Learning,"AUDIO,VIDEO,SURVEY","PROS,GAZE,TRANS,AFFECT,SURVEY","STATS,NET",HYBRID,JEDM,Journal of Educational Data Mining,29,PHYS,STEM,IND,INSTR,K12,MB,"""Firstly, we note a clear causal pathway between the group of variables describing parents’ support and another group representing the child’s cognitive-affective experience.""

""Secondly, we note the causal pathway from Profile to Affect and, indirectly, to Support.""",Clayton,2,"Model-based via presented framework.

""We explore an analytical framework that is explainable and amenable to incorporating domain knowledge. The proposed framework combines statistical approaches in Sparse Multiple Canonical Correlation, causal discovery, and inference methods for observations. We demonstrate this framework using a multimodal one-on-one math problem-solving coaching dataset collected in naturalistic home environments involving parents and young children."""
1426267857,"affect, support, and personal factors: multimodal causal models of one-on-one coaching",Lujie Karen Chen,2021,Learning,"AUDIO,VIDEO,SURVEY","PROS,GAZE,TRANS,AFFECT,SURVEY","STATS,NET",HYBRID,JEDM,Journal of Educational Data Mining,29,PHYS,STEM,IND,INSTR,K12,MB,,Caleb/Clayton,1&2,
2055153191,round or rectangular tables for collaborative problem solving? a multimodal learning analytics study,Milica Vujovic,2020,Learning,"VIDEO,MOTION","POSE,GEST,ACT","STATS,QUAL",OTH,BJET,British Journal of Educational Technology,30,BLND,"HUM, STEM",MULTI,INSTR,K12,MF,"""The statistical analysis has shown significant differences between the levels of independent variables related to table shape and how the effect differs between two different levels of education, and this was further supported by a qualitative analysis of the observations obtained from the video recording of the activities.""",Clayton,1,"2 environments: blended arduino and blended pen/paper. 
STEM for CS, humanities for design."
2055153191,round or rectangular tables for collaborative problem solving? a multimodal learning analytics study,Milica Vujovic,2020,Learning,"VIDEO,MOTION","POSE,GEST,ACT","STATS,QUAL",OTH,BJET,British Journal of Educational Technology,30,BLND,"HUM, STEM",MULTI,INSTR,K12,MF,"""Results show that the use of round tables (vs rectangular tables) leads to higher levels of on-task participation in the case of elementary school students. For university students, different table shapes seem to have a limited impact on their levels of participation in collaborative problem solving.""",Caleb,2,
2055153191,round or rectangular tables for collaborative problem solving? a multimodal learning analytics study,Milica Vujovic,2020,Learning,"VIDEO,MOTION","POSE,GEST,ACT","STATS,QUAL",OTH,BJET,British Journal of Educational Technology,30,BLND,"HUM, STEM",MULTI,INSTR,K12,MF,,Clayton/Caleb,1&2,
2273914836,many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities,Jauwairia Nasir,2022,Learning,"VIDEO,AUDIO,LOGS,PPA,SURVEY","PROS,AFFECT,GAZE,TRANS,LOGS","STATS,QUAL,CLUST,CLS",HYBRID,IJCSCL,International Journal of Computer-Supported Collaborative Learning,31,BLND,STEM,MULTI,INSTR,K12,"MB, MF","""Our combined multi-modal learning analytics and interaction analysis methodology enabled us to identify two multi-modal profles of learners who have learning gains and one multi-modal profle of learners who do not have learning gains.""

""Using this approach, we are able to build the multimodal behavioral profles for each group of learners.""",Clayton,1,"blended: environment is the screen, but outside env is physical robot agent IRL.
model-based (SVM, RF) and model-free because classification and clustering used, along with stats.
subject is STEM because goal of env is to teach about minimum-spanning-tree."
2273914836,many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities,Jauwairia Nasir,2022,Learning,"VIDEO,AUDIO,LOGS,PPA,SURVEY","PROS,AFFECT,GAZE,TRANS,LOGS","STATS,QUAL,CLUST,CLS",HYBRID,IJCSCL,International Journal of Computer-Supported Collaborative Learning,31,BLND,STEM,MULTI,INSTR,K12,MB,"""Our combined multi-modal learning analytics and interaction analysis methodology enabled us to identify two multi-modal profles of learners who have learning gains and one multi-modal profle of learners who do not have learning gains.""",Caleb,2,
2273914836,many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities,Jauwairia Nasir,2022,Learning,"VIDEO,AUDIO,LOGS,PPA,SURVEY","PROS,AFFECT,GAZE,TRANS,LOGS","STATS,QUAL,CLUST,CLS",HYBRID,IJCSCL,International Journal of Computer-Supported Collaborative Learning,31,BLND,STEM,MULTI,INSTR,K12,MB,,Clayton/Caleb,1&2,
1763513559,keep me in the loop: real-time feedback with multimodal data,Daniele Di Mitri,2021,Training,"SURVEY,LOGS,VIDEO,SENSOR,MOTION","POSE,EMG,GEST,SURVEY","CLS,QUAL,STATS",HYBRID,IJAIED,International Journal of Artificial Intelligence in Education,32,PHYS,PSY,IND,TRAIN,UNSP,MB,"""the architecture used for the CPR Tutor allowed for the provision of real-time multimodal feedback, and the generated feedback seemed to have a short-term positive influence on the considered CPR performance indicators.""

""The CPR Tutor automatically recognises and assesses the quality of the chest compressions according to five CPR performance
indicators. It detects training mistakes in real-time by analysing a multimodal data stream consisting of kinematic and electromyographic data. Based on this assessment, the CPR Tutor provides audio feedback to correct the most critical mistakes and improve the CPR performance.""",Clayton,1,CPR tutor. LSTM model.
1763513559,keep me in the loop: real-time feedback with multimodal data,Daniele Di Mitri,2021,Training,"SURVEY,LOGS,VIDEO,SENSOR,MOTION","POSE,EMG,GEST,SURVEY","CLS,QUAL,STATS",HYBRID,IJAIED,International Journal of Artificial Intelligence in Education,32,PHYS,PSY,IND,TRAIN,UNI,MB,"""we collected findings that, while they cannot be generalised, indicate that the feedback of the CPR tutor had a short-term positive influence on the CPR performance in the target classes.""",Caleb,2,
1763513559,keep me in the loop: real-time feedback with multimodal data,Daniele Di Mitri,2021,Training,"SURVEY,LOGS,VIDEO,SENSOR,MOTION","POSE,EMG,GEST,SURVEY","CLS,QUAL,STATS",HYBRID,IJAIED,International Journal of Artificial Intelligence in Education,32,PHYS,PSY,IND,TRAIN,PROF,MB,,Clayton/Caleb,1&2,
1345598079,intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning,Sofia Tancredi,2022,Learning,"EYE,VIDEO,AUDIO,INTER","GAZE,GEST,TRANS,POSE,INTER","PATT,QUAL,STATS",OTH,MMLA Handbook,The Multimodal Learning Analytics Handbook,33,VIRT,STEM,IND,INSTR,K12,MF,"""Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Exploration, increasing confluence during Discovery, and high confluence during Fluency.""

""Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Exploration, increasing confluence during Discovery, and high confluence during Fluency. Towards the end of the Discovery stage, a coordination of coordinations (Piaget, 1970) developed wherein the coordination between the left- and right hands became coordinated with newly developed gaze structures spanning different screen locations.""",Clayton,1,Physcal interaction with tablet to change ratios of two bars on screen.
1345598079,intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning,Sofia Tancredi,2022,Learning,"EYE,VIDEO,AUDIO,INTER","GAZE,GEST,TRANS,POSE,INTER","PATT,QUAL,STATS",OTH,MMLA Handbook,The Multimodal Learning Analytics Handbook,33,VIRT,STEM,IND,INSTR,K12,MF,"""Our findings point to the importance of MMLA work that attunes to intermodal dynamics of learning, both as a pragmatic resource for identifying key moments in learning and as a resource for refining theoretical understandings of learning processes.""",Caleb,2,
1345598079,intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning,Sofia Tancredi,2022,Learning,"EYE,VIDEO,AUDIO,INTER","GAZE,GEST,TRANS,POSE,INTER","PATT,QUAL,STATS",OTH,MMLA Handbook,The Multimodal Learning Analytics Handbook,33,VIRT,STEM,IND,INSTR,K12,MF,,Clayton/Caleb,1&2,
3135645357,multimodal teaching analytics: automated extraction of orchestration graphs from wearable sensor data,Luis P. Prieto,2018,Learning,"EYE,VIDEO,AUDIO,MOTION","GAZE,PROS,ACT,PIXEL","NET,CLS,STATS,PATT,QUAL",HYBRID,JCAL,Journal of Computer Assisted Learning,34,BLND,STEM,MULTI,INSTR,K12,MB,"""In summary, the results from our evaluation of personalized and general models to automatically extract orchestration graphs highlight the fact that machine learning models can be successfully trained with such multimodal sensor data, using relatively low-level features.""",Clayton,1,Orchestration graphs. Multi-person because classroom-based and therefore multiple students.
3135645357,multimodal teaching analytics: automated extraction of orchestration graphs from wearable sensor data,Luis P. Prieto,2018,Learning,"EYE,VIDEO,AUDIO,MOTION","GAZE,PROS,ACT,PIXEL","NET,CLS,STATS,PATT,QUAL",HYBRID,JCAL,Journal of Computer Assisted Learning,34,PHYS,STEM,MULTI,INSTR,PROF,MB,"""In summary, the results from our evaluation of personalized and general models to automatically extract orchestration graphs highlight the fact that machine learning models can be successfully trained with such multimodal sensor data, using relatively low-level features.""",Caleb,2,
3135645357,multimodal teaching analytics: automated extraction of orchestration graphs from wearable sensor data,Luis P. Prieto,2018,Learning,"EYE,VIDEO,AUDIO,MOTION","GAZE,PROS,ACT,PIXEL","NET,CLS,STATS,PATT,QUAL",HYBRID,JCAL,Journal of Computer Assisted Learning,34,PHYS,STEM,MULTI,INSTR,PROF,MB,,Clayton/Caleb,1&2,
3856280479,children's play and problem-solving in motion-based learning technologies using a multi-modal mixed methods approach,Serena Lee-Cultura,2021,Learning,"VIDEO,SENSOR,EYE,LOGS","ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP,BP","STATS,QUAL,CLS",HYBRID,IJCCI,International Journal of Child-Computer Interaction,35,BLND,STEM,IND,INSTR,K12,"MB, MF","""Our work exemplifies how the confluence of MMD and video coding can go further than data triangulation, and contribute to a holistic understanding of children’s play and problem-solving behaviours during their interactions with MBLT, by enabling researchers and designers the capacity to cater to children’s cognitive, affective and physiological processes to support learning through use of MBLT. To the best of our knowledge, there are no previous studies that use MMD from wearable and ubiquitous sensors (e.g., eye tracking glasses, wristbands and skeletal tracking) to investigate children’s behaviours in this context.""",Clayton,1,Marvy learns (geometry). RF for classification (model-based). Exploratory Factor Analysis (EFA) for statistical analysis (model-free)
3856280479,children's play and problem-solving in motion-based learning technologies using a multi-modal mixed methods approach,Serena Lee-Cultura,2021,Learning,"VIDEO,SENSOR,EYE,LOGS","ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP,BP","STATS,QUAL,CLS",HYBRID,IJCCI,International Journal of Child-Computer Interaction,35,BLND,STEM,IND,INSTR,K12,MB,"""Our work exemplifies how the confluence of MMD and video coding can go further than data triangulation, and contribute to a holistic understanding of children’s play and problem-solving behaviours during their interactions with MBLT, by enabling researchers and designers the capacity to cater to children’s cognitive, affective and physiological processes to support learning through use of MBLT.""",Caleb,2,
3856280479,children's play and problem-solving in motion-based learning technologies using a multi-modal mixed methods approach,Serena Lee-Cultura,2021,Learning,"VIDEO,SENSOR,EYE,LOGS","ACT,GAZE,EDA,PULSE,AFFECT,FATIG,LOGS,QUAL,TEMP,BP","STATS,QUAL,CLS",HYBRID,IJCCI,International Journal of Child-Computer Interaction,35,BLND,STEM,IND,INSTR,K12,"MB, MF",,Clayton/Caleb,1&2,
2609260641,visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication,René Noël,2022,Learning,"AUDIO,VIDEO,RPA,INTER","PROS,POSE,RPA,INTER,QUAL",QUAL,OTH,DAMLE,"Applied Sciences, Special Issue ""Data Analytics and Machine Learning in Education""",36,PHYS,OTH,MULTI,INF,PROF,MF,"""We conducted a case study to compare the visualizations provided by the system in two different situations: collaborative and competitive activities. The results suggest that the provided visualizations help to identify issues on cognitive contribution, assimilation, self-regulation, and integration of the team members. They could also support teachers to decide whether they must assist a team in fostering collaboration.

While the results are naturally constrained to the characteristics of the activities in which we tested the platform, they provide initial evidence about the technical fea-
sibility of extracting behavioral indicators and traces using MMLA to give insights onteam collaboration.""",Clayton,1,"2 env tasks: collaboratively write a sentence about what might be the first article of Chile's new constitution, and decide who should be saved in a bunker in an apocalyptic scenario"
2609260641,visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication,René Noël,2022,Learning,"AUDIO,VIDEO,RPA,INTER","PROS,POSE,RPA,INTER,QUAL",QUAL,OTH,DAMLE,"Applied Sciences, Special Issue ""Data Analytics and Machine Learning in Education""",36,PHYS,HUM,MULTI,INF,UNI,MF,"""The results suggest that the provided visualizations help to identify issues on cognitive contribution, assimilation, self-regulation, and integration of the team members. They could also support teachers to decide whether they must assist a team in fostering collaboration.""",Caleb,2,
2609260641,visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication,René Noël,2022,Learning,"AUDIO,VIDEO,RPA,INTER","PROS,POSE,RPA,INTER,QUAL",QUAL,OTH,DAMLE,"Applied Sciences, Special Issue ""Data Analytics and Machine Learning in Education""",36,PHYS,HUM,MULTI,INF,"PROF, UNI",MF,,Clayton/Caleb,1&2,
666050348,multicraft: a multimodal interface for supporting and studying learning in minecraft,Marcelo Worsley,2021,Learning,"AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS","PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS",QUAL,OTH,HCII,International Conference on Human-Computer Interaction,37,VIRT,PSY,IND,INF,K12,MF,"""Through our user studies, we found that the platform helps fulfill some of those goals by providing capabilities that can spur on amazement and excitement among traditional Minecraft users and novices. We also find that many of the multimodal components, while not immediately intuitive for users, proved to be preferred modes of game play. In this sense, we feel that this tool is moving in the right direction in terms of the system capabilities that it provides. Our analyses also point to the meaningful ways that multimodal data can be used to study student learning in these game-based environments, and free students from standardized testing and learning experiences.""",Clayton,1,Multicraft: multimodal Minecraft.
666050348,multicraft: a multimodal interface for supporting and studying learning in minecraft,Marcelo Worsley,2021,Learning,"AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS","PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS",QUAL,OTH,HCII,International Conference on Human-Computer Interaction,37,VIRT,STEM,IND,INF,K12,MF,"""Through our user studies, we found that the platform helps fulfill some of those goals by providing capabilities that can spur on amazement and excitement among traditional Minecraft users and novices. We also find that many of the multimodal components, while not immediately intuitive for users, proved to be preferred modes of game play.""",Caleb,2,
666050348,multicraft: a multimodal interface for supporting and studying learning in minecraft,Marcelo Worsley,2021,Learning,"AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS","PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS",QUAL,OTH,HCII,International Conference on Human-Computer Interaction,37,VIRT,STEM,IND,INF,K12,MF,,Clayton/Caleb,1&2,
1637690235,supervised machine learning in multimodal learning analytics for estimating success in project-based learning,Daniel Spikol,2018,Learning,"VIDEO,AUDIO,LOGS,PPA,RPA","POSE,GEST,PROS,LOGS,PPA,RPA","REG,CLS",MID,JCAL,Journal of Computer Assisted Learning,38,BLND,STEM,MULTI,INSTR,UNI,MB,"""In this paper, we show that MMLA and the state-of-the-art computational techniques can be used to generate insights into the ""black box"" of learning in students’ project-based activities. These insights generated from multimodal data can be used to inform teachers about the key features of project-based learning and help them support students appropriately in similar pedagogical approaches.""",Clayton,1,"Arduino + browser IDE, so blended engineering. DNN + regression. Also tested SVM, NB, LR."
1637690235,supervised machine learning in multimodal learning analytics for estimating success in project-based learning,Daniel Spikol,2018,Learning,"VIDEO,AUDIO,LOGS,PPA,RPA","POSE,GEST,PROS,LOGS,PPA,RPA","REG,CLS",MID,JCAL,Journal of Computer Assisted Learning,38,BLND,STEM,MULTI,INSTR,UNI,MB,"""In this paper, we show that MMLA and the state-of-the-art computational techniques can be used to generate insights into the ""black box"" of learning in students’ project-based activities. These insights generated from multimodal data can be used to inform teachers about the key features of project-based learning and help them support students appropriately in similar pedagogical approaches. Towards achieving this ultimate aim, this paper has three main contributions to the field. First, we show that the distances between students’ hands and faces while they are working on projects is a strong predictor of students’ artefact quality which indicates the value of student collaboration in these pedagogical approaches. Second, we show that both, new and promising approaches such as neural networks and more traditional regression approaches, can be used to classify MMLA data and both have advantages and disadvantages depending on the research questions and contexts being investigated. At last but not least, although, it is traditionally notoriously challenging to provide evidence about the robust and objective evaluations of project-based learning activities, techniques and types of data we presented here can be the first step towards effective implementation and evaluation of project-based learning at a scale.""",Caleb,2,
1637690235,supervised machine learning in multimodal learning analytics for estimating success in project-based learning,Daniel Spikol,2018,Learning,"VIDEO,AUDIO,LOGS,PPA,RPA","POSE,GEST,PROS,LOGS,PPA,RPA","REG,CLS",MID,JCAL,Journal of Computer Assisted Learning,38,BLND,STEM,MULTI,INSTR,UNI,MB,,Clayton/Caleb,1&2,
2155422499,a multimodal analysis of pair work engagement episodes: implications for emi lecturer training,Teresa Morell,2022,Training,"VIDEO,AUDIO,PPA","TRANS,PPA,QUAL,POSE,ACT",QUAL,OTH,JEAP,Journal of English for Academic Purposes,39,PHYS,OTH,MULTI,TRAIN,PROF,MF,"""In this study, we have given special emphasis to the interpersonal metafunction (see Fig. 1), which is crucial to enhance EMI students’ active participation and engagement. The outcomes of this study point to the need to address multimodal interactional competence in professional development programs, as described in Morell et al. (2022) to promote effective lecturing in the diverse EMI scenarios.""",Clayton,1,Workshop for professional development for professors teaching in English (in non-English speaking country)
2155422499,a multimodal analysis of pair work engagement episodes: implications for emi lecturer training,Teresa Morell,2022,Training,"VIDEO,AUDIO,PPA","TRANS,PPA,QUAL,POSE,ACT",QUAL,OTH,JEAP,Journal of English for Academic Purposes,39,PHYS,OTH,MULTI,TRAIN,PROF,MF,"""The exploration of how EMI lecturers use semiotic resources to construct meaning and to create engagement paves the way to a unified multimodal interactional competence. In general, the mastery of this competence enables lecturers to convert students from passive listeners/observers to active participants, giving them opportunities to engage in active learning, language usage and critical thinking.""",Caleb,2,
2155422499,a multimodal analysis of pair work engagement episodes: implications for emi lecturer training,Teresa Morell,2022,Training,"VIDEO,AUDIO,PPA","TRANS,PPA,QUAL,POSE,ACT",QUAL,OTH,JEAP,Journal of English for Academic Purposes,39,PHYS,OTH,MULTI,TRAIN,PROF,MF,,Clayton/Caleb,1&2,
3309250332,(dis)engagement matters: identifying efficacious learning practices with multimodal learning analytics,Marcelo Worsley,2018,Learning,"VIDEO,PPA","GEST,QUAL,PPA","CLUST,CLS,QUAL",HYBRID,LAK,International Conference on Learning Analytics & Knowledge,40,PHYS,STEM,MULTI,INSTR,UNI,"MB, MF","""I was able to use multimodal data and machine learning to develop
plausible arguments for students’ differential learning gains.""",Clayton,1,"2 tasks: use sheet of paper to hold text book, and use other materials to support a 0.5lb mass

Model-based w/ DT classifier, model-free with k-means clustering."
3309250332,(dis)engagement matters: identifying efficacious learning practices with multimodal learning analytics,Marcelo Worsley,2018,Learning,"VIDEO,PPA","GEST,QUAL,PPA","CLUST,CLS,QUAL",HYBRID,LAK,International Conference on Learning Analytics & Knowledge,40,PHYS,STEM,MULTI,INSTR,UNI,MB,"""I was able to draw inferences related to productive engagement and disengagement in the context of collaborative problem solving. These inferences would have been difficult to articulate using only machine learning, and hard to identify using only human coding. Hence, I argue that an intermediate model that leverages the affordances of multimodal data and computation, but leaves inference development to trained scholars could offer a viable alternative to purely qualitative or machine learning approaches.""",Caleb,2,
3309250332,(dis)engagement matters: identifying efficacious learning practices with multimodal learning analytics,Marcelo Worsley,2018,Learning,"VIDEO,PPA","GEST,QUAL,PPA","CLUST,CLS,QUAL",HYBRID,LAK,International Conference on Learning Analytics & Knowledge,40,PHYS,STEM,MULTI,INSTR,UNI,MB,,Clayton/Caleb,1&2,
3625722965,table tennis tutor: forehand strokes classification based on multimodal data and neural networks,Khaleel Asyraaf Mat Sanusi,2021,Training,"VIDEO,MOTION,INTER","POSE,GEST,ACT,INTER","CLS,QUAL",HYBRID,Sensors,MDPI Sensors,41,PHYS,PSY,MULTI,TRAIN,UNI,MB,"""The precision (73%) and recall (61%) for the combined devices (Smartphone + Kinect) achieved the best results compared to the other two classes.""",Clayton,1,"Table Tennis Tutor, LSTM classification."
3625722965,table tennis tutor: forehand strokes classification based on multimodal data and neural networks,Khaleel Asyraaf Mat Sanusi,2021,Training,"VIDEO,MOTION,INTER","POSE,GEST,ACT,INTER","CLS,QUAL",HYBRID,Sensors,MDPI Sensors,41,PHYS,PSY,"IND, MULTI",TRAIN,"UNI, UNSP",MB,"""We observed, within our context, that smartphone sensors by themselves are unable to perform better than the Kinect. In addition, it is likely that the smartphone sensors are able to classify the strokes by complete chance due to 51% of accuracy. However,the performance improves when both of the devices are combined.""",Caleb,2,
3625722965,table tennis tutor: forehand strokes classification based on multimodal data and neural networks,Khaleel Asyraaf Mat Sanusi,2021,Training,"VIDEO,MOTION,INTER","POSE,GEST,ACT,INTER","CLS,QUAL",HYBRID,Sensors,MDPI Sensors,41,PHYS,PSY,IND,TRAIN,UNI,MB,,Clayton/Caleb,1&2,
4278392816,multimodal data as a means to understand the learning experience,Michail Giannakos,2019,Training,"LOGS,EYE,SENSOR,VIDEO","EEG,GAZE,LOGS,PULSE,EDA,TEMP,BP,POSE","REG,STATS",HYBRID,IJIM,International Journal of Information Management,42,VIRT,PSY,IND,INF,UNI,"MB, MF","""Overall, our work shows that capturing multimodal data can help us increase the prediction accuracy of users’ learning performance in learner–computer interaction (LCI). In addition, the study shows that the most commonly used data-stream (i.e., keystrokes) is the poorest proxy of our learning performance. Thus, leveraging advances in contemporary learning environments and physiological sensing (wearable, EEG etc.), we provide evidence that multimodal data can be a viable method to accurately track users’ states during learning, thereby providing unique possibilities of closing the loop between the learning technology and the learner. Therefore, the incorporation of multimodal data enables HCI and learning technology researchers to examine unscripted, complex tasks in more holistic and accurate ways.""

Identified specific multimodal features that were best predictors of performance.",Clayton,1,"Pac-Man. Model-based for regression, model-free for statistical methods.

Does training always map to training or is Pacman both a training environment that is informal, for instance?"
4278392816,multimodal data as a means to understand the learning experience,Michail Giannakos,2019,Training,"LOGS,EYE,SENSOR,VIDEO","EEG,GAZE,LOGS,PULSE,EDA,TEMP,BP,POSE","REG,STATS",HYBRID,IJIM,International Journal of Information Management,42,VIRT,PSY,IND,INF,UNI,MB,"""Overall, our work shows that capturing multimodal data can help us increase the prediction accuracy of users’ learning performance in learner–computer interaction (LCI). In addition, the study shows that the most commonly used data-stream (i.e., keystrokes) is the poorest proxy of our learning performance. Thus, leveraging advances in contemporary learning environments and physiological sensing (wearable, EEG etc.), we provide evidence that multimodal data can be a viable method to accurately track users’ states during learning, thereby providing unique possibilities of closing the loop between the learning technology and the learner. Therefore, the incorporation of multimodal data enables HCI and learning technology researchers to examine unscripted, complex tasks in more holistic and accurate ways.""",Caleb,2,
4278392816,multimodal data as a means to understand the learning experience,Michail Giannakos,2019,Training,"LOGS,EYE,SENSOR,VIDEO","EEG,GAZE,LOGS,PULSE,EDA,TEMP,BP,POSE","REG,STATS",HYBRID,IJIM,International Journal of Information Management,42,VIRT,PSY,IND,INF,UNI,MB,,Clayton/Caleb,1&2,
566043228,automatic student engagement in online learning environment based on neural turing machine,Xiaoyang Ma,2021,Learning,VIDEO,"POSE,GAZE",CLS,MID,IJIET,International Journal of Information and Education Technology,43,VIRT,UNSP,IND,INSTR,UNI,MB,"""Through comparison we can find that our proposed model is superior to the most commonly used model of feature fusion in the past.""",Clayton,1,"Engagement detection, DAiSEE dataset. Neural Turing Machine (NTM) RNN."
566043228,automatic student engagement in online learning environment based on neural turing machine,Xiaoyang Ma,2021,Learning,VIDEO,"POSE,GAZE",CLS,MID,IJIET,International Journal of Information and Education Technology,43,VIRT,UNSP,IND,INSTR,UNSP,MB,"""From the perspective of multiple features, we get the fusion of multiple short video features by using two fully connected layers. It can be found from Table I that our method can more accurately predict the student's learning participation than the traditional method of weighted summation and averaging.""",Caleb,2,
566043228,automatic student engagement in online learning environment based on neural turing machine,Xiaoyang Ma,2021,Learning,VIDEO,"POSE,GAZE",CLS,MID,IJIET,International Journal of Information and Education Technology,43,VIRT,UNSP,IND,INSTR,UNSP,MB,,Clayton/Caleb,1&2,
853680639,sensor-based data fusion for multimodal affect detection in game-based learning environments,Nathan Henderson,2019,Training,"VIDEO,SENSOR,RPA","POSE,EDA,AFFECT",CLS,HYBRID,EDM,International Conference on Educational Data Mining,44,VIRT,OTH,IND,TRAIN,UNI,MB,"""Results indicate that multimodal approaches outperform unimodal baseline classifiers, and feature-level concatenation offers the highest performance among the data fusion techniques.""",Clayton,1,"Affect detection in simulated medical environment. ""other"" for environment type because medical? SVM and DNN for model-based."
853680639,sensor-based data fusion for multimodal affect detection in game-based learning environments,Nathan Henderson,2019,Training,"VIDEO,SENSOR,RPA","POSE,EDA,AFFECT",CLS,HYBRID,EDM,International Conference on Educational Data Mining,44,VIRT,STEM,IND,TRAIN,UNI,MB,"""We show the improvement that multimodal classifiers achieve compared with unimodal classifiers for both modalities. We also demonstrate that SVMs outperform ANNs as a unimodal classifier in this particular domain. Finally, we demonstrate that data fusion is an effective way to combine multiple modalities, either prior to or following classification.""",Caleb,2,
853680639,sensor-based data fusion for multimodal affect detection in game-based learning environments,Nathan Henderson,2019,Training,"VIDEO,SENSOR,RPA","POSE,EDA,AFFECT",CLS,HYBRID,EDM,International Conference on Educational Data Mining,44,VIRT,STEM,IND,TRAIN,UNI,MB,,Clayton/Caleb,1&2,
3637456466,impact of inquiry interventions on students in e-learning and classroom environments using affective computing framework,T. S. Ashwin,2020,Learning,"VIDEO,PPA","AFFECT,POSE,GEST","CLS,STATS,PATT",MID,UMUAI,User Modeling and User-Adapted Interaction,45,"PHYS, VIRT",UNSP,"IND, MULTI",INSTR,UNSP,MB,"""The proposed method with multi-person detection, multi-modality, group engagement score, inquiry intervention and with an accuracy of 0.77 for a test data of more than 350 students, outperforms the existing methods.",Clayton,1,Ashwin paper! Automated inquiry-based instruction based on affective state. Two environments: classroom (multi-student) and e-learning (single-student). Classification via Inception and YOLO.
3637456466,impact of inquiry interventions on students in e-learning and classroom environments using affective computing framework,T. S. Ashwin,2020,Learning,"VIDEO,PPA","AFFECT,POSE,GEST","CLS,STATS,PATT",MID,UMUAI,User Modeling and User-Adapted Interaction,45,"PHYS, VIRT",UNSP,"IND, MULTI",INSTR,UNI,MB,"""The overall experimental results demonstrate that there is a positive correlation with r = 0.74 between students’ affective states and their performance. Proposed inquiry intervention improved the students’ performance as there is a decrease of 65%, 43%, 43%, and 53% in overall in-attentive affective state instances using the inquiry interventions in e-learning, flipped classroom, classroom and webinar environments, respectively.""",Caleb,2,
3637456466,impact of inquiry interventions on students in e-learning and classroom environments using affective computing framework,T. S. Ashwin,2020,Learning,"VIDEO,PPA","AFFECT,POSE,GEST","CLS,STATS,PATT",MID,UMUAI,User Modeling and User-Adapted Interaction,45,"PHYS, VIRT",UNSP,"IND, MULTI",INSTR,UNI,MB,,Clayton/Caleb,1&2,
3754172825,detecting impasse during collaborative problem solving with multimodal learning analytics,Yingbo Ma,2022,Learning,"VIDEO,AUDIO","TRANS,PROS,SPECT,GAZE,POSE",CLS,HYBRID,LAK,International Conference on Learning Analytics & Knowledge,46,VIRT,STEM,MULTI,INSTR,K12,MB,"""We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements in the video modality are the most significant unimodal indicators of impasse.""",Clayton,1,"Learning CS w/ Snap! collaboratively. SVM, BERT, MLP for model-based."
3754172825,detecting impasse during collaborative problem solving with multimodal learning analytics,Yingbo Ma,2022,Learning,"VIDEO,AUDIO","TRANS,PROS,SPECT,GAZE,POSE",CLS,HYBRID,LAK,International Conference on Learning Analytics & Knowledge,46,VIRT,STEM,MULTI,INSTR,K12,MB,"""We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements in the video modality are the most significant unimodal indicators of impasse. We also trained several multimodal models and found that combining indicators from these three modalities provided the best impasse detection performance.""",Caleb,2,
3754172825,detecting impasse during collaborative problem solving with multimodal learning analytics,Yingbo Ma,2022,Learning,"VIDEO,AUDIO","TRANS,PROS,SPECT,GAZE,POSE",CLS,HYBRID,LAK,International Conference on Learning Analytics & Knowledge,46,VIRT,STEM,MULTI,INSTR,K12,MB,,Clayton/Caleb,1&2,
1296637108,towards collaboration translucence: giving meaning to multimodal group data,Vanessa Echeverria,2019,Training,"VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER","POSE,LOGS,TRANS,EDA,ACT,PROS,INTER",QUAL,OTH,CHI,Conference on Human Factors in Computing Systems,47,PHYS,OTH,MULTI,TRAIN,UNI,MF,"""We illustrate the approach from the automated analysis of healthcare simulations to train nurses, generating four visual proxies that fuse multimodal data into higher order patterns.""",Clayton,1,Monash nursing group. Other for healthcare?
1296637108,towards collaboration translucence: giving meaning to multimodal group data,Vanessa Echeverria,2019,Training,"VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER","POSE,LOGS,TRANS,EDA,ACT,PROS,INTER",QUAL,OTH,CHI,Conference on Human Factors in Computing Systems,47,PHYS,STEM,MULTI,TRAIN,UNI,MB,"""we introduce the concept of collaboration translucence, an approach to make visible selected features of group activity. This is grounded both theoretically (in the physical, epistemic, social and affective dimensions of group activity), and contextually (using domain-specific concepts). We illustrate the approach from the automated analysis of healthcare simulations to train nurses, generating four visual proxies that fuse multimodal data into higher order patterns.""",Caleb,2,
1296637108,towards collaboration translucence: giving meaning to multimodal group data,Vanessa Echeverria,2019,Training,"VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER","POSE,LOGS,TRANS,EDA,ACT,PROS,INTER",QUAL,OTH,CHI,Conference on Human Factors in Computing Systems,47,PHYS,STEM,MULTI,TRAIN,UNI,MB,,Clayton/Caleb,1&2,
