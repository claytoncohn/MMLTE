UUID,Title,First Author,Year,Environment Type,Data Collection Mediums,Modalities,Analysis Methods,Fusion Types,Publication,Environment Setting,Environment Subject,Participant Structure,Didactic Nature,Level of Instruction or Training,Analysis Approach,Results
1118315889,using multimodal learning analytics to identify aspects of collaboration in project-based learning,Daniel Spikol,2017,Learning,"VIDEO,AUDIO,LOGS","POSE,PROS",REG,MID,CSCL,PHYS,STEM,MULTI,INSTR,UNI,MB,"""physical aspect of collaborative is an important part of this type of learning and that learning analytics systems can identify features that are relevant for helping researchers, teachers, and learners unpack what is happening.""

""In this research study, we presented that where the students are looking, the distance between them, the motion of their hands our key features for a learning analytics system to be effectively used to identify collaboration in small groups of Engineering students."""
3339002981,estimation of success in collaborative learning based on multimodal learning analytics features,Daniel Spikol,2017,Learning,"EYE,LOGS,VIDEO,AUDIO","GAZE,LOGS,PROS,POSE",CLS,MID,ICALT,VIRT,STEM,MULTI,INSTR,UNI,MB,"Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, authors found that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.

Predicting the learning gains via classification (bad, ok, good) through gaze, logs, audio, and dialog. Determined that distance measures between hands and gaze fixations was the key features to predict students' performance."
3093310941,embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders,Hiroki Tanaka,2017,Training,"AUDIO,VIDEO,PPA","POSE,PROS,AFFECT","REG,STATS",MID,PLOS,VIRT,HUM,IND,TRAIN,"K12, UNI",MF,"We identified a positive correlation between overall narrative skills and other non-verbal behaviors. This result indicates that the overall narrative skills were subjectively decided based on other non-verbal behavior skills [22]. Although the automated social skills trainer did not provide feedback regarding narrative structure or eye gaze, these elements were also improved after the training.

The focus of this study assessed the effectiveness of an automated social skills trainer with multimodal information that adheres to the basic human-based SST as closely as possible. Authors extended a previous method for automatic social skills training by adding audiovisual information regarding smiling ratio and head pose that improved the training effect. 
Multimodal feedback is also useful for both members of the general population with social difficulties and people with ASD because it helps such people understand and improve their narrative skills, as was previously reported in human-based SST [2, 3]."
3095923626,a multimodal analysis of making,Marcelo Worsley,2017,Learning,"VIDEO,AUDIO,SENSOR,PPA,INTER","GEST,PPA,EDA,ACT,PROS,QUAL,INTER","STATS,CLUST,QUAL,PATT",EARLY,IJAIED,PHYS,STEM,MULTI,INF,"K12, UNI",MB,"""each approach provides different affordances depending on the similarity metric and the dependent variable.""
 ""The take-away is that there are several strategies for doing multimodal learning analytics, and that many of these approaches can provide a meaningful glimpse into a complex data set, glimpses that may be difficult to identify using traditional approaches.""

""Looking across analyses, there are clear instances where each provided some novel insights. In this sense, the overall algorithm appears to have relevance for studying learning, success and experimental condition; but honing in on these correlations requires different modes of analysis.

As a whole this article has shown that success, learning and process are not equivalent, though they may occasionally overlap. Thus, when thinking about measuring the effectiveness of a given learning environment it is important to be clear about which metrics one hopes to optimize. At the same time, this article has provided additional evidence that experimental condition can have an impact on learning, success and process. Because of this, one has to be cognizant about how to develop learning and reasoning approaches that allow the environment to realize the desired outcomes."""
85990093,multimodal markers of persuasive speech : designing a virtual debate coach,Volha Petukhova,2017,Training,"VIDEO,AUDIO","PROS,GEST","CLS,QUAL,STATS",MID,INTERSPEECH,PHYS,HUM,MULTI,TRAIN,K12,MB,"""Experiments of different types supported fairly reliable identification of markers from multimodal data, and linking these to assessments of debater confidence level and intensification behaviour.""

""In line with previous empirical findings, we acknowledge that persuasive speech is rather difficult to characterize. Neverthe- less, based on theoretical and empirical frameworks set up by Grice (1975), Gussenhoven (2002) and Hirschberg (2002), we were able to define a set of criteria which help us to explain observed regularities and define rules, strategies and constraints for the generation, assessment and correction of trainees’ debate performance. Experiments of different types supported fairly reliable identification of markers from multimodal data, and linking these to assessments of debater confidence level and intensification behaviour."""
957160695,virtual debate coach design: assessing multimodal argumentation performance,Volha Petukhova,2017,Training,"VIDEO,AUDIO","GEST,TRANS,PROS,SURVEY,GAZE","STATS,CLS,QUAL",MID,ICMI,PHYS,HUM,MULTI,TRAIN,K12,MB,"""We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain corpora and classification procedures resulted in the best performance on an argument structure mining task.""

""We observed that linguistic features (i.e.n-gram of various size and types in combination with syntactic information), multimodal in-domain corpora and classification procedures resulted in the best performance on an argument structure mining task. Results of the argument quality experiments showed that argument com- prehensibility is affected by the number of referring expressions, information complexity, and presentation fluency. Presence of intensification and segmentation markers, position and movements of hands/ams and certain postures may affect the perception of the clarity, persuasiveness, and credibility of debaters."""
1637690235,supervised machine learning in multimodal learning analytics for estimating success in project-based learning,Daniel Spikol,2018,Learning,"VIDEO,AUDIO,LOGS,PPA,RPA","POSE,GEST,PROS,LOGS,PPA,RPA","REG,CLS",MID,JCAL,BLND,STEM,MULTI,INSTR,UNI,MB,"""In this paper, we show that MMLA and the state-of-the-art computational techniques can be used to generate insights into the ""black box"" of learning in students’ project-based activities. These insights generated from multimodal data can be used to inform teachers about the key features of project-based learning and help them support students appropriately in similar pedagogical approaches.""

""In this paper, we show that MMLA and the state-of-the-art computational techniques can be used to generate insights into the ""black box"" of learning in students’ project-based activities. These insights generated from multimodal data can be used to inform teachers about the key features of project-based learning and help them support students appropriately in similar pedagogical approaches. Towards achieving this ultimate aim, this paper has three main contributions to the field. First, we show that the distances between students’ hands and faces while they are working on projects is a strong predictor of students’ artefact quality which indicates the value of student collaboration in these pedagogical approaches. Second, we show that both, new and promising approaches such as neural networks and more traditional regression approaches, can be used to classify MMLA data and both have advantages and disadvantages depending on the research questions and contexts being investigated. At last but not least, although, it is traditionally notoriously challenging to provide evidence about the robust and objective evaluations of project-based learning activities, techniques and types of data we presented here can be the first step towards effective implementation and evaluation of project-based learning at a scale."""
2181637610,toward using multi-modal learning analytics to support and measure collaboration in co-located dyads,Emma L. Starr,2018,Learning,"VIDEO,AUDIO,PPA,RPA,LOGS","POSE,PROS,PPA,RPA,LOGS","STATS,QUAL",OTH,ICLS,BLND,STEM,MULTI,INF,UNI,MF,"While this study was not able to show a clear effect of providing a real-time visualization to support
collaboration, it made many other valuable contributions. First, it showed that simple verbal interventions can
help participants pay attention to particular aspects of their collaborative behavior (i.e., how much they are
talking and how much space they are providing to their partner). Second, it suggested that awareness tools such
as the one developed for this study have to be designed differently to impact social interactions (e.g., by being
more salient or be used in a setting where users have the mental bandwidth to reflect on their collaborative
style). Third, we collected a rich multi-modal dataset that can be used to build proxies for measuring effective
collaborations.

The purpose of this paper was to explore the effect of two collaboration interventions and the relationship between collaboration quality, task performance and learning gains, however this study was not able to show a clear effect of providing a real-time visualization to support collaboration. It did show that simple verbal interventions can help participants pay attention to particular aspects of their collaborative behavior, and suggested that awareness tools such as the one developed for this study have to be designed differently to impact social interactions. Authors built a rich multi-modal dataset that can be used to build proxies for measuring effective collaborations. As a preliminary analysis, they found that various indicators captured by the Kinect sensor were correlated with participants’ quality of collaboration."
3146393211,mobile mixed reality for experiential learning and simulation in medical and health sciences education,James Birt,2018,Learning,"PPA,INTER","PPA,TRANS","QUAL,STATS",OTH,Information,BLND,STEM,IND,INSTR,UNI,MF,"Finally, it was clear from the two intervention studies that the simulation provided by mobile devices, although not as powerful as the desktop-computer-powered Oculus Rift mixed reality or physical simulators, has no negative impacts on the learners. In fact, the simulation improves learners’ competence and skills with the skills intervention showing statistically significant improvements in the learners that received the mobile mixed reality simulation tools prior to residential school.

This study validates the use of mobile devices in university undergraduate health sciences curricula, and shows that not only are these modes (game engines, free AR/VR SDKs and mobile-based devices with GPU-enabled processors and high-quality screens) useful for enhancing the development of physical skills in students, but they are also received favorably."
3135645357,multimodal teaching analytics: automated extraction of orchestration graphs from wearable sensor data,Luis P. Prieto,2018,Learning,"EYE,VIDEO,AUDIO,MOTION","GAZE,PROS,ACT,PIXEL","NET,CLS,STATS,PATT,QUAL",HYBRID,JCAL,PHYS,STEM,MULTI,INSTR,PROF,MB,"""In summary, the results from our evaluation of personalized and general models to automatically extract orchestration graphs highlight the fact that machine learning models can be successfully trained with such multimodal sensor data, using relatively low-level features.""

""In summary, the results from our evaluation of personalized and general models to automatically extract orchestration graphs highlight the fact that machine learning models can be successfully trained with such multimodal sensor data, using relatively low-level features."""
3783339081,a novel method for the in-depth multimodal analysis of student learning trajectories in intelligent tutoring systems,Ran Liu,2018,Learning,"LOGS,AUDIO,SCREEN,PPA","LOGS,TRANS,ACT,QUAL,PPA","STATS,REG,QUAL",MID,JLA,VIRT,STEM,IND,INSTR,K12,MB,"Corse-grained learning trajectories were able to identify key moments during learning that warrented futher analysis with other MMD

""Our results from this analysis showed how students’ early experiences struggling with a novel concept could significantly affect both their entire learning trajectories within an activity and pre-test–post-test measurements of learning gains related to that concept. It provided evidence of an important “moment” for early instructional intervention.

The analysis we conducted on the Concentration knowledge component was an example of how a knowledge component-centred analysis can benefit from this multi-step approach as well. Our results led to a modification in the knowledge component assignment to problem steps within a ChemVLab+ activity as well as instructional implications for promoting better learning of a previously hidden conceptual difficulty."""
3796180663,learning linkages: integrating data streams of multiple modalities and timescales,Ran Liu,2018,Learning,"VIDEO,AUDIO,LOGS,SCREEN,PPA","TRANS,QUAL,PPA","CLS,STATS",MID,JCAL,VIRT,STEM,"IND, MULTI",INSTR,K12,"MB, MF","We presented two empirical studies, collected in classroom studies with two distinct learning technology systems in different contexts (individual and collaborative). Our analyses and findings showcase a few different ways, in which multimodal data sources can enrich our understanding of student learning and provide a more holistic picture.

Authors collected student‐focused screen and webcam video which were useful for understanding students' learning processes and approaches based on detailed analyses of their interactions with the tutor interface, mouse movements, and out‐of‐tutor (in person) help‐seeking. High‐fidelity audio of students' collaborative dialogue was collected to generate high‐quality transcriptions of students' dialogue and apply an NLP approach to make use of the large quantity of audio dialogue. The verbal data allowed authors to identify linguistic features in students' collaborative dialogue that were highly predictive of math performance on pretest and posttest assessments, above and beyond any nonlinguistic variables."
2345021698,exploring collaborative writing of user stories with multimodal learning analytics: a case study on a software engineering course,René Noël,2018,Learning,"AUDIO,PPA,RPA","RPA,PROS","QUAL,NET,STATS",OTH,Access,PHYS,STEM,MULTI,INF,UNI,"MB, MF","``There was no difference between the proportion of the session spent being bored or frustrated or in the achievement scores for the two conditions. In order to determine whether the effect on achievement was different depending on length of exposure, participants with a duration less than 60 minutes were excluded from the analysis. However, there was still no significant difference between achievement scores from intervention and control sessions for the group as a whole or for each subgroup.``

Main findings of the case study are the relationships between prior experience in software requirements and the way the team members collaborate, and the lower productivity of low experienced groups. No evidence was found that performance of domain experts was superior from non-experts during collaborative problem-solving sessions. Although it was stated that low experience subjects produced more user stories, a greater productivity of top experience subjects was not statistically verified."
2497456347,the rap system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors,Xavier Ochoa,2018,Training,"AUDIO,VIDEO,PPA,SURVEY","PPA,GAZE,POSE,PROS","CLS,STATS,QUAL",LATE,LAK,BLND,HUM,IND,TRAIN,UNI,MB,"Feedback generated by RAP is similar to human feedback across several dimensions and similar to other more complex systems

""It revealed an overwhelmingly positive perception of the system especially in the dimensions of perceived usefulness and feedback which were rated as excellent by 65% and 58% of the students respectively. The qualitative analysis helped discover specific issues, on the positive side, students commented on the potential of the system to quickly learn some basic presentation skills: ""I would like to see this system used in our Communications class"". On the negative side, students commented that they sometimes were aware that they were being recorded and that the environment was too small. Also, some students felt uncomfortable with a pre-recorded audience because it didn’t seem to react to their presentation: ""the audience had always the same expressions"". Overall, the students agreed that the system was useful and that they learned about their own presentation skills while using it."""
4019205162,introducing low-cost sensors into the classroom settings: improving the assessment in agile practices with multimodal learning analytics,Hector Cornide-Reyes,2019,Learning,"AUDIO,SURVEY,PPA,RPA","SURVEY,TRANS,PPA,RPA","NET,STATS","MID,OTH",Sensors,PHYS,STEM,MULTI,INSTR,UNI,"MB, MF","RQ1: Better communication, better collaboration
RQ2: Collaborative teams showed lower variability in the estimates of story points (same page)
RQ3: Democratic leadership in collaborative groups

The results obtained allowed the detection of important relationships and characteristics of the collaborative and Non-Collaborative groups, with productivity, effort, and predominant personality styles in the groups. Authors conclude that the Multimodal Learning Analysis techniques offer considerable feasibilities to support the process of skills development in students."
1847468084,computationally augmented ethnography: emotion tracking and learning in museum games,Kit Martin,2019,Learning,"VIDEO,AUDIO,PPA,RPA","TRANS,AFFECT,QUAL",QUAL,OTH,ICQE,VIRT,STEM,MULTI,INF,UNSP,"MB, MF","Authors present an initial pilot using these methods in concert to identify key moments in multiple modalities. While the use of constructivist dialogue mapping showed that the users learned during their interaction with Ant Adaption, emotional logging identified alternative moments of learning outside of their analytic framework.

This paper presented a preliminary approach to augment qualitative analysis of an
informal learning environment. Using techniques from multimodal learning analytics,
we were able to expand our analysis of learning while participants interacted with a
multitouch environment. Our methodological approach required us to extract emotions
from the low-level logs of facial action units using FACET and then revisit video
corresponding to particular FACET values to identify moments of high emotional stimulation theoretically implicated in learning."
1326191931,multimodal learning analytics in a laboratory classroom,Man Ching Esther Chan,2019,Learning,"VIDEO,AUDIO","POSE,GAZE,PROS","CLS,CLUST",LATE,MLPALA,PHYS,STEM,"IND, MULTI",INSTR,UNSP,MB,"The MMLA techniques applied were able to successfully extract relevant features to quantify and visualize teacher and student behaviors and activities related to student engagement based on the classroom video and audio recordings

Within heavily rigged smart classroom, visual (gaze, posture) and auditory (speech levels) were extracted via AI and were then used (rule's based approach -> model) to compute student's engagement in individual, pair, and group structures."
1576545447,artificial intelligence and multimodal data in the service of human decision-making: a case study in debate tutoring,Mutlu Cukurova,2019,Learning,"AUDIO,SURVEY","AFFECT,LOGS",CLS,MID,BJET,UNSP,HUM,IND,TRAIN,UNSP,"MB, MF","In this study, we used non-transparent prediction models of openSMILE to predict the emotional traits of tutor candidates based on their audio data, and we used transparent logistic regression models to identify exactly what personality, emotion, and experience traits lead to effective debate tutoring skills. Predictive models were very powerful to make sense of complex and nonlinear audio data, whereas the transparent regression models were valuable to identify key aspects for tutors to reflect upon their own decisions and provide tutor candidates with feedback on their performance.

Authors combined predictive and transparent models to support the human decision-making processes involved in tutor trainee evaluations and results showed that models with multimodal data can accurately classify tutors and have the potential to support the intuitive decision-making of expert tutors in the context of evaluating trainee applicants."
3398902089,what multimodal data can tell us about the students’ regulation of their learning process?,Sanna Järvelä,2019,Learning,"SENSOR,VIDEO,AUDIO","EDA,AFFECT,QUAL",QUAL,OTH,LAI,BLND,STEM,MULTI,INSTR,K12,MF,"Our major claim is that multichannel data can be potential for understanding regulatory processes in collaboration. With our five empirical case examples, we illustrate how triangulating multiple sources of data has potential to advance the theoretical and conceptual progress in social aspects of SRL theory.

Authors show with five empirical cases that multichannel data can be potential for understanding regulatory processes in collaboration, illustrating how triangulating multiple sources of data has potential to advance the theoretical and conceptual progress in social aspects of SRL theory: (1) understanding how interactions between different facets of regulation, such as cognition, motivation and emotion interact with cognitive strategic action by using video and EDA data; (2) visualizing how physiological synchrony measured from the heart rate can reveal or backup the interpretation of socially shared regulation of learning or co-regulation of learning located from the video; (3) visualizing temporality and cyclical processes (i.e., planning, enacting strategies, reflecting, adapting) of regulation by using video, EDA and facial expression recognition data; (5)) illustrating how combining not only physiological measures, but also facial expression data can lead even more accurate interpretations of the situations where regulation of learning is needed."
86191824,examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes,Shiyan Jiang,2019,Learning,"SCREEN,INTER,PPA,AUDIO","INTER,QUAL,TRANS",QUAL,OTH,ILE,VIRT,STEM,MULTI,INSTR,K12,MB,"""This study contributes an initial understanding into how different modalities mediate students’ interactions and offers implications for scaffolding peer interactions during multimodal composing processes.""

""Overall, students were more likely to share ideas and ask questions, and they tended to use quick- response strategies while multimodal composing.""

""Students’ interview responses also suggested that providing short responses was a typical strategy during multimodal composing.""

""When examining interactions across sessions, the group was more engaged in discussions at the beginning and the end of the project while fewer interactions occurred during the middle of their composing process.""

""Giving commands occurred much less frequently than other interaction types (Figure 3).""

""Students discussed more often about comics that combined visuals and text than other modal elements.""

""Making learning visible in different modes was critical to foster peer interaction (Jahnke, Norqvist, &
Olsson, 2013).""

""Results showed that there were interactional differences based on different modes.""

""While comparing discussions on static visual modes, namely images and multimodal comics, we found that images involved more self-oriented and less group-oriented contributions.""

""Discussions on animations included more elaborated feedback.""

""Written narrative provided the least opportunity for group-oriented contributions."""
3448122334,"investigating the impact of a real-time, multimodal student engagement analytics technology in authentic classrooms",Sinem Aslan,2019,Learning,"VIDEO,LOGS,SCREEN,AUDIO,SURVEY,RPA,PPA,INTER","AFFECT,LOGS,POSE,QUAL,INTER,SURVEY,RPA","QUAL,STATS,CLS",LATE,CHI,VIRT,STEM,IND,INSTR,K12,MB,"Significant impact on the teacher's scaffolding behavior and student engagement (less bordem)

SEAT had positive impact on student engagement and was also helpful to teachers."
1296637108,towards collaboration translucence: giving meaning to multimodal group data,Vanessa Echeverria,2019,Training,"VIDEO,AUDIO,LOGS,SENSOR,MOTION,INTER","POSE,LOGS,TRANS,EDA,ACT,PROS,INTER",QUAL,OTH,CHI,PHYS,STEM,MULTI,TRAIN,UNI,MB,"""We illustrate the approach from the automated analysis of healthcare simulations to train nurses, generating four visual proxies that fuse multimodal data into higher order patterns.""

""we introduce the concept of collaboration translucence, an approach to make visible selected features of group activity. This is grounded both theoretically (in the physical, epistemic, social and affective dimensions of group activity), and contextually (using domain-specific concepts). We illustrate the approach from the automated analysis of healthcare simulations to train nurses, generating four visual proxies that fuse multimodal data into higher order patterns."""
1770989706,focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving,Hana Vrzakova,2020,Learning,"AUDIO,VIDEO,SCREEN,SURVEY,PPA","PROS,ACT,GEST,PPA","STATS,PATT",MID,LAK,VIRT,STEM,MULTI,INSTR,UNI,MF,"We found that idling with limited speech (i.e., silence or backchannel feedback only) and without movement was negatively correlated with task performance and with participants’ subjective perceptions of the collaboration. However, being silent and focused during solution execution was positively correlated with task performance. Results illustrate that in some cases, multimodal patterns improved the predictions and improved explanatory power over the unimodal primitives.

""Here, we examine unimodal primitives (activity on the screen, speech, and body movements), and their multimodal combinations during remote CPS.""

Mixed findings for uni- versus multi-modal:

""These results lead us to question: are the multimodal patterns better than the unimodal primitives? As illustrated above, we found evidence for both sides of the argument. In the case of code execution, the answer is no, but it is a yes in the case of idling. However, it is important to go beyond the significant correlations as there is an informative signal in the non-significant ones as well. For example, consider idling once again. By itself, this pattern is negatively correlated with the task score (r = -.21) and the correlation is even more negative when idling is accompanied by silence/back channeling and little movement (r = -.35). However, there are many other configurations where idling is weak or negligible predictor of task score. For example, idling occurring in the context of the contributors speaking with some movement is more weakly correlated with task score (r = -.11) and the correlation is essentially null when idling is accompanied with the controller speaking and some movement (r = -.06). Thus, even when they do not improve predictive power, multimodal patterns help contextualize and reveal nuances in the unimodal primitives. This supports the overall idea of multimodal learning analytics in which the additional modalities (speech and body movement in our case) help to understand unclear patterns such as idling. This finding is interesting from two perspectives."""
3051560548,temporal analysis of multimodal data to predict collaborative learning outcomes,Jennifer K. Olsen,2020,Learning,"LOGS,AUDIO,EYE","GAZE,LOGS,PROS,TRANS,QUAL",REG,MID,BJET,VIRT,STEM,MULTI,INSTR,K12,MB,"Assessing the relation of dual gaze, tutor log, audio and dialog data to students’ learning gains, we find that a combination of modalities, especially those at a smaller time scale, such as gaze and audio, provides a more accurate prediction of learning gains than models with a single modality.

Evaluating how multimodal features contribute to a model's performance to predict learning gains. Audio features introduce noise that negatively impacted the error of the model."
3796643912,an evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities,Penelope J. Standen,2020,Learning,"VIDEO,AUDIO,LOGS,RPA","AFFECT,POSE,LOGS,RPA,GAZE,PROS,GEST","CLS,STATS",HYBRID,BJET,VIRT,"HUM, OTH, STEM",IND,INSTR,K12,MB,"This is the first study to evaluate an adaptive learning system for learners with ID based on multi-modal affect recognition. Three separate states were automatically identified, with lower levels of the state labelled “boredom” having the strongest link to learning achievement. Both those labelled “frustration” and “engagement” were positively related to achievement. Our results are in line with other studies showing that engagement increases when activities are tailored to the personal needs
and emotional states of learners (Athanasiadis et al., 2017), but no significant difference in learn-ing achievement was found (at least for the period of our study) when adaption was based on both the affective state and achievement of the learner, compared with achievement alone.

Results suggest that engagement does increase when activities are tailored to the personal needs and emotional state of the learner and that the system was promoting affective states that in turn promote learning. However, longer exposure is necessary to determine the effect on learning."
2634033325,controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting,Xavier Ochoa,2020,Training,"VIDEO,AUDIO,PPA","POSE,PROS,PPA",STATS,OTH,BJET,BLND,HUM,IND,TRAIN,UNSP,MF,"Evidence found in this paper suggests that automated feedback has a positive effect on oral presentation quality, but that the strength of this effect is small. Furthermore, different oral presentation dimensions are affected differently by the use of the system (i.e., there are large gains in looking at the audience during the presentation, while there is a negligible improvement in the avoidance of filled pauses)

Authors showcase that the training tool improved manually defined scores between an initial and second use of the tool."
1426267857,"affect, support, and personal factors: multimodal causal models of one-on-one coaching",Lujie Karen Chen,2021,Learning,"AUDIO,VIDEO,SURVEY","PROS,GAZE,TRANS,AFFECT,SURVEY","STATS,NET",HYBRID,JEDM,PHYS,STEM,IND,INSTR,K12,MB,"""this research contributes to the relatively sparse literature in multimodal learning analytics by providing a balanced view of the teacher and student interactions with a data set collected in naturalistic home environments.""

""Firstly, we note a clear causal pathway between the group of variables describing parents’ support and another group representing the child’s cognitive-affective experience.""

""Secondly, we note the causal pathway from Profile to Affect and, indirectly, to Support."""
666050348,multicraft: a multimodal interface for supporting and studying learning in minecraft,Marcelo Worsley,2021,Learning,"AUDIO,EYE,TEXT,VIDEO,SCREEN,INTER,SURVEY,LOGS","PROS,TRANS,GAZE,TEXT,INTER,SURVEY,LOGS",QUAL,OTH,HCII,VIRT,STEM,IND,INF,K12,MF,"""Through our user studies, we found that the platform helps fulfill some of those goals by providing capabilities that can spur on amazement and excitement among traditional Minecraft users and novices. We also find that many of the multimodal components, while not immediately intuitive for users, proved to be preferred modes of game play. In this sense, we feel that this tool is moving in the right direction in terms of the system capabilities that it provides. Our analyses also point to the meaningful ways that multimodal data can be used to study student learning in these game-based environments, and free students from standardized testing and learning experiences.""

""Through our user studies, we found that the platform helps fulfill some of those goals by providing capabilities that can spur on amazement and excitement among traditional Minecraft users and novices. We also find that many of the multimodal components, while not immediately intuitive for users, proved to be preferred modes of game play."""
518268671,using multimodal learning analytics to explore collaboration in a sustainability co-located tabletop game,María Ximena López,2021,Learning,"SURVEY,LOGS,AUDIO,VIDEO","LOGS,SURVEY,GAZE,PROS",STATS,OTH,ECGBL,BLND,STEM,MULTI,INSTR,UNI,MF,"Overall, our study suggests that coupling style can be operationalised as a multimodal indicator suitable to investigate complex collaboration dynamics in games, thus confirming what has been found in other domains

""Firstly, we found that coupling styles can be used to characterise collaboration in a co-located SG. Consistently with previous research (Isenberg et al, 2010; Niu et al, 2018), our results showed that teams displayed both close and loose coupling styles while performing individual actions to accomplish shared goals. Interestingly, we found a positive association between the time spent working closely coupled and the individual interactions with the technology. This suggests that the pursuit of collective goals requires players to continuously alternate and integrate individual planning and action with closely-coupled, likely to verify and synchronise their own actions with others. Secondly, we found that the perceived quality of collaboration does not appear to be an effective indicator of collaboration quality by itself. However, its small association with close-coupling style suggests a conscious, continuous, and proactive approach to collaboration, since players who appreciate the value of collaboration also seem to actively engage in closely-coupled interactions with others. Thirdly, our findings suggest that better-performing teams do work more closely-coupled and alternate their interactions with individual work. This result indicates that freely alternating individual work with closely-coupled interaction is an effective collaboration strategy, and that collaborative SGs should afford this opportunity. Overall, our study suggests that coupling style can be operationalised as a multimodal indicator suitable to investigate complex collaboration dynamics in games, thus confirming what has been found in other domains (Isenberg et al, 2010; Niu et al, 2018)."""
2273914836,many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities,Jauwairia Nasir,2022,Learning,"VIDEO,AUDIO,LOGS,PPA,SURVEY","PROS,AFFECT,GAZE,TRANS,LOGS","STATS,QUAL,CLUST,CLS",HYBRID,IJCSCL,BLND,STEM,MULTI,INSTR,K12,MB,"""Our combined multi-modal learning analytics and interaction analysis methodology enabled us to identify two multi-modal profles of learners who have learning gains and one multi-modal profle of learners who do not have learning gains.""

""Using this approach, we are able to build the multimodal behavioral profles for each group of learners.""

""Our combined multi-modal learning analytics and interaction analysis methodology enabled us to identify two multi-modal profles of learners who have learning gains and one multi-modal profle of learners who do not have learning gains."""
32184286,once more with feeling: emotions in multimodal learning analytics,Marcus Kubsch,2022,Learning,"SURVEY,PPA,AUDIO","INTER,SURVEY,TRANS,PROS,AFFECT","CLS,REG,STATS",OTH,MMLA Handbook,PHYS,STEM,IND,INSTR,K12,"MB, MF","Results demonstrate how NLP and ML techniques allow us to use different modalities of the same data, voice and transcript, and different modalities of different data sources, voice data from interviews, answers to a goal orientation questionnaire, and answers to open-ended questions about energy, in order to better understand individual differences in students’ performances

Used text and audio to predict student's affect. With the affect, the authors' explored its statistical relation to student's knowledge. Results point that they need more data to improve performance in affect prediction but promising direction."
2609260641,visualizing collaboration in teamwork: a multimodal learning analytics platform for non-verbal communication,René Noël,2022,Learning,"AUDIO,VIDEO,RPA,INTER","PROS,POSE,RPA,INTER,QUAL",QUAL,OTH,DAMLE,PHYS,HUM,MULTI,INF,"PROF, UNI",MF,"""We conducted a case study to compare the visualizations provided by the system in two different situations: collaborative and competitive activities. The results suggest that the provided visualizations help to identify issues on cognitive contribution, assimilation, self-regulation, and integration of the team members. They could also support teachers to decide whether they must assist a team in fostering collaboration.

While the results are naturally constrained to the characteristics of the activities in which we tested the platform, they provide initial evidence about the technical fea-
sibility of extracting behavioral indicators and traces using MMLA to give insights onteam collaboration.""

""The results suggest that the provided visualizations help to identify issues on cognitive contribution, assimilation, self-regulation, and integration of the team members. They could also support teachers to decide whether they must assist a team in fostering collaboration."""
1345598079,intermodality in multimodal learning analytics for cognitive theory development: a case from embodied design for mathematics learning,Sofia Tancredi,2022,Learning,"EYE,VIDEO,AUDIO,INTER","GAZE,GEST,TRANS,POSE,INTER","PATT,QUAL,STATS",OTH,MMLA Handbook,VIRT,STEM,IND,INSTR,K12,MF,"""Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Exploration, increasing confluence during Discovery, and high confluence during Fluency.""

""Looking intermodally across hand and gaze dynamics, each stage was characterized by distinct meta patterns: disconfluence of hand and gaze during Exploration, increasing confluence during Discovery, and high confluence during Fluency. Towards the end of the Discovery stage, a coordination of coordinations (Piaget, 1970) developed wherein the coordination between the left- and right hands became coordinated with newly developed gaze structures spanning different screen locations.""

""Our findings point to the importance of MMLA work that attunes to intermodal dynamics of learning, both as a pragmatic resource for identifying key moments in learning and as a resource for refining theoretical understandings of learning processes."""
2155422499,a multimodal analysis of pair work engagement episodes: implications for emi lecturer training,Teresa Morell,2022,Training,"VIDEO,AUDIO,PPA","TRANS,PPA,QUAL,POSE,ACT",QUAL,OTH,JEAP,PHYS,OTH,MULTI,TRAIN,PROF,MF,"""In this study, we have given special emphasis to the interpersonal metafunction (see Fig. 1), which is crucial to enhance EMI students’ active participation and engagement. The outcomes of this study point to the need to address multimodal interactional competence in professional development programs, as described in Morell et al. (2022) to promote effective lecturing in the diverse EMI scenarios.""

""The exploration of how EMI lecturers use semiotic resources to construct meaning and to create engagement paves the way to a unified multimodal interactional competence. In general, the mastery of this competence enables lecturers to convert students from passive listeners/observers to active participants, giving them opportunities to engage in active learning, language usage and critical thinking."""
3754172825,detecting impasse during collaborative problem solving with multimodal learning analytics,Yingbo Ma,2022,Learning,"VIDEO,AUDIO","TRANS,PROS,SPECT,GAZE,POSE",CLS,HYBRID,LAK,VIRT,STEM,MULTI,INSTR,K12,MB,"""We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements in the video modality are the most significant unimodal indicators of impasse.""

""We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements in the video modality are the most significant unimodal indicators of impasse. We also trained several multimodal models and found that combining indicators from these three modalities provided the best impasse detection performance."""
